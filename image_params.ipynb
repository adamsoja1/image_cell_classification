{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2dd2f40a-b828-42c0-96b3-07cdee6c66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils_cells import get_images_list, transform_image, transform_target, resize_with_padding\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, target_transform=None, reduce=False):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dataset = shuffle(self.load_dataset(data_path))\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        files = os.listdir(path)\n",
    "        dataset_final = pd.DataFrame()\n",
    "        dataset_final['filename'] = []\n",
    "        dataset_final['class'] = []\n",
    "        for filename in files:\n",
    "            dataset = pd.DataFrame()\n",
    "            if filename.endswith('.txt'):\n",
    "                files = get_images_list(f'{path}/{filename}')\n",
    "                dataset['filename'] = files\n",
    "                dataset['class'] = filename.split('_')[1][:-3]\n",
    "                dataset_final = pd.concat([dataset_final, dataset], ignore_index=True)\n",
    "        return dataset_final                \n",
    "                          \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(f'{self.dataset[\"filename\"].loc[idx]}')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "        #image = resize_with_padding(image, (32, 32))\n",
    "        image = image.astype(np.float32)\n",
    "        image = self.transform(image = image)['image'] if self.transform is not None else image\n",
    "\n",
    "        target = self.dataset[\"class\"].loc[idx]\n",
    "\n",
    "        if target == 'normal.':\n",
    "            target_ = [1, 0, 0, 0]\n",
    "        elif target == 'inflamatory.':\n",
    "            target_ = [0, 1, 0, 0]\n",
    "        elif target == 'tumor.':\n",
    "            target_ = [0, 0, 1, 0]\n",
    "        elif target == 'other.':\n",
    "            target_ = [0, 0, 0, 1]\n",
    "        else:\n",
    "            print(target)\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "       \n",
    "     \n",
    "\n",
    "        \"\"\"To see transorms use:\n",
    "            image, target = trainset[15]\n",
    "            image = image.numpy()\n",
    "            image=np.swapaxes(image,0,1)\n",
    "            image=np.swapaxes(image,1,2)\n",
    "            plt.imshow(image)\"\"\"\n",
    "\n",
    "        return image.float(), torch.Tensor(np.array(target_, dtype=np.float32))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbaec861-bf77-4c8b-ab61-e4b7d66f6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageDataset(data_path='train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68821c6b-5a82-403c-99ce-0c6240c4673f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_val = 0\n",
    "for idx in range(len(trainset)):\n",
    "    img = trainset[idx][0]\n",
    "    if torch.max(img).item() > max_val:\n",
    "        max_val = torch.max(img).item()\n",
    "max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "988c2a97-dfa5-4628-9565-2e3425ebc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([127.6431,  89.9149, 140.1093])\n",
      "tensor([38.0793, 34.1893, 28.6506])\n"
     ]
    }
   ],
   "source": [
    "mean = 0\n",
    "std = 0\n",
    "for idx in range(len(trainset)):\n",
    "    img = trainset[idx][0]\n",
    "    mean += torch.mean(img, dim=(1, 2))\n",
    "    std += torch.std(img, dim=(1, 2))\n",
    "\n",
    "mean /= len(trainset)\n",
    "std /= len(trainset)\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9eb05bd-a40d-480b-a331-51f205454739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_mean_std(loader):\n",
    "    # Compute the mean and standard deviation of all pixels in the dataset\n",
    "    num_pixels = 0\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        num_pixels += batch_size * height * width\n",
    "        mean += images.mean(axis=(0, 2, 3)).sum()\n",
    "        std += images.std(axis=(0, 2, 3)).sum()\n",
    "\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d530e069-8d5d-4f1c-9f0c-a96af185d75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([-2.4241e-04,  7.3988e-05, -4.0785e-04])\n",
      "Std: tensor([1.0002, 0.9998, 0.9996])\n"
     ]
    }
   ],
   "source": [
    "mean = 0.0\n",
    "std = 0.0\n",
    "n_images = 0\n",
    "for images, _ in loader:\n",
    "    # Flatten the image tensors to (batch_size, channels * height * width)\n",
    "    images = images.view(images.size(0), images.size(1), -1)\n",
    "    \n",
    "    # Update the total number of images\n",
    "    n_images += images.size(0)\n",
    "    \n",
    "    # Calculate the sum and sum of squares of pixel values\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean /= len(trainset)\n",
    "std /= len(trainset)\n",
    "\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9b8b0f2-4a3f-4246-a842-307fea0aa065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    "Lambda\n",
    ")\n",
    "\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=0, std=1)],\n",
    ")\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform_test)\n",
    "max_val = 0\n",
    "min_val = 1000\n",
    "for idx in range(len(trainset)):\n",
    "    img = trainset[idx][0]\n",
    "    if torch.max(img).item() > max_val:\n",
    "        max_val = torch.max(img)\n",
    "    if torch.min(img).item() < min_val:\n",
    "        min_val = torch.min(img)\n",
    "\n",
    "max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b320b07-ba42-47ac-b71a-c7bfdcfab7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495dc8d-5608-417a-ba09-73c82262c7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
