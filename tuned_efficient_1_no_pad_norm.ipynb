{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb74ae06-2775-4bb7-841c-8e11a4b3e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003305526571026246\n",
    "batch_size = 230\n",
    "dropout_rate = 0.30806216013664534\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdce5c8-56c3-4c69-b604-58c13391dbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils_cells import get_images_list, transform_image, transform_target, resize_with_padding\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics import Precision, Recall\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "\n",
    "optuna.samplers.TPESampler(seed=2233)\n",
    "\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, target_transform=None, reduce=False):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dataset = shuffle(self.load_dataset(data_path))\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        files = os.listdir(path)\n",
    "        dataset_final = pd.DataFrame()\n",
    "        dataset_final['filename'] = []\n",
    "        dataset_final['class'] = []\n",
    "        for filename in files:\n",
    "            dataset = pd.DataFrame()\n",
    "            if filename.endswith('.txt'):\n",
    "                files = get_images_list(f'{path}/{filename}')\n",
    "                dataset['filename'] = files\n",
    "                dataset['class'] = filename.split('_')[1][:-3]\n",
    "                dataset_final = pd.concat([dataset_final, dataset], ignore_index=True)\n",
    "        return dataset_final                \n",
    "                          \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(f'{self.dataset[\"filename\"].loc[idx]}')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "        #image = resize_with_padding(image, (32, 32))\n",
    "        image = image.astype(np.float32)\n",
    "        image = self.transform(image = image)['image'] if self.transform is not None else image\n",
    "\n",
    "        target = self.dataset[\"class\"].loc[idx]\n",
    "\n",
    "        if target == 'normal.':\n",
    "            target_ = [1, 0, 0, 0]\n",
    "        elif target == 'inflamatory.':\n",
    "            target_ = [0, 1, 0, 0]\n",
    "        elif target == 'tumor.':\n",
    "            target_ = [0, 0, 1, 0]\n",
    "        elif target == 'other.':\n",
    "            target_ = [0, 0, 0, 1]\n",
    "        else:\n",
    "            print(target)\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "       \n",
    "     \n",
    "\n",
    "        \"\"\"To see transorms use:\n",
    "            image, target = trainset[15]\n",
    "            image = image.numpy()\n",
    "            image=np.swapaxes(image,0,1)\n",
    "            image=np.swapaxes(image,1,2)\n",
    "            plt.imshow(image)\"\"\"\n",
    "\n",
    "        return image.float(), torch.Tensor(np.array(target_, dtype=np.float32))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c9ce6d-ff90-47af-9c56-b9fedb9fdef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:2587: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2233)\n",
    "\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=0, std=1),\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=0, std=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf1a2930-151c-47fb-9dfa-049423ebc571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4012672"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=4, dropout_rate=dropout_rate):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.base_model = models.efficientnet_b0(pretrained=False)\n",
    "        num_ftrs = self.base_model.classifier[1].in_features\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),  # Add dropout layer\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "    \n",
    "model = EfficientNetB0(num_classes=4, dropout_rate=dropout_rate)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abc075-67ea-4128-9aa2-bb4ee837775e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8eb3155-df02-49ac-a6b2-6bbea2a732a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240805_001214-2v93fw2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/2v93fw2q' target=\"_blank\">efficient_net1__nopad_norm2024-08-05 00:12:13.150564</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/2v93fw2q' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/2v93fw2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.9516421472325044\n",
      "train_precision: tensor([0.5269, 0.6703, 0.6219, 0.0539], device='cuda:0')\n",
      "train_recall: tensor([0.5211, 0.7515, 0.6140, 0.0013], device='cuda:0')\n",
      "val_loss: 1.006547550459455\n",
      "val_precision: tensor([0.6281, 0.5653, 0.6566, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.3411, 0.9011, 0.6323, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.8986515983540744\n",
      "train_precision: tensor([0.5645, 0.6820, 0.6619, 0.5122], device='cuda:0')\n",
      "train_recall: tensor([0.5551, 0.7696, 0.6529, 0.0091], device='cuda:0')\n",
      "val_loss: 0.8553745480546927\n",
      "val_precision: tensor([0.5912, 0.6712, 0.7064, 0.7684], device='cuda:0')\n",
      "val_recall: tensor([0.5667, 0.8342, 0.6305, 0.0458], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.8361220604595654\n",
      "train_precision: tensor([0.5904, 0.7065, 0.6810, 0.6213], device='cuda:0')\n",
      "train_recall: tensor([0.5769, 0.7816, 0.6857, 0.0599], device='cuda:0')\n",
      "val_loss: 0.9351788194399522\n",
      "val_precision: tensor([0.5631, 0.6527, 0.6090, 0.7432], device='cuda:0')\n",
      "val_recall: tensor([0.4703, 0.8087, 0.6209, 0.0458], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.8153594864880975\n",
      "train_precision: tensor([0.6040, 0.7114, 0.6901, 0.6539], device='cuda:0')\n",
      "train_recall: tensor([0.5858, 0.7897, 0.6918, 0.1156], device='cuda:0')\n",
      "val_loss: 0.8737128148055137\n",
      "val_precision: tensor([0.5450, 0.7361, 0.7308, 0.3516], device='cuda:0')\n",
      "val_recall: tensor([0.7136, 0.7391, 0.5301, 0.2158], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7674309183569515\n",
      "train_precision: tensor([0.6251, 0.7295, 0.7018, 0.6079], device='cuda:0')\n",
      "train_recall: tensor([0.6027, 0.7976, 0.7092, 0.1944], device='cuda:0')\n",
      "val_loss: 0.7948394363509153\n",
      "val_precision: tensor([0.6515, 0.6886, 0.6658, 0.8037], device='cuda:0')\n",
      "val_recall: tensor([0.4977, 0.8229, 0.7452, 0.1751], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.8481101370112781\n",
      "train_precision: tensor([0.5724, 0.6981, 0.6691, 0.6204], device='cuda:0')\n",
      "train_recall: tensor([0.5589, 0.7823, 0.6612, 0.0922], device='cuda:0')\n",
      "val_loss: 0.8735789980674326\n",
      "val_precision: tensor([0.6060, 0.6871, 0.6266, 0.7849], device='cuda:0')\n",
      "val_recall: tensor([0.4776, 0.8097, 0.7050, 0.0663], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7751819397676437\n",
      "train_precision: tensor([0.6098, 0.7298, 0.6969, 0.6075], device='cuda:0')\n",
      "train_recall: tensor([0.5980, 0.7942, 0.6954, 0.2022], device='cuda:0')\n",
      "val_loss: 0.7217342962796551\n",
      "val_precision: tensor([0.6045, 0.7586, 0.7602, 0.7304], device='cuda:0')\n",
      "val_recall: tensor([0.7033, 0.7819, 0.6594, 0.2828], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7203208600773531\n",
      "train_precision: tensor([0.6411, 0.7529, 0.7160, 0.6004], device='cuda:0')\n",
      "train_recall: tensor([0.6196, 0.8056, 0.7242, 0.3137], device='cuda:0')\n",
      "val_loss: 0.6973197650433776\n",
      "val_precision: tensor([0.6763, 0.7341, 0.7230, 0.7733], device='cuda:0')\n",
      "val_recall: tensor([0.6057, 0.8332, 0.7426, 0.3263], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6937937682325189\n",
      "train_precision: tensor([0.6575, 0.7633, 0.7258, 0.6077], device='cuda:0')\n",
      "train_recall: tensor([0.6363, 0.8092, 0.7336, 0.3722], device='cuda:0')\n",
      "val_loss: 0.687087258421572\n",
      "val_precision: tensor([0.7317, 0.7279, 0.6881, 0.7073], device='cuda:0')\n",
      "val_recall: tensor([0.5044, 0.8694, 0.8025, 0.3539], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6734339383196704\n",
      "train_precision: tensor([0.6697, 0.7738, 0.7325, 0.6264], device='cuda:0')\n",
      "train_recall: tensor([0.6488, 0.8167, 0.7416, 0.3973], device='cuda:0')\n",
      "val_loss: 0.6874577467726948\n",
      "val_precision: tensor([0.6727, 0.7584, 0.7336, 0.4826], device='cuda:0')\n",
      "val_recall: tensor([0.6134, 0.8281, 0.7212, 0.5461], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7317189551292256\n",
      "train_precision: tensor([0.6392, 0.7475, 0.7123, 0.5969], device='cuda:0')\n",
      "train_recall: tensor([0.6183, 0.7989, 0.7217, 0.3017], device='cuda:0')\n",
      "val_loss: 0.6781552740166015\n",
      "val_precision: tensor([0.6782, 0.7766, 0.7061, 0.7024], device='cuda:0')\n",
      "val_recall: tensor([0.6091, 0.8168, 0.7775, 0.3441], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7844701963949968\n",
      "train_precision: tensor([0.6177, 0.7173, 0.6896, 0.6073], device='cuda:0')\n",
      "train_recall: tensor([0.5954, 0.7874, 0.6930, 0.2030], device='cuda:0')\n",
      "val_loss: 0.7194860214158484\n",
      "val_precision: tensor([0.6575, 0.7924, 0.6716, 0.7136], device='cuda:0')\n",
      "val_recall: tensor([0.5843, 0.7592, 0.8130, 0.3054], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7017829775810241\n",
      "train_precision: tensor([0.6589, 0.7566, 0.7225, 0.6284], device='cuda:0')\n",
      "train_recall: tensor([0.6321, 0.8096, 0.7323, 0.3592], device='cuda:0')\n",
      "val_loss: 0.8173293718821985\n",
      "val_precision: tensor([0.7079, 0.6381, 0.7188, 0.3212], device='cuda:0')\n",
      "val_recall: tensor([0.4291, 0.8710, 0.6920, 0.5343], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6966438037507674\n",
      "train_precision: tensor([0.6586, 0.7665, 0.7181, 0.6320], device='cuda:0')\n",
      "train_recall: tensor([0.6324, 0.8114, 0.7346, 0.3683], device='cuda:0')\n",
      "val_loss: 0.6653869556369925\n",
      "val_precision: tensor([0.6577, 0.8253, 0.7182, 0.7335], device='cuda:0')\n",
      "val_recall: tensor([0.6895, 0.7505, 0.7816, 0.3966], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6661274442698228\n",
      "train_precision: tensor([0.6736, 0.7759, 0.7341, 0.6368], device='cuda:0')\n",
      "train_recall: tensor([0.6535, 0.8163, 0.7443, 0.4058], device='cuda:0')\n",
      "val_loss: 0.6454862770891546\n",
      "val_precision: tensor([0.6575, 0.7967, 0.7582, 0.8213], device='cuda:0')\n",
      "val_recall: tensor([0.7190, 0.8132, 0.7156, 0.3434], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6635295415944594\n",
      "train_precision: tensor([0.6766, 0.7791, 0.7319, 0.6505], device='cuda:0')\n",
      "train_recall: tensor([0.6530, 0.8176, 0.7469, 0.4229], device='cuda:0')\n",
      "val_loss: 0.7005145850026994\n",
      "val_precision: tensor([0.6065, 0.8493, 0.7020, 0.7384], device='cuda:0')\n",
      "val_recall: tensor([0.7055, 0.7110, 0.7414, 0.3212], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6691247324892544\n",
      "train_precision: tensor([0.6702, 0.7744, 0.7335, 0.6466], device='cuda:0')\n",
      "train_recall: tensor([0.6516, 0.8160, 0.7403, 0.4177], device='cuda:0')\n",
      "val_loss: 0.6408754871819085\n",
      "val_precision: tensor([0.7097, 0.7930, 0.7309, 0.5583], device='cuda:0')\n",
      "val_recall: tensor([0.6435, 0.8193, 0.7711, 0.5869], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6387807941054278\n",
      "train_precision: tensor([0.6847, 0.7871, 0.7467, 0.6657], device='cuda:0')\n",
      "train_recall: tensor([0.6695, 0.8239, 0.7513, 0.4606], device='cuda:0')\n",
      "val_loss: 0.6528538900271913\n",
      "val_precision: tensor([0.6959, 0.8580, 0.6630, 0.7374], device='cuda:0')\n",
      "val_recall: tensor([0.6027, 0.7397, 0.8688, 0.4519], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6295727701748118\n",
      "train_precision: tensor([0.6898, 0.7897, 0.7495, 0.6596], device='cuda:0')\n",
      "train_recall: tensor([0.6713, 0.8263, 0.7559, 0.4750], device='cuda:0')\n",
      "val_loss: 0.7106674549882847\n",
      "val_precision: tensor([0.5866, 0.8281, 0.7596, 0.8602], device='cuda:0')\n",
      "val_recall: tensor([0.7557, 0.7111, 0.6973, 0.3003], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.7101692696306158\n",
      "train_precision: tensor([0.6518, 0.7503, 0.7211, 0.6318], device='cuda:0')\n",
      "train_recall: tensor([0.6171, 0.8106, 0.7322, 0.3629], device='cuda:0')\n",
      "val_loss: 0.6514950572255246\n",
      "val_precision: tensor([0.6798, 0.8210, 0.7162, 0.6518], device='cuda:0')\n",
      "val_recall: tensor([0.6860, 0.7669, 0.7729, 0.5061], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6392878133345415\n",
      "train_precision: tensor([0.6845, 0.7845, 0.7494, 0.6603], device='cuda:0')\n",
      "train_recall: tensor([0.6677, 0.8228, 0.7528, 0.4698], device='cuda:0')\n",
      "val_loss: 0.6387564959817397\n",
      "val_precision: tensor([0.6924, 0.8411, 0.6926, 0.7739], device='cuda:0')\n",
      "val_recall: tensor([0.6543, 0.7640, 0.8235, 0.4461], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6195954520434619\n",
      "train_precision: tensor([0.6963, 0.7938, 0.7577, 0.6672], device='cuda:0')\n",
      "train_recall: tensor([0.6806, 0.8278, 0.7612, 0.5003], device='cuda:0')\n",
      "val_loss: 0.6258697540980027\n",
      "val_precision: tensor([0.6984, 0.8494, 0.6968, 0.7373], device='cuda:0')\n",
      "val_recall: tensor([0.6464, 0.7578, 0.8438, 0.5263], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6246756099762126\n",
      "train_precision: tensor([0.6932, 0.7901, 0.7555, 0.6686], device='cuda:0')\n",
      "train_recall: tensor([0.6750, 0.8279, 0.7580, 0.5025], device='cuda:0')\n",
      "val_loss: 0.6039667700294248\n",
      "val_precision: tensor([0.6989, 0.7914, 0.7610, 0.7848], device='cuda:0')\n",
      "val_recall: tensor([0.6623, 0.8523, 0.7736, 0.4616], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.609951755101668\n",
      "train_precision: tensor([0.6988, 0.7956, 0.7622, 0.6818], device='cuda:0')\n",
      "train_recall: tensor([0.6832, 0.8301, 0.7630, 0.5316], device='cuda:0')\n",
      "val_loss: 0.640729546101016\n",
      "val_precision: tensor([0.6762, 0.8647, 0.6917, 0.7087], device='cuda:0')\n",
      "val_recall: tensor([0.6435, 0.7438, 0.8412, 0.5104], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6108954279180516\n",
      "train_precision: tensor([0.6950, 0.7981, 0.7618, 0.6745], device='cuda:0')\n",
      "train_recall: tensor([0.6845, 0.8292, 0.7600, 0.5296], device='cuda:0')\n",
      "val_loss: 0.6178438390728244\n",
      "val_precision: tensor([0.7032, 0.8564, 0.6998, 0.6887], device='cuda:0')\n",
      "val_recall: tensor([0.6441, 0.7642, 0.8420, 0.6101], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.6178978782286618\n",
      "train_precision: tensor([0.6905, 0.7967, 0.7569, 0.6723], device='cuda:0')\n",
      "train_recall: tensor([0.6789, 0.8283, 0.7574, 0.5137], device='cuda:0')\n",
      "val_loss: 0.5848584728050708\n",
      "val_precision: tensor([0.7063, 0.8184, 0.7641, 0.7313], device='cuda:0')\n",
      "val_recall: tensor([0.6957, 0.8247, 0.7872, 0.5626], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5971935176275631\n",
      "train_precision: tensor([0.7035, 0.8027, 0.7684, 0.6777], device='cuda:0')\n",
      "train_recall: tensor([0.6928, 0.8335, 0.7664, 0.5382], device='cuda:0')\n",
      "val_loss: 0.6260112722467008\n",
      "val_precision: tensor([0.6897, 0.8469, 0.7493, 0.6981], device='cuda:0')\n",
      "val_recall: tensor([0.7133, 0.7854, 0.7910, 0.5778], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5889443092805179\n",
      "train_precision: tensor([0.7069, 0.8049, 0.7692, 0.6913], device='cuda:0')\n",
      "train_recall: tensor([0.6931, 0.8353, 0.7689, 0.5681], device='cuda:0')\n",
      "val_loss: 0.5918194455398883\n",
      "val_precision: tensor([0.7640, 0.7793, 0.7318, 0.7291], device='cuda:0')\n",
      "val_recall: tensor([0.5857, 0.8763, 0.8273, 0.5717], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5940510850857923\n",
      "train_precision: tensor([0.7050, 0.8047, 0.7677, 0.6885], device='cuda:0')\n",
      "train_recall: tensor([0.6927, 0.8340, 0.7668, 0.5662], device='cuda:0')\n",
      "val_loss: 0.5877963180405243\n",
      "val_precision: tensor([0.6972, 0.8185, 0.7866, 0.6252], device='cuda:0')\n",
      "val_recall: tensor([0.7294, 0.8200, 0.7478, 0.6337], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5813113475547117\n",
      "train_precision: tensor([0.7126, 0.8098, 0.7727, 0.6900], device='cuda:0')\n",
      "train_recall: tensor([0.6991, 0.8366, 0.7738, 0.5835], device='cuda:0')\n",
      "val_loss: 0.5916333372456177\n",
      "val_precision: tensor([0.6932, 0.8502, 0.7496, 0.7121], device='cuda:0')\n",
      "val_recall: tensor([0.7118, 0.7773, 0.8087, 0.5697], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5823727060129298\n",
      "train_precision: tensor([0.7102, 0.8087, 0.7737, 0.6898], device='cuda:0')\n",
      "train_recall: tensor([0.6994, 0.8371, 0.7704, 0.5825], device='cuda:0')\n",
      "val_loss: 0.6002226806712567\n",
      "val_precision: tensor([0.7280, 0.7490, 0.7865, 0.8014], device='cuda:0')\n",
      "val_recall: tensor([0.6438, 0.8951, 0.7520, 0.5162], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5954088496333138\n",
      "train_precision: tensor([0.7025, 0.8051, 0.7705, 0.6926], device='cuda:0')\n",
      "train_recall: tensor([0.6942, 0.8321, 0.7681, 0.5631], device='cuda:0')\n",
      "val_loss: 0.5932688646185725\n",
      "val_precision: tensor([0.6838, 0.8286, 0.7721, 0.7820], device='cuda:0')\n",
      "val_recall: tensor([0.7302, 0.8071, 0.7657, 0.5182], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5722403038313044\n",
      "train_precision: tensor([0.7147, 0.8125, 0.7775, 0.6908], device='cuda:0')\n",
      "train_recall: tensor([0.7035, 0.8384, 0.7758, 0.5938], device='cuda:0')\n",
      "val_loss: 0.584513558302437\n",
      "val_precision: tensor([0.7735, 0.7509, 0.7629, 0.8083], device='cuda:0')\n",
      "val_recall: tensor([0.6127, 0.8961, 0.8029, 0.5027], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5668731897272529\n",
      "train_precision: tensor([0.7183, 0.8125, 0.7801, 0.7044], device='cuda:0')\n",
      "train_recall: tensor([0.7063, 0.8400, 0.7777, 0.6051], device='cuda:0')\n",
      "val_loss: 0.5716465172030385\n",
      "val_precision: tensor([0.6870, 0.8489, 0.7848, 0.7080], device='cuda:0')\n",
      "val_recall: tensor([0.7503, 0.7921, 0.7730, 0.6350], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5768798214866516\n",
      "train_precision: tensor([0.7102, 0.8106, 0.7770, 0.6977], device='cuda:0')\n",
      "train_recall: tensor([0.7017, 0.8382, 0.7716, 0.5919], device='cuda:0')\n",
      "val_loss: 0.6167059966602231\n",
      "val_precision: tensor([0.7321, 0.8163, 0.7031, 0.7770], device='cuda:0')\n",
      "val_recall: tensor([0.6149, 0.8177, 0.8464, 0.4236], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5717711557678998\n",
      "train_precision: tensor([0.7154, 0.8124, 0.7797, 0.6928], device='cuda:0')\n",
      "train_recall: tensor([0.7047, 0.8386, 0.7769, 0.5996], device='cuda:0')\n",
      "val_loss: 0.577761085104764\n",
      "val_precision: tensor([0.6639, 0.8155, 0.8410, 0.7738], device='cuda:0')\n",
      "val_recall: tensor([0.7902, 0.8356, 0.6841, 0.5552], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5685991417277944\n",
      "train_precision: tensor([0.7154, 0.8126, 0.7818, 0.7004], device='cuda:0')\n",
      "train_recall: tensor([0.7067, 0.8402, 0.7760, 0.5991], device='cuda:0')\n",
      "val_loss: 0.5859145467864011\n",
      "val_precision: tensor([0.7407, 0.8378, 0.7191, 0.6891], device='cuda:0')\n",
      "val_recall: tensor([0.6459, 0.8137, 0.8408, 0.6030], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.562429120817924\n",
      "train_precision: tensor([0.7186, 0.8131, 0.7814, 0.7106], device='cuda:0')\n",
      "train_recall: tensor([0.7052, 0.8407, 0.7794, 0.6205], device='cuda:0')\n",
      "val_loss: 0.5527290495999734\n",
      "val_precision: tensor([0.7311, 0.8117, 0.7785, 0.8079], device='cuda:0')\n",
      "val_recall: tensor([0.7024, 0.8515, 0.7942, 0.5663], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5519909346167416\n",
      "train_precision: tensor([0.7249, 0.8196, 0.7870, 0.7058], device='cuda:0')\n",
      "train_recall: tensor([0.7166, 0.8433, 0.7826, 0.6224], device='cuda:0')\n",
      "val_loss: 0.5995570676879692\n",
      "val_precision: tensor([0.6945, 0.8800, 0.7203, 0.8345], device='cuda:0')\n",
      "val_recall: tensor([0.7139, 0.7406, 0.8415, 0.5178], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5511986262020581\n",
      "train_precision: tensor([0.7236, 0.8187, 0.7882, 0.7112], device='cuda:0')\n",
      "train_recall: tensor([0.7152, 0.8438, 0.7826, 0.6276], device='cuda:0')\n",
      "val_loss: 0.5661183303579725\n",
      "val_precision: tensor([0.6991, 0.8383, 0.7944, 0.6507], device='cuda:0')\n",
      "val_recall: tensor([0.7446, 0.8100, 0.7649, 0.6886], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5622343998223065\n",
      "train_precision: tensor([0.7195, 0.8141, 0.7843, 0.7047], device='cuda:0')\n",
      "train_recall: tensor([0.7105, 0.8418, 0.7774, 0.6146], device='cuda:0')\n",
      "val_loss: 0.5705989724085515\n",
      "val_precision: tensor([0.7131, 0.7741, 0.8260, 0.7259], device='cuda:0')\n",
      "val_recall: tensor([0.7149, 0.8824, 0.7213, 0.6108], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5501278846659124\n",
      "train_precision: tensor([0.7254, 0.8184, 0.7883, 0.7098], device='cuda:0')\n",
      "train_recall: tensor([0.7165, 0.8433, 0.7829, 0.6289], device='cuda:0')\n",
      "val_loss: 0.6455898917746961\n",
      "val_precision: tensor([0.7211, 0.7016, 0.8661, 0.5179], device='cuda:0')\n",
      "val_recall: tensor([0.6645, 0.9299, 0.6316, 0.6418], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5456852515432287\n",
      "train_precision: tensor([0.7294, 0.8207, 0.7894, 0.7184], device='cuda:0')\n",
      "train_recall: tensor([0.7189, 0.8462, 0.7857, 0.6328], device='cuda:0')\n",
      "val_loss: 0.5640346009163488\n",
      "val_precision: tensor([0.7470, 0.7644, 0.8136, 0.6681], device='cuda:0')\n",
      "val_recall: tensor([0.6774, 0.8922, 0.7490, 0.7034], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5530032573536755\n",
      "train_precision: tensor([0.7214, 0.8194, 0.7862, 0.7038], device='cuda:0')\n",
      "train_recall: tensor([0.7144, 0.8428, 0.7804, 0.6237], device='cuda:0')\n",
      "val_loss: 0.7104600612213486\n",
      "val_precision: tensor([0.5829, 0.9314, 0.7358, 0.7493], device='cuda:0')\n",
      "val_recall: tensor([0.7868, 0.5608, 0.7992, 0.4640], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5432531864247857\n",
      "train_precision: tensor([0.7283, 0.8202, 0.7914, 0.7236], device='cuda:0')\n",
      "train_recall: tensor([0.7209, 0.8432, 0.7866, 0.6380], device='cuda:0')\n",
      "val_loss: 0.5698120209047027\n",
      "val_precision: tensor([0.7485, 0.8323, 0.7363, 0.7557], device='cuda:0')\n",
      "val_recall: tensor([0.6702, 0.8288, 0.8302, 0.6145], device='cuda:0')\n",
      "Learning rate: 0.003305526571026246\n",
      "train_loss: 0.5388265213226889\n",
      "train_precision: tensor([0.7318, 0.8221, 0.7919, 0.7194], device='cuda:0')\n",
      "train_recall: tensor([0.7201, 0.8472, 0.7884, 0.6460], device='cuda:0')\n",
      "val_loss: 0.5589824693458634\n",
      "val_precision: tensor([0.7563, 0.8343, 0.7314, 0.8190], device='cuda:0')\n",
      "val_recall: tensor([0.6590, 0.8294, 0.8548, 0.5407], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.49997561051246314\n",
      "train_precision: tensor([0.7511, 0.8352, 0.8081, 0.7530], device='cuda:0')\n",
      "train_recall: tensor([0.7407, 0.8568, 0.8035, 0.7048], device='cuda:0')\n",
      "val_loss: 0.5130678164096841\n",
      "val_precision: tensor([0.7534, 0.8346, 0.7889, 0.7851], device='cuda:0')\n",
      "val_recall: tensor([0.7207, 0.8489, 0.8225, 0.6582], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.48942512219602413\n",
      "train_precision: tensor([0.7559, 0.8388, 0.8131, 0.7665], device='cuda:0')\n",
      "train_recall: tensor([0.7471, 0.8590, 0.8078, 0.7196], device='cuda:0')\n",
      "val_loss: 0.525097668988449\n",
      "val_precision: tensor([0.7517, 0.8598, 0.7594, 0.8009], device='cuda:0')\n",
      "val_recall: tensor([0.7114, 0.8191, 0.8528, 0.6229], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.48632630578336866\n",
      "train_precision: tensor([0.7567, 0.8396, 0.8144, 0.7642], device='cuda:0')\n",
      "train_recall: tensor([0.7482, 0.8594, 0.8087, 0.7244], device='cuda:0')\n",
      "val_loss: 0.5117254763321389\n",
      "val_precision: tensor([0.7569, 0.8227, 0.8002, 0.7559], device='cuda:0')\n",
      "val_recall: tensor([0.7169, 0.8630, 0.8114, 0.6788], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4828161666737521\n",
      "train_precision: tensor([0.7581, 0.8420, 0.8155, 0.7655], device='cuda:0')\n",
      "train_recall: tensor([0.7511, 0.8607, 0.8087, 0.7297], device='cuda:0')\n",
      "val_loss: 0.5080478426970151\n",
      "val_precision: tensor([0.7566, 0.8449, 0.7830, 0.7899], device='cuda:0')\n",
      "val_recall: tensor([0.7195, 0.8434, 0.8360, 0.6569], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.48064201885366187\n",
      "train_precision: tensor([0.7584, 0.8427, 0.8154, 0.7714], device='cuda:0')\n",
      "train_recall: tensor([0.7508, 0.8617, 0.8093, 0.7322], device='cuda:0')\n",
      "val_loss: 0.5063917784768149\n",
      "val_precision: tensor([0.7404, 0.8464, 0.8058, 0.7617], device='cuda:0')\n",
      "val_recall: tensor([0.7493, 0.8401, 0.8092, 0.6943], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4793419485423654\n",
      "train_precision: tensor([0.7600, 0.8429, 0.8185, 0.7702], device='cuda:0')\n",
      "train_recall: tensor([0.7541, 0.8626, 0.8095, 0.7345], device='cuda:0')\n",
      "val_loss: 0.5062027471767102\n",
      "val_precision: tensor([0.7433, 0.8531, 0.7962, 0.7738], device='cuda:0')\n",
      "val_recall: tensor([0.7480, 0.8321, 0.8194, 0.6899], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.47662647172091477\n",
      "train_precision: tensor([0.7607, 0.8435, 0.8180, 0.7741], device='cuda:0')\n",
      "train_recall: tensor([0.7531, 0.8628, 0.8110, 0.7405], device='cuda:0')\n",
      "val_loss: 0.5068016675047744\n",
      "val_precision: tensor([0.7529, 0.8235, 0.8140, 0.7500], device='cuda:0')\n",
      "val_recall: tensor([0.7333, 0.8666, 0.7991, 0.6899], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4759521877383166\n",
      "train_precision: tensor([0.7616, 0.8435, 0.8184, 0.7761], device='cuda:0')\n",
      "train_recall: tensor([0.7534, 0.8637, 0.8116, 0.7388], device='cuda:0')\n",
      "val_loss: 0.5125978594260323\n",
      "val_precision: tensor([0.7339, 0.8657, 0.7908, 0.7695], device='cuda:0')\n",
      "val_recall: tensor([0.7524, 0.8145, 0.8259, 0.6892], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.473233476711467\n",
      "train_precision: tensor([0.7621, 0.8456, 0.8194, 0.7736], device='cuda:0')\n",
      "train_recall: tensor([0.7570, 0.8627, 0.8115, 0.7436], device='cuda:0')\n",
      "val_loss: 0.5218132299526671\n",
      "val_precision: tensor([0.7390, 0.8765, 0.7665, 0.7947], device='cuda:0')\n",
      "val_recall: tensor([0.7402, 0.7969, 0.8485, 0.6529], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4733562227238946\n",
      "train_precision: tensor([0.7632, 0.8457, 0.8187, 0.7774], device='cuda:0')\n",
      "train_recall: tensor([0.7553, 0.8630, 0.8137, 0.7481], device='cuda:0')\n",
      "val_loss: 0.5057191998881295\n",
      "val_precision: tensor([0.7541, 0.8408, 0.7953, 0.7895], device='cuda:0')\n",
      "val_recall: tensor([0.7317, 0.8475, 0.8246, 0.6707], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.47108598603284296\n",
      "train_precision: tensor([0.7645, 0.8463, 0.8202, 0.7748], device='cuda:0')\n",
      "train_recall: tensor([0.7575, 0.8637, 0.8137, 0.7488], device='cuda:0')\n",
      "val_loss: 0.5049587940783274\n",
      "val_precision: tensor([0.7468, 0.8520, 0.7960, 0.8059], device='cuda:0')\n",
      "val_recall: tensor([0.7492, 0.8361, 0.8224, 0.6640], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.47141350392989295\n",
      "train_precision: tensor([0.7638, 0.8462, 0.8207, 0.7766], device='cuda:0')\n",
      "train_recall: tensor([0.7577, 0.8637, 0.8133, 0.7468], device='cuda:0')\n",
      "val_loss: 0.5115343897865895\n",
      "val_precision: tensor([0.7261, 0.8749, 0.7945, 0.7847], device='cuda:0')\n",
      "val_recall: tensor([0.7704, 0.8040, 0.8223, 0.6677], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.46827926138505577\n",
      "train_precision: tensor([0.7640, 0.8478, 0.8223, 0.7817], device='cuda:0')\n",
      "train_recall: tensor([0.7584, 0.8659, 0.8139, 0.7506], device='cuda:0')\n",
      "val_loss: 0.5081372204564159\n",
      "val_precision: tensor([0.7455, 0.8537, 0.7911, 0.8271], device='cuda:0')\n",
      "val_recall: tensor([0.7440, 0.8323, 0.8313, 0.6313], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4681050054848513\n",
      "train_precision: tensor([0.7651, 0.8486, 0.8205, 0.7823], device='cuda:0')\n",
      "train_recall: tensor([0.7595, 0.8644, 0.8142, 0.7535], device='cuda:0')\n",
      "val_loss: 0.5075895261586159\n",
      "val_precision: tensor([0.7540, 0.8547, 0.7822, 0.8261], device='cuda:0')\n",
      "val_recall: tensor([0.7342, 0.8326, 0.8414, 0.6320], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.46834848523139955\n",
      "train_precision: tensor([0.7648, 0.8473, 0.8206, 0.7787], device='cuda:0')\n",
      "train_recall: tensor([0.7584, 0.8654, 0.8132, 0.7476], device='cuda:0')\n",
      "val_loss: 0.5019312516560875\n",
      "val_precision: tensor([0.7436, 0.8527, 0.8051, 0.7896], device='cuda:0')\n",
      "val_recall: tensor([0.7560, 0.8380, 0.8175, 0.6724], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.46474375237118115\n",
      "train_precision: tensor([0.7657, 0.8480, 0.8227, 0.7762], device='cuda:0')\n",
      "train_recall: tensor([0.7603, 0.8653, 0.8150, 0.7452], device='cuda:0')\n",
      "val_loss: 0.5087688177303781\n",
      "val_precision: tensor([0.7495, 0.8607, 0.7782, 0.8045], device='cuda:0')\n",
      "val_recall: tensor([0.7348, 0.8239, 0.8418, 0.6485], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.466130562134605\n",
      "train_precision: tensor([0.7656, 0.8469, 0.8239, 0.7828], device='cuda:0')\n",
      "train_recall: tensor([0.7597, 0.8661, 0.8149, 0.7517], device='cuda:0')\n",
      "val_loss: 0.5133509293310066\n",
      "val_precision: tensor([0.7571, 0.8597, 0.7686, 0.8186], device='cuda:0')\n",
      "val_recall: tensor([0.7200, 0.8241, 0.8552, 0.6367], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.46523595484182795\n",
      "train_precision: tensor([0.7663, 0.8495, 0.8225, 0.7813], device='cuda:0')\n",
      "train_recall: tensor([0.7605, 0.8653, 0.8162, 0.7556], device='cuda:0')\n",
      "val_loss: 0.5019593496720987\n",
      "val_precision: tensor([0.7557, 0.8299, 0.8101, 0.7520], device='cuda:0')\n",
      "val_recall: tensor([0.7316, 0.8619, 0.8099, 0.7044], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.464277227677126\n",
      "train_precision: tensor([0.7673, 0.8475, 0.8237, 0.7842], device='cuda:0')\n",
      "train_recall: tensor([0.7611, 0.8659, 0.8156, 0.7540], device='cuda:0')\n",
      "val_loss: 0.5024919014173256\n",
      "val_precision: tensor([0.7404, 0.8506, 0.8068, 0.7995], device='cuda:0')\n",
      "val_recall: tensor([0.7604, 0.8364, 0.8127, 0.6593], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4635918451502999\n",
      "train_precision: tensor([0.7673, 0.8495, 0.8240, 0.7846], device='cuda:0')\n",
      "train_recall: tensor([0.7616, 0.8662, 0.8172, 0.7522], device='cuda:0')\n",
      "val_loss: 0.5068077771295039\n",
      "val_precision: tensor([0.7738, 0.8105, 0.8154, 0.7127], device='cuda:0')\n",
      "val_recall: tensor([0.7102, 0.8814, 0.8067, 0.7508], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4611380223284431\n",
      "train_precision: tensor([0.7688, 0.8497, 0.8245, 0.7801], device='cuda:0')\n",
      "train_recall: tensor([0.7624, 0.8667, 0.8172, 0.7583], device='cuda:0')\n",
      "val_loss: 0.499107351326883\n",
      "val_precision: tensor([0.7509, 0.8378, 0.8132, 0.7877], device='cuda:0')\n",
      "val_recall: tensor([0.7474, 0.8554, 0.8114, 0.6758], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.46034510003372947\n",
      "train_precision: tensor([0.7683, 0.8491, 0.8260, 0.7820], device='cuda:0')\n",
      "train_recall: tensor([0.7631, 0.8674, 0.8161, 0.7599], device='cuda:0')\n",
      "val_loss: 0.5044150549426043\n",
      "val_precision: tensor([0.7607, 0.8496, 0.7836, 0.8244], device='cuda:0')\n",
      "val_recall: tensor([0.7285, 0.8421, 0.8426, 0.6246], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.45921201329817746\n",
      "train_precision: tensor([0.7688, 0.8505, 0.8253, 0.7876], device='cuda:0')\n",
      "train_recall: tensor([0.7653, 0.8646, 0.8180, 0.7631], device='cuda:0')\n",
      "val_loss: 0.5044914173366423\n",
      "val_precision: tensor([0.7611, 0.8313, 0.8018, 0.7948], device='cuda:0')\n",
      "val_recall: tensor([0.7271, 0.8639, 0.8205, 0.6522], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.45783642453943346\n",
      "train_precision: tensor([0.7691, 0.8507, 0.8264, 0.7917], device='cuda:0')\n",
      "train_recall: tensor([0.7652, 0.8667, 0.8180, 0.7633], device='cuda:0')\n",
      "val_loss: 0.5074523697024271\n",
      "val_precision: tensor([0.7512, 0.8637, 0.7812, 0.7782], device='cuda:0')\n",
      "val_recall: tensor([0.7392, 0.8222, 0.8408, 0.6838], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.45830598922974286\n",
      "train_precision: tensor([0.7701, 0.8495, 0.8258, 0.7889], device='cuda:0')\n",
      "train_recall: tensor([0.7636, 0.8659, 0.8191, 0.7659], device='cuda:0')\n",
      "val_loss: 0.5036578699389004\n",
      "val_precision: tensor([0.7501, 0.8512, 0.7942, 0.8130], device='cuda:0')\n",
      "val_recall: tensor([0.7435, 0.8380, 0.8288, 0.6559], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.45708038373426957\n",
      "train_precision: tensor([0.7700, 0.8504, 0.8254, 0.7885], device='cuda:0')\n",
      "train_recall: tensor([0.7637, 0.8665, 0.8192, 0.7622], device='cuda:0')\n",
      "val_loss: 0.5039814101193016\n",
      "val_precision: tensor([0.7430, 0.8156, 0.8431, 0.7702], device='cuda:0')\n",
      "val_recall: tensor([0.7566, 0.8790, 0.7706, 0.6929], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.45641103230695673\n",
      "train_precision: tensor([0.7708, 0.8512, 0.8268, 0.7928], device='cuda:0')\n",
      "train_recall: tensor([0.7652, 0.8681, 0.8191, 0.7668], device='cuda:0')\n",
      "val_loss: 0.49940281199695463\n",
      "val_precision: tensor([0.7549, 0.8434, 0.7998, 0.7840], device='cuda:0')\n",
      "val_recall: tensor([0.7367, 0.8480, 0.8260, 0.6734], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.45704284849013876\n",
      "train_precision: tensor([0.7701, 0.8490, 0.8269, 0.7870], device='cuda:0')\n",
      "train_recall: tensor([0.7634, 0.8661, 0.8197, 0.7657], device='cuda:0')\n",
      "val_loss: 0.5020047198804537\n",
      "val_precision: tensor([0.7544, 0.8256, 0.8152, 0.8168], device='cuda:0')\n",
      "val_recall: tensor([0.7402, 0.8683, 0.8064, 0.6364], device='cuda:0')\n",
      "Learning rate: 0.0003305526571026246\n",
      "train_loss: 0.4564521902703984\n",
      "train_precision: tensor([0.7716, 0.8514, 0.8272, 0.7852], device='cuda:0')\n",
      "train_recall: tensor([0.7645, 0.8688, 0.8200, 0.7655], device='cuda:0')\n",
      "val_loss: 0.5039717527547679\n",
      "val_precision: tensor([0.7542, 0.8515, 0.7905, 0.7924], device='cuda:0')\n",
      "val_recall: tensor([0.7358, 0.8386, 0.8342, 0.6684], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.44960069904990374\n",
      "train_precision: tensor([0.7752, 0.8540, 0.8284, 0.7984], device='cuda:0')\n",
      "train_recall: tensor([0.7682, 0.8693, 0.8233, 0.7771], device='cuda:0')\n",
      "val_loss: 0.49818662960927684\n",
      "val_precision: tensor([0.7667, 0.8316, 0.8018, 0.7952], device='cuda:0')\n",
      "val_recall: tensor([0.7255, 0.8633, 0.8260, 0.6785], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4476956573080889\n",
      "train_precision: tensor([0.7745, 0.8545, 0.8305, 0.7955], device='cuda:0')\n",
      "train_recall: tensor([0.7696, 0.8699, 0.8222, 0.7815], device='cuda:0')\n",
      "val_loss: 0.5038600208010162\n",
      "val_precision: tensor([0.7531, 0.8586, 0.7844, 0.7926], device='cuda:0')\n",
      "val_recall: tensor([0.7364, 0.8297, 0.8399, 0.6741], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4480122338004291\n",
      "train_precision: tensor([0.7737, 0.8545, 0.8302, 0.7940], device='cuda:0')\n",
      "train_recall: tensor([0.7686, 0.8700, 0.8223, 0.7778], device='cuda:0')\n",
      "val_loss: 0.5053501419741614\n",
      "val_precision: tensor([0.7661, 0.8465, 0.7803, 0.7911], device='cuda:0')\n",
      "val_recall: tensor([0.7176, 0.8437, 0.8445, 0.6670], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.44656700023355334\n",
      "train_precision: tensor([0.7750, 0.8543, 0.8295, 0.7897], device='cuda:0')\n",
      "train_recall: tensor([0.7691, 0.8700, 0.8220, 0.7755], device='cuda:0')\n",
      "val_loss: 0.5011309843854119\n",
      "val_precision: tensor([0.7536, 0.8524, 0.7950, 0.7982], device='cuda:0')\n",
      "val_recall: tensor([0.7422, 0.8394, 0.8321, 0.6687], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4464028824459423\n",
      "train_precision: tensor([0.7746, 0.8547, 0.8295, 0.7968], device='cuda:0')\n",
      "train_recall: tensor([0.7696, 0.8704, 0.8222, 0.7723], device='cuda:0')\n",
      "val_loss: 0.501607033231312\n",
      "val_precision: tensor([0.7574, 0.8501, 0.7915, 0.7945], device='cuda:0')\n",
      "val_recall: tensor([0.7336, 0.8425, 0.8351, 0.6768], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4467771144792995\n",
      "train_precision: tensor([0.7749, 0.8551, 0.8313, 0.7981], device='cuda:0')\n",
      "train_recall: tensor([0.7714, 0.8715, 0.8209, 0.7808], device='cuda:0')\n",
      "val_loss: 0.5004241483615818\n",
      "val_precision: tensor([0.7501, 0.8584, 0.7961, 0.7720], device='cuda:0')\n",
      "val_recall: tensor([0.7488, 0.8304, 0.8303, 0.7044], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4473153609643008\n",
      "train_precision: tensor([0.7757, 0.8544, 0.8297, 0.7944], device='cuda:0')\n",
      "train_recall: tensor([0.7693, 0.8697, 0.8238, 0.7745], device='cuda:0')\n",
      "val_loss: 0.5084720743713236\n",
      "val_precision: tensor([0.7527, 0.8499, 0.7869, 0.8122], device='cuda:0')\n",
      "val_recall: tensor([0.7359, 0.8361, 0.8328, 0.6508], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4478343534597101\n",
      "train_precision: tensor([0.7738, 0.8557, 0.8294, 0.7970], device='cuda:0')\n",
      "train_recall: tensor([0.7701, 0.8708, 0.8209, 0.7762], device='cuda:0')\n",
      "val_loss: 0.49876974786903494\n",
      "val_precision: tensor([0.7584, 0.8407, 0.8028, 0.7989], device='cuda:0')\n",
      "val_recall: tensor([0.7381, 0.8533, 0.8252, 0.6700], device='cuda:0')\n",
      "Learning rate: 3.305526571026246e-05\n",
      "train_loss: 0.4460914349492221\n",
      "train_precision: tensor([0.7752, 0.8558, 0.8304, 0.7979], device='cuda:0')\n",
      "train_recall: tensor([0.7704, 0.8722, 0.8221, 0.7752], device='cuda:0')\n",
      "val_loss: 0.5005308478847703\n",
      "val_precision: tensor([0.7515, 0.8491, 0.8017, 0.8022], device='cuda:0')\n",
      "val_recall: tensor([0.7487, 0.8417, 0.8254, 0.6650], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.4444258946467211\n",
      "train_precision: tensor([0.7755, 0.8545, 0.8305, 0.7990], device='cuda:0')\n",
      "train_recall: tensor([0.7690, 0.8718, 0.8228, 0.7788], device='cuda:0')\n",
      "val_loss: 0.5023003909504622\n",
      "val_precision: tensor([0.7670, 0.8458, 0.7869, 0.7498], device='cuda:0')\n",
      "val_recall: tensor([0.7178, 0.8467, 0.8399, 0.7175], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.44374087382765376\n",
      "train_precision: tensor([0.7768, 0.8549, 0.8312, 0.8038], device='cuda:0')\n",
      "train_recall: tensor([0.7712, 0.8718, 0.8234, 0.7792], device='cuda:0')\n",
      "val_loss: 0.4984504871088965\n",
      "val_precision: tensor([0.7626, 0.8426, 0.7939, 0.8023], device='cuda:0')\n",
      "val_recall: tensor([0.7302, 0.8510, 0.8334, 0.6640], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.44457723915895675\n",
      "train_precision: tensor([0.7762, 0.8560, 0.8305, 0.7996], device='cuda:0')\n",
      "train_recall: tensor([0.7713, 0.8716, 0.8227, 0.7779], device='cuda:0')\n",
      "val_loss: 0.501334522653399\n",
      "val_precision: tensor([0.7461, 0.8542, 0.8032, 0.7823], device='cuda:0')\n",
      "val_recall: tensor([0.7540, 0.8358, 0.8218, 0.6862], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.44481041839416013\n",
      "train_precision: tensor([0.7753, 0.8555, 0.8317, 0.7977], device='cuda:0')\n",
      "train_recall: tensor([0.7722, 0.8698, 0.8233, 0.7775], device='cuda:0')\n",
      "val_loss: 0.5001307424167147\n",
      "val_precision: tensor([0.7589, 0.8421, 0.7986, 0.8234], device='cuda:0')\n",
      "val_recall: tensor([0.7377, 0.8529, 0.8283, 0.6404], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.4439324740738792\n",
      "train_precision: tensor([0.7759, 0.8551, 0.8302, 0.7997], device='cuda:0')\n",
      "train_recall: tensor([0.7694, 0.8718, 0.8224, 0.7866], device='cuda:0')\n",
      "val_loss: 0.49891880585665715\n",
      "val_precision: tensor([0.7425, 0.8566, 0.8033, 0.7943], device='cuda:0')\n",
      "val_recall: tensor([0.7581, 0.8325, 0.8209, 0.6774], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.4436837067897307\n",
      "train_precision: tensor([0.7754, 0.8554, 0.8304, 0.8013], device='cuda:0')\n",
      "train_recall: tensor([0.7708, 0.8709, 0.8223, 0.7811], device='cuda:0')\n",
      "val_loss: 0.5013690188044028\n",
      "val_precision: tensor([0.7734, 0.8221, 0.8022, 0.7893], device='cuda:0')\n",
      "val_recall: tensor([0.7130, 0.8740, 0.8255, 0.6774], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "train_loss: 0.4441502827692797\n",
      "train_precision: tensor([0.7765, 0.8567, 0.8317, 0.7974], device='cuda:0')\n",
      "train_recall: tensor([0.7727, 0.8709, 0.8240, 0.7788], device='cuda:0')\n",
      "val_loss: 0.5021324130216441\n",
      "val_precision: tensor([0.7525, 0.8518, 0.7942, 0.8010], device='cuda:0')\n",
      "val_recall: tensor([0.7398, 0.8392, 0.8320, 0.6721], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "Early stopping at epoch 90 with best validation loss 0.49818662960927684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_name = f'efficient_net1__nopad_norm{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "set_seed(2233)\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=7, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        # Logowanie precision dla każdej klasy\n",
    "        wandb.log({'Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        # Logowanie recall dla każdej klasy\n",
    "        wandb.log({'Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        # Obliczanie głównych metryk\n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        # Logowanie głównych metryk\n",
    "        wandb.log({'main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        precision_ = main_metrics_precision\n",
    "        recall_ = main_metrics_recall\n",
    "        \n",
    "        if (precision_ + recall_) > 0:\n",
    "            f1_score_val = 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "        else:\n",
    "            f1_score_val = 0\n",
    "        \n",
    "        wandb.log({'f1_score_val': f1_score_val}, step=self.step)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "    \n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        \n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'val_main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        precision_ = main_metrics_precision\n",
    "        recall_ = main_metrics_recall\n",
    "        \n",
    "        if (precision_ + recall_) > 0:\n",
    "            f1_score_val = 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "        else:\n",
    "            f1_score_val = 0\n",
    "        \n",
    "        wandb.log({'f1_score_val': f1_score_val}, step=self.step)\n",
    "        \n",
    "        \n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate)\n",
    "my_model = my_model.to('cuda')\n",
    "early_stop_patience = 15\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(my_model.state_dict(), f\"{run_path}.pt\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d95ffee-8ed1-42d1-97f6-8d39b5e1b3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[716  99 180   5]\n",
      " [ 88 862  46   4]\n",
      " [118  42 832   8]\n",
      " [  6  10  11  73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.74      1000\n",
      "           1       0.85      0.86      0.86      1000\n",
      "           2       0.78      0.83      0.80      1000\n",
      "           3       0.81      0.73      0.77       100\n",
      "\n",
      "    accuracy                           0.80      3100\n",
      "   macro avg       0.80      0.78      0.79      3100\n",
      "weighted avg       0.80      0.80      0.80      3100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "def test_report(model, dataloader):\n",
    "    \"\"\"Prints confusion matrix for testing dataset\n",
    "    dataloader should be of batch_size=1.\"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            output = model(data)\n",
    "            label = label.numpy()\n",
    "            output = output.numpy()\n",
    "            y_pred.append(np.argmax(output))\n",
    "            y_test.append(np.argmax(label))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "testset =ImageDataset(data_path='test_data', transform=transform_test, reduce=True)\n",
    "dataloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "test_report(my_model.to('cpu'), dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bf43fc-7235-4296-9807-8411b1dfbbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test: t-statistic = 1.2247448713915892, p-value = 0.308068009250357\n",
      "Wilcoxon signed-rank test: statistic = 2.0, p-value = 0.375\n",
      "ANOVA: F-statistic = 0.12531328320802032, p-value = 0.9433148818773759\n",
      "Friedman test: chi-squared = 4.852941176470589, p-value = 0.18288770966116996\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Example data: precision values for Class 0 under different approaches\n",
    "precision_no_padding_no_norm = [0.78, 0.87, 0.80, 0.79]\n",
    "precision_padding_norm = [0.77, 0.86, 0.81, 0.76]\n",
    "precision_padding_no_norm = [0.77, 0.85, 0.79, 0.76]\n",
    "precision_no_padding_norm = [0.77, 0.85, 0.78, 0.81]\n",
    "\n",
    "# Paired t-test between two approaches\n",
    "t_stat, p_value = stats.ttest_rel(precision_no_padding_no_norm, precision_padding_norm)\n",
    "print(f\"Paired t-test: t-statistic = {t_stat}, p-value = {p_value}\")\n",
    "\n",
    "# Wilcoxon signed-rank test between two approaches\n",
    "w_stat, p_value = stats.wilcoxon(precision_no_padding_no_norm, precision_padding_norm)\n",
    "print(f\"Wilcoxon signed-rank test: statistic = {w_stat}, p-value = {p_value}\")\n",
    "\n",
    "# ANOVA across all approaches\n",
    "data = np.array([precision_no_padding_no_norm, precision_padding_norm, precision_padding_no_norm, precision_no_padding_norm])\n",
    "f_stat, p_value = stats.f_oneway(*data)\n",
    "print(f\"ANOVA: F-statistic = {f_stat}, p-value = {p_value}\")\n",
    "\n",
    "# Friedman test across all approaches\n",
    "f_stat, p_value = stats.friedmanchisquare(precision_no_padding_no_norm, precision_padding_norm, precision_padding_no_norm, precision_no_padding_norm)\n",
    "print(f\"Friedman test: chi-squared = {f_stat}, p-value = {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d2b9cd-4b74-46ba-857e-6891e4cece0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Tests:\n",
      "{'t-test (1 vs 2)': (1.2247448713915892, 0.308068009250357), 't-test (1 vs 3)': (3.655630775069654, 0.03535284700251738), 't-test (1 vs 4)': (0.7924058156930613, 0.4860036297301704), 't-test (2 vs 3)': (1.5666989036012804, 0.21516994256954994), 't-test (2 vs 4)': (-0.14691063206231755, 0.8925198609712901), 't-test (3 vs 4)': (-0.7385489458759964, 0.5137127113616159), 'Wilcoxon (1 vs 2)': (2.0, 0.375), 'Wilcoxon (1 vs 3)': (0.0, 0.125), 'Wilcoxon (1 vs 4)': (3.0, 0.625), 'Wilcoxon (2 vs 3)': (0.0, 0.17971249487899976), 'Wilcoxon (2 vs 4)': (3.0, 1.0), 'Wilcoxon (3 vs 4)': (1.0, 0.6547208460185769), 'ANOVA': (0.12531328320802032, 0.9433148818773759), 'Friedman': (4.852941176470589, 0.18288770966116996)}\n",
      "\n",
      "Recall Tests:\n",
      "{'t-test (1 vs 2)': (1.4770978917519928, 0.23615367738664056), 't-test (1 vs 3)': (1.3211565181516332, 0.27817825276726316), 't-test (1 vs 4)': (1.7320508075688774, 0.18169011381620923), 't-test (2 vs 3)': (1.123902973898033, 0.3428716928579315), 't-test (2 vs 4)': (1.4142135623730951, 0.2522154963555042), 't-test (3 vs 4)': (-0.7385489458759958, 0.5137127113616164), 'Wilcoxon (1 vs 2)': (0.0, 0.10247043485974937), 'Wilcoxon (1 vs 3)': (0.0, 0.10880943004054568), 'Wilcoxon (1 vs 4)': (0.0, 0.10247043485974937), 'Wilcoxon (2 vs 3)': (1.0, 0.28504940740261275), 'Wilcoxon (2 vs 4)': (1.5, 0.375), 'Wilcoxon (3 vs 4)': (1.0, 0.6547208460185769), 'ANOVA': (0.23178807947019892, 0.8724843824504435), 'Friedman': (5.545454545454549, 0.13594506857585226)}\n",
      "\n",
      "F1-Score Tests:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/scipy/stats/_wilcoxon.py:198: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero_method 'wilcox' and 'pratt' do not work if x - y is zero for all elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(perform_tests(recall_no_padding_no_norm, recall_padding_norm, recall_padding_no_norm, recall_no_padding_norm))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mF1-Score Tests:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mperform_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1_no_padding_no_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_padding_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_padding_no_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_no_padding_norm\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mperform_tests\u001b[0;34m(data1, data2, data3, data4)\u001b[0m\n\u001b[1;32m     47\u001b[0m w_stat, p_value \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mwilcoxon(data1, data3)\n\u001b[1;32m     48\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWilcoxon (1 vs 3)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (w_stat, p_value)\n\u001b[0;32m---> 50\u001b[0m w_stat, p_value \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwilcoxon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWilcoxon (1 vs 4)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (w_stat, p_value)\n\u001b[1;32m     53\u001b[0m w_stat, p_value \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mwilcoxon(data2, data3)\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/scipy/_lib/_util.py:794\u001b[0m, in \u001b[0;36m_rename_parameter.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[1;32m    793\u001b[0m     kwargs[new_name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old_name)\n\u001b[0;32m--> 794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:531\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentinel:\n\u001b[1;32m    530\u001b[0m     samples \u001b[38;5;241m=\u001b[39m _remove_sentinel(samples, paired, sentinel)\n\u001b[0;32m--> 531\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mhypotest_fun_out\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m res \u001b[38;5;241m=\u001b[39m result_to_tuple(res)\n\u001b[1;32m    533\u001b[0m res \u001b[38;5;241m=\u001b[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/scipy/stats/_morestats.py:4111\u001b[0m, in \u001b[0;36mwilcoxon\u001b[0;34m(x, y, zero_method, correction, alternative, method, axis)\u001b[0m\n\u001b[1;32m   3893\u001b[0m \u001b[38;5;129m@_rename_parameter\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;129m@_axis_nan_policy_factory\u001b[39m(\n\u001b[1;32m   3895\u001b[0m     wilcoxon_result_object, paired\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwilcoxon\u001b[39m(x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, zero_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwilcox\u001b[39m\u001b[38;5;124m\"\u001b[39m, correction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3900\u001b[0m              alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo-sided\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the Wilcoxon signed-rank test.\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \n\u001b[1;32m   3903\u001b[0m \u001b[38;5;124;03m    The Wilcoxon signed-rank test tests the null hypothesis that two\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4109\u001b[0m \n\u001b[1;32m   4110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wilcoxon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wilcoxon_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malternative\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/scipy/stats/_wilcoxon.py:198\u001b[0m, in \u001b[0;36m_wilcoxon_nd\u001b[0;34m(x, y, zero_method, correction, alternative, method, axis)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wilcoxon_nd\u001b[39m(x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, zero_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwilcox\u001b[39m\u001b[38;5;124m'\u001b[39m, correction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    196\u001b[0m                  alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo-sided\u001b[39m\u001b[38;5;124m'\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 198\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43m_wilcoxon_iv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malternative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     d, zero_method, correction, alternative, method, axis \u001b[38;5;241m=\u001b[39m temp\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/scipy/stats/_wilcoxon.py:124\u001b[0m, in \u001b[0;36m_wilcoxon_iv\u001b[0;34m(x, y, zero_method, correction, alternative, method, axis)\u001b[0m\n\u001b[1;32m    118\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExact p-value calculation does not work if there are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros. Switching to normal approximation.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    120\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m zero_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwilcox\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpratt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m n_zero \u001b[38;5;241m==\u001b[39m d\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mand\u001b[39;00m d\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m d\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_method \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwilcox\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpratt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m do not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork if x - y is zero for all elements.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m d\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    128\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample size too small for normal approximation.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: zero_method 'wilcox' and 'pratt' do not work if x - y is zero for all elements."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define the metrics for each class and approach\n",
    "precision_no_padding_no_norm = [0.78, 0.87, 0.80, 0.79]\n",
    "precision_padding_norm = [0.77, 0.86, 0.81, 0.76]\n",
    "precision_padding_no_norm = [0.77, 0.85, 0.79, 0.76]\n",
    "precision_no_padding_norm = [0.77, 0.85, 0.78, 0.81]\n",
    "\n",
    "recall_no_padding_no_norm = [0.74, 0.88, 0.83, 0.81]\n",
    "recall_padding_norm = [0.74, 0.87, 0.82, 0.75]\n",
    "recall_padding_no_norm = [0.72, 0.87, 0.83, 0.68]\n",
    "recall_no_padding_norm = [0.72, 0.86, 0.83, 0.73]\n",
    "\n",
    "f1_no_padding_no_norm = [0.74, 0.86, 0.80, 0.77]\n",
    "f1_padding_norm = [0.75, 0.87, 0.81, 0.75]\n",
    "f1_padding_no_norm = [0.74, 0.86, 0.81, 0.72]\n",
    "f1_no_padding_norm = [0.74, 0.86, 0.80, 0.77]\n",
    "\n",
    "# Function to perform statistical tests\n",
    "def perform_tests(data1, data2, data3, data4):\n",
    "    results = {}\n",
    "\n",
    "    # Paired t-tests\n",
    "    t_stat, p_value = stats.ttest_rel(data1, data2)\n",
    "    results['t-test (1 vs 2)'] = (t_stat, p_value)\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(data1, data3)\n",
    "    results['t-test (1 vs 3)'] = (t_stat, p_value)\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(data1, data4)\n",
    "    results['t-test (1 vs 4)'] = (t_stat, p_value)\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(data2, data3)\n",
    "    results['t-test (2 vs 3)'] = (t_stat, p_value)\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(data2, data4)\n",
    "    results['t-test (2 vs 4)'] = (t_stat, p_value)\n",
    "\n",
    "    t_stat, p_value = stats.ttest_rel(data3, data4)\n",
    "    results['t-test (3 vs 4)'] = (t_stat, p_value)\n",
    "\n",
    "    # Wilcoxon signed-rank tests\n",
    "    w_stat, p_value = stats.wilcoxon(data1, data2)\n",
    "    results['Wilcoxon (1 vs 2)'] = (w_stat, p_value)\n",
    "\n",
    "    w_stat, p_value = stats.wilcoxon(data1, data3)\n",
    "    results['Wilcoxon (1 vs 3)'] = (w_stat, p_value)\n",
    "\n",
    "    w_stat, p_value = stats.wilcoxon(data1, data4)\n",
    "    results['Wilcoxon (1 vs 4)'] = (w_stat, p_value)\n",
    "\n",
    "    w_stat, p_value = stats.wilcoxon(data2, data3)\n",
    "    results['Wilcoxon (2 vs 3)'] = (w_stat, p_value)\n",
    "\n",
    "    w_stat, p_value = stats.wilcoxon(data2, data4)\n",
    "    results['Wilcoxon (2 vs 4)'] = (w_stat, p_value)\n",
    "\n",
    "    w_stat, p_value = stats.wilcoxon(data3, data4)\n",
    "    results['Wilcoxon (3 vs 4)'] = (w_stat, p_value)\n",
    "\n",
    "    # ANOVA\n",
    "    f_stat, p_value = stats.f_oneway(data1, data2, data3, data4)\n",
    "    results['ANOVA'] = (f_stat, p_value)\n",
    "\n",
    "    # Friedman Test\n",
    "    f_stat, p_value = stats.friedmanchisquare(data1, data2, data3, data4)\n",
    "    results['Friedman'] = (f_stat, p_value)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Perform tests for each metric\n",
    "print(\"Precision Tests:\")\n",
    "print(perform_tests(precision_no_padding_no_norm, precision_padding_norm, precision_padding_no_norm, precision_no_padding_norm))\n",
    "\n",
    "print(\"\\nRecall Tests:\")\n",
    "print(perform_tests(recall_no_padding_no_norm, recall_padding_norm, recall_padding_no_norm, recall_no_padding_norm))\n",
    "\n",
    "print(\"\\nF1-Score Tests:\")\n",
    "print(perform_tests(f1_no_padding_no_norm, f1_padding_norm, f1_padding_no_norm, f1_no_padding_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6208bc8-5bbe-4fc6-89f9-9a2f4327a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman Test: chi-squared = 1.0384615384615268, p-value = 0.7919465161444407\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "\n",
    "# Define the confusion matrices for each approach\n",
    "conf_matrix_no_padding_no_norm = np.array([[740,  89, 159,  12],\n",
    "                                          [ 77, 875,  45,   3],\n",
    "                                          [128,  37, 828,   7],\n",
    "                                          [  8,   5,   6,  81]])\n",
    "\n",
    "conf_matrix_padding_norm = np.array([[742,  94, 156,   8],\n",
    "                                     [ 87, 873,  34,   6],\n",
    "                                     [130,  42, 818,  10],\n",
    "                                     [  9,   9,   7,  75]])\n",
    "\n",
    "conf_matrix_padding_no_norm = np.array([[718, 102, 170,  10],\n",
    "                                        [ 84, 872,  40,   4],\n",
    "                                        [123,  44, 826,   7],\n",
    "                                        [ 13,  10,   9,  68]])\n",
    "\n",
    "conf_matrix_no_padding_norm = np.array([[716,  99, 180,   5],\n",
    "                                        [ 88, 862,  46,   4],\n",
    "                                        [118,  42, 832,   8],\n",
    "                                        [  6,  10,  11,  73]])\n",
    "\n",
    "# Flatten the confusion matrices\n",
    "flattened_matrices = {\n",
    "    'No Padding, No Norm': conf_matrix_no_padding_no_norm.flatten(),\n",
    "    'Padding & Norm': conf_matrix_padding_norm.flatten(),\n",
    "    'Padding, No Norm': conf_matrix_padding_no_norm.flatten(),\n",
    "    'No Padding, Norm': conf_matrix_no_padding_norm.flatten()\n",
    "}\n",
    "\n",
    "# Stack the flattened matrices for Friedman test\n",
    "data = np.vstack(list(flattened_matrices.values()))\n",
    "\n",
    "# Perform Friedman test\n",
    "f_stat, p_value = friedmanchisquare(*data)\n",
    "print(f\"Friedman Test: chi-squared = {f_stat}, p-value = {p_value}\")\n",
    "\n",
    "# If Friedman test is significant, perform pairwise comparisons\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nPairwise Comparisons (Nemenyi test):\")\n",
    "    posthoc_results = posthoc_nemenyi_friedman(data.T)\n",
    "    print(posthoc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdc0120-a13f-4e4b-8d98-81684832002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for No Padding, No Norm vs Padding & Norm: -0.0119\n",
      "Cohen's Kappa for No Padding, No Norm vs Padding, No Norm: 0.0588\n",
      "Cohen's Kappa for No Padding, No Norm vs No Padding, Norm: -0.0119\n",
      "Cohen's Kappa for Padding & Norm vs Padding, No Norm: -0.0199\n",
      "Cohen's Kappa for Padding & Norm vs No Padding, Norm: 0.0476\n",
      "Cohen's Kappa for Padding, No Norm vs No Padding, Norm: 0.1146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Define confusion matrices\n",
    "conf_matrix_no_padding_no_norm = np.array([[740,  89, 159,  12],\n",
    "                                          [ 77, 875,  45,   3],\n",
    "                                          [128,  37, 828,   7],\n",
    "                                          [  8,   5,   6,  81]])\n",
    "\n",
    "conf_matrix_padding_norm = np.array([[742,  94, 156,   8],\n",
    "                                     [ 87, 873,  34,   6],\n",
    "                                     [130,  42, 818,  10],\n",
    "                                     [  9,   9,   7,  75]])\n",
    "\n",
    "conf_matrix_padding_no_norm = np.array([[718, 102, 170,  10],\n",
    "                                        [ 84, 872,  40,   4],\n",
    "                                        [123,  44, 826,   7],\n",
    "                                        [ 13,  10,   9,  68]])\n",
    "\n",
    "conf_matrix_no_padding_norm = np.array([[716,  99, 180,   5],\n",
    "                                        [ 88, 862,  46,   4],\n",
    "                                        [118,  42, 832,   8],\n",
    "                                        [  6,  10,  11,  73]])\n",
    "\n",
    "# Flatten the confusion matrices\n",
    "flattened_matrices = {\n",
    "    'No Padding, No Norm': conf_matrix_no_padding_no_norm.flatten(),\n",
    "    'Padding & Norm': conf_matrix_padding_norm.flatten(),\n",
    "    'Padding, No Norm': conf_matrix_padding_no_norm.flatten(),\n",
    "    'No Padding, Norm': conf_matrix_no_padding_norm.flatten()\n",
    "}\n",
    "\n",
    "# Compute Cohen's Kappa for each pair of matrices\n",
    "kappa_scores = {}\n",
    "matrix_names = list(flattened_matrices.keys())\n",
    "\n",
    "for i, name1 in enumerate(matrix_names):\n",
    "    for j, name2 in enumerate(matrix_names):\n",
    "        if i < j:  # Avoid redundant pairs\n",
    "            kappa = cohen_kappa_score(flattened_matrices[name1], flattened_matrices[name2])\n",
    "            kappa_scores[f\"{name1} vs {name2}\"] = kappa\n",
    "\n",
    "# Print the results\n",
    "for pair, kappa in kappa_scores.items():\n",
    "    print(f\"Cohen's Kappa for {pair}: {kappa:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebb5320-97f1-498c-af1d-48398e1ee212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for No Padding, No Norm vs Padding & Norm: -0.0119, p-value: 0.4510\n",
      "Cohen's Kappa for No Padding, No Norm vs Padding, No Norm: 0.0588, p-value: 0.6220\n",
      "Cohen's Kappa for No Padding, No Norm vs No Padding, Norm: -0.0119, p-value: 0.4390\n",
      "Cohen's Kappa for Padding & Norm vs Padding, No Norm: -0.0199, p-value: 0.4410\n",
      "Cohen's Kappa for No Padding, Norm vs Padding & Norm: 0.0476, p-value: 0.4860\n",
      "Cohen's Kappa for No Padding, Norm vs Padding, No Norm: 0.1146, p-value: 0.4240\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_kappa(x, y, n_iterations=1000):\n",
    "    kappa_values = []\n",
    "    for _ in range(n_iterations):\n",
    "        x_resampled, y_resampled = resample(x, y)\n",
    "        kappa = cohen_kappa_score(x_resampled, y_resampled)\n",
    "        kappa_values.append(kappa)\n",
    "    return np.array(kappa_values)\n",
    "\n",
    "def calculate_p_value(observed_kappa, bootstrap_kappas):\n",
    "    return np.mean(np.abs(bootstrap_kappas) >= np.abs(observed_kappa))\n",
    "\n",
    "# Flattened matrices as per previous example\n",
    "flattened_matrices = {\n",
    "    'No Padding, No Norm': conf_matrix_no_padding_no_norm.flatten(),\n",
    "    'Padding & Norm': conf_matrix_padding_norm.flatten(),\n",
    "    'Padding, No Norm': conf_matrix_padding_no_norm.flatten(),\n",
    "    'No Padding, Norm': conf_matrix_no_padding_norm.flatten()\n",
    "}\n",
    "\n",
    "# Compute Cohen’s Kappa and bootstrap for each pair of matrices\n",
    "results = {}\n",
    "for name1, matrix1 in flattened_matrices.items():\n",
    "    for name2, matrix2 in flattened_matrices.items():\n",
    "        if name1 < name2:  # avoid redundant pairs\n",
    "            kappa = cohen_kappa_score(matrix1, matrix2)\n",
    "            bootstrap_kappas = bootstrap_kappa(matrix1, matrix2)\n",
    "            p_value = calculate_p_value(kappa, bootstrap_kappas)\n",
    "            results[f\"{name1} vs {name2}\"] = (kappa, p_value)\n",
    "\n",
    "# Print the results\n",
    "for pair, (kappa, p_value) in results.items():\n",
    "    print(f\"Cohen's Kappa for {pair}: {kappa:.4f}, p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f5287-b1e5-4562-80c0-dee63160ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
