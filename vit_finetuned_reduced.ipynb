{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afab07e8-069a-4344-91d2-7fbbf4d8ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params = [batch_size: 103, learning_rate: 0.00023844542114720738, dropout_rate: 0.11187612585438168, num_heads: 9, patch_size: 4, num_layers: 4]\n",
    "\n",
    "batch_size = 300\n",
    "lr = 0.00023\n",
    "dropout_rate = 0.11\n",
    "num_heads = 9\n",
    "patch_size = 4\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31bc6b8-7563-4311-a409-3806f3ff8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240629_001017-moqjgdpj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/moqjgdpj' target=\"_blank\">vit_after_tuning_reduced2024-06-29 00:10:16.556966</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/moqjgdpj' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/moqjgdpj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics import Precision, Recall\n",
    "from dataset import ImageDataset\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet50\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "from vit.vit import VisionTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'vit_after_tuning_reduced{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2233)\n",
    "\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "def get_valid_embedding_dims(num_heads):\n",
    "    return [dim for dim in range(32, 257) if dim % num_heads == 0]\n",
    "valid_embedding_dims = get_valid_embedding_dims(num_heads)\n",
    "embedding_dim = valid_embedding_dims[len(valid_embedding_dims)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37905244-aa69-4e50-a4b9-2fbf778f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(image_size=32, \n",
    "                      in_channels=3, \n",
    "                      num_classes=4, \n",
    "                      hidden_dims=[32], \n",
    "                      dropout_rate=dropout_rate,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      patch_size=patch_size,\n",
    "                      num_layers=3,\n",
    "                      num_heads=num_heads,\n",
    "                      use_linear_patch=False,\n",
    "                      use_conv_stem=True,\n",
    "                      use_conv_patch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79849476-e5ef-42ec-9ac3-7760c07ac9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=7, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        # Logowanie precision dla każdej klasy\n",
    "        wandb.log({'Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        # Logowanie recall dla każdej klasy\n",
    "        wandb.log({'Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        # Obliczanie głównych metryk\n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        # Logowanie głównych metryk\n",
    "        wandb.log({'main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        precision_ = main_metrics_precision\n",
    "        recall_ = main_metrics_recall\n",
    "        \n",
    "        if (precision_ + recall_) > 0:\n",
    "            f1_score_val = 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "        else:\n",
    "            f1_score_val = 0\n",
    "        \n",
    "        wandb.log({'f1_score_val': f1_score_val}, step=self.step)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "    \n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        \n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'val_main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        precision_ = main_metrics_precision\n",
    "        recall_ = main_metrics_recall\n",
    "        \n",
    "        if (precision_ + recall_) > 0:\n",
    "            f1_score_val = 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "        else:\n",
    "            f1_score_val = 0\n",
    "        \n",
    "        wandb.log({'f1_score_val': f1_score_val}, step=self.step)\n",
    "        \n",
    "        \n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca87432-715c-432f-8334-ab43c834bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 0.9282462659382051\n",
      "train_precision: tensor([0.5405, 0.6304, 0.5762, 0.6645], device='cuda:0')\n",
      "train_recall: tensor([0.3815, 0.7644, 0.6510, 0.0260], device='cuda:0')\n",
      "val_loss: 0.7855055979768136\n",
      "val_precision: tensor([0.6104, 0.6772, 0.7361, 0.5891], device='cuda:0')\n",
      "val_recall: tensor([0.5666, 0.8495, 0.6308, 0.2468], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 0 time:  125.27327691300161\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.8090495446997304\n",
      "train_precision: tensor([0.5954, 0.6984, 0.6714, 0.6730], device='cuda:0')\n",
      "train_recall: tensor([0.5498, 0.7816, 0.6754, 0.1436], device='cuda:0')\n",
      "val_loss: 0.7428478674781054\n",
      "val_precision: tensor([0.6640, 0.6844, 0.7264, 0.5464], device='cuda:0')\n",
      "val_recall: tensor([0.5270, 0.8721, 0.6902, 0.3355], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 1 time:  124.94000907000009\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.7729893701691781\n",
      "train_precision: tensor([0.6132, 0.7137, 0.6864, 0.6764], device='cuda:0')\n",
      "train_recall: tensor([0.5686, 0.7886, 0.6936, 0.2034], device='cuda:0')\n",
      "val_loss: 0.6918769900063823\n",
      "val_precision: tensor([0.6689, 0.7530, 0.7180, 0.5870], device='cuda:0')\n",
      "val_recall: tensor([0.5779, 0.8287, 0.7541, 0.3884], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 2 time:  124.47686079800042\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.7514201265188956\n",
      "train_precision: tensor([0.6252, 0.7250, 0.6942, 0.6676], device='cuda:0')\n",
      "train_recall: tensor([0.5786, 0.7942, 0.7067, 0.2379], device='cuda:0')\n",
      "val_loss: 0.6705079562681958\n",
      "val_precision: tensor([0.6987, 0.7573, 0.7155, 0.7274], device='cuda:0')\n",
      "val_recall: tensor([0.5564, 0.8404, 0.7969, 0.3684], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 3 time:  124.46874008300074\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.735190577756974\n",
      "train_precision: tensor([0.6380, 0.7307, 0.7011, 0.6911], device='cuda:0')\n",
      "train_recall: tensor([0.5903, 0.7981, 0.7149, 0.2634], device='cuda:0')\n",
      "val_loss: 0.6812087911412232\n",
      "val_precision: tensor([0.7375, 0.7291, 0.6933, 0.7930], device='cuda:0')\n",
      "val_recall: tensor([0.4907, 0.8647, 0.8081, 0.3467], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 4 time:  124.44638531800229\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.7200217408518638\n",
      "train_precision: tensor([0.6462, 0.7374, 0.7074, 0.7016], device='cuda:0')\n",
      "train_recall: tensor([0.5993, 0.8009, 0.7241, 0.2702], device='cuda:0')\n",
      "val_loss: 0.6371318753948785\n",
      "val_precision: tensor([0.6900, 0.8027, 0.7253, 0.7181], device='cuda:0')\n",
      "val_recall: tensor([0.6274, 0.8102, 0.7983, 0.4266], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 5 time:  124.41589774899694\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.7075453728437424\n",
      "train_precision: tensor([0.6568, 0.7440, 0.7131, 0.7035], device='cuda:0')\n",
      "train_recall: tensor([0.6062, 0.8067, 0.7330, 0.2853], device='cuda:0')\n",
      "val_loss: 0.6442866650290955\n",
      "val_precision: tensor([0.6472, 0.8384, 0.7330, 0.8237], device='cuda:0')\n",
      "val_recall: tensor([0.6758, 0.7582, 0.7961, 0.3760], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 6 time:  124.41808518399921\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.6994323781421108\n",
      "train_precision: tensor([0.6590, 0.7457, 0.7153, 0.7121], device='cuda:0')\n",
      "train_recall: tensor([0.6111, 0.8073, 0.7329, 0.3004], device='cuda:0')\n",
      "val_loss: 0.6211052426045999\n",
      "val_precision: tensor([0.7102, 0.7698, 0.7512, 0.8170], device='cuda:0')\n",
      "val_recall: tensor([0.6172, 0.8565, 0.7852, 0.3907], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 7 time:  124.4277641789995\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.6922149268850204\n",
      "train_precision: tensor([0.6639, 0.7488, 0.7186, 0.7169], device='cuda:0')\n",
      "train_recall: tensor([0.6164, 0.8093, 0.7368, 0.3012], device='cuda:0')\n",
      "val_loss: 0.6273882960466514\n",
      "val_precision: tensor([0.7026, 0.8261, 0.7094, 0.7278], device='cuda:0')\n",
      "val_recall: tensor([0.6166, 0.7976, 0.8314, 0.4383], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 8 time:  124.43554533699717\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.6856004716888551\n",
      "train_precision: tensor([0.6683, 0.7496, 0.7217, 0.7165], device='cuda:0')\n",
      "train_recall: tensor([0.6196, 0.8106, 0.7393, 0.3203], device='cuda:0')\n",
      "val_loss: 0.617677421162003\n",
      "val_precision: tensor([0.7208, 0.7705, 0.7475, 0.8997], device='cuda:0')\n",
      "val_recall: tensor([0.6148, 0.8583, 0.7956, 0.3531], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 9 time:  124.44887708799797\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.6795937388173996\n",
      "train_precision: tensor([0.6680, 0.7547, 0.7256, 0.7312], device='cuda:0')\n",
      "train_recall: tensor([0.6241, 0.8119, 0.7426, 0.3208], device='cuda:0')\n",
      "val_loss: 0.6042408314638568\n",
      "val_precision: tensor([0.7097, 0.8115, 0.7407, 0.6645], device='cuda:0')\n",
      "val_recall: tensor([0.6466, 0.8163, 0.8109, 0.4747], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 10 time:  124.42358716300078\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.6749476911560182\n",
      "train_precision: tensor([0.6733, 0.7555, 0.7283, 0.7094], device='cuda:0')\n",
      "train_recall: tensor([0.6304, 0.8119, 0.7441, 0.3211], device='cuda:0')\n",
      "val_loss: 0.592750533855051\n",
      "val_precision: tensor([0.6951, 0.7906, 0.7876, 0.8623], device='cuda:0')\n",
      "val_recall: tensor([0.6989, 0.8459, 0.7575, 0.3901], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 11 time:  124.42319664800016\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.6690380363695083\n",
      "train_precision: tensor([0.6740, 0.7589, 0.7304, 0.7291], device='cuda:0')\n",
      "train_recall: tensor([0.6338, 0.8128, 0.7453, 0.3432], device='cuda:0')\n",
      "val_loss: 0.5871110515935081\n",
      "val_precision: tensor([0.7380, 0.7936, 0.7477, 0.8098], device='cuda:0')\n",
      "val_recall: tensor([0.6298, 0.8496, 0.8201, 0.4377], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 12 time:  124.45484180800122\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.6657140892359519\n",
      "train_precision: tensor([0.6770, 0.7592, 0.7332, 0.7217], device='cuda:0')\n",
      "train_recall: tensor([0.6348, 0.8150, 0.7474, 0.3516], device='cuda:0')\n",
      "val_loss: 0.594802369972817\n",
      "val_precision: tensor([0.6998, 0.8151, 0.7695, 0.5607], device='cuda:0')\n",
      "val_recall: tensor([0.6811, 0.8341, 0.7739, 0.5288], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 13 time:  124.41107918899797\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.6621459603309632\n",
      "train_precision: tensor([0.6796, 0.7595, 0.7336, 0.7088], device='cuda:0')\n",
      "train_recall: tensor([0.6362, 0.8169, 0.7474, 0.3478], device='cuda:0')\n",
      "val_loss: 0.592200475527828\n",
      "val_precision: tensor([0.7081, 0.8244, 0.7451, 0.7443], device='cuda:0')\n",
      "val_recall: tensor([0.6607, 0.8176, 0.8140, 0.4788], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 14 time:  124.38856208700236\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.6571718169796852\n",
      "train_precision: tensor([0.6809, 0.7640, 0.7347, 0.7234], device='cuda:0')\n",
      "train_recall: tensor([0.6405, 0.8161, 0.7501, 0.3611], device='cuda:0')\n",
      "val_loss: 0.5861648817483643\n",
      "val_precision: tensor([0.7235, 0.8319, 0.7323, 0.6978], device='cuda:0')\n",
      "val_recall: tensor([0.6522, 0.8081, 0.8331, 0.4912], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 15 time:  124.40611592400091\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.6536434053413329\n",
      "train_precision: tensor([0.6831, 0.7622, 0.7385, 0.7277], device='cuda:0')\n",
      "train_recall: tensor([0.6421, 0.8166, 0.7522, 0.3644], device='cuda:0')\n",
      "val_loss: 0.6019193257828405\n",
      "val_precision: tensor([0.7243, 0.8478, 0.7119, 0.7746], device='cuda:0')\n",
      "val_recall: tensor([0.6472, 0.7801, 0.8543, 0.4806], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 16 time:  124.4432373669988\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.6509526716124627\n",
      "train_precision: tensor([0.6841, 0.7631, 0.7374, 0.7216], device='cuda:0')\n",
      "train_recall: tensor([0.6411, 0.8183, 0.7521, 0.3672], device='cuda:0')\n",
      "val_loss: 0.5757665880640647\n",
      "val_precision: tensor([0.7351, 0.8262, 0.7432, 0.7899], device='cuda:0')\n",
      "val_recall: tensor([0.6548, 0.8270, 0.8363, 0.4771], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 17 time:  124.42970754999988\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.6472457326227619\n",
      "train_precision: tensor([0.6881, 0.7660, 0.7398, 0.7356], device='cuda:0')\n",
      "train_recall: tensor([0.6438, 0.8220, 0.7545, 0.3800], device='cuda:0')\n",
      "val_loss: 0.5714916956603975\n",
      "val_precision: tensor([0.7496, 0.8043, 0.7519, 0.6705], device='cuda:0')\n",
      "val_recall: tensor([0.6349, 0.8505, 0.8256, 0.5452], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 18 time:  124.40312279499994\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.6448417913529181\n",
      "train_precision: tensor([0.6896, 0.7654, 0.7415, 0.7303], device='cuda:0')\n",
      "train_recall: tensor([0.6466, 0.8221, 0.7539, 0.3816], device='cuda:0')\n",
      "val_loss: 0.5671662805896056\n",
      "val_precision: tensor([0.7241, 0.8069, 0.7752, 0.7379], device='cuda:0')\n",
      "val_recall: tensor([0.6759, 0.8447, 0.8036, 0.5029], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 19 time:  124.44128994300263\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n",
      "train_loss: 0.6403382544555972\n",
      "train_precision: tensor([0.6878, 0.7678, 0.7432, 0.7302], device='cuda:0')\n",
      "train_recall: tensor([0.6487, 0.8208, 0.7548, 0.3909], device='cuda:0')\n",
      "val_loss: 0.5616826830725921\n",
      "val_precision: tensor([0.7434, 0.8150, 0.7644, 0.6829], device='cuda:0')\n",
      "val_recall: tensor([0.6647, 0.8440, 0.8225, 0.5505], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 20 time:  124.44218319200081\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 21\n",
      "train_loss: 0.6377483738045538\n",
      "train_precision: tensor([0.6910, 0.7696, 0.7448, 0.7263], device='cuda:0')\n",
      "train_recall: tensor([0.6518, 0.8220, 0.7569, 0.3919], device='cuda:0')\n",
      "val_loss: 0.5620819733555156\n",
      "val_precision: tensor([0.7313, 0.8057, 0.7764, 0.7214], device='cuda:0')\n",
      "val_recall: tensor([0.6741, 0.8516, 0.8011, 0.5552], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 21 time:  124.40175003900004\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 22\n",
      "train_loss: 0.6347733538958334\n",
      "train_precision: tensor([0.6937, 0.7714, 0.7454, 0.7288], device='cuda:0')\n",
      "train_recall: tensor([0.6545, 0.8218, 0.7592, 0.3962], device='cuda:0')\n",
      "val_loss: 0.5732492865028238\n",
      "val_precision: tensor([0.7241, 0.8271, 0.7618, 0.5785], device='cuda:0')\n",
      "val_recall: tensor([0.6752, 0.8202, 0.8136, 0.6016], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 22 time:  124.45645144300215\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 23\n",
      "train_loss: 0.633442521287549\n",
      "train_precision: tensor([0.6932, 0.7716, 0.7473, 0.7292], device='cuda:0')\n",
      "train_recall: tensor([0.6551, 0.8210, 0.7609, 0.3969], device='cuda:0')\n",
      "val_loss: 0.5572563697745029\n",
      "val_precision: tensor([0.7285, 0.8213, 0.7737, 0.7184], device='cuda:0')\n",
      "val_recall: tensor([0.6853, 0.8373, 0.8142, 0.5335], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 23 time:  124.45737052799814\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 24\n",
      "train_loss: 0.6297154999067707\n",
      "train_precision: tensor([0.6942, 0.7717, 0.7493, 0.7431], device='cuda:0')\n",
      "train_recall: tensor([0.6560, 0.8235, 0.7609, 0.4032], device='cuda:0')\n",
      "val_loss: 0.5771708470538146\n",
      "val_precision: tensor([0.7313, 0.8312, 0.7400, 0.6913], device='cuda:0')\n",
      "val_recall: tensor([0.6638, 0.8127, 0.8271, 0.5764], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 24 time:  124.43485177500042\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 25\n",
      "train_loss: 0.6279342994574577\n",
      "train_precision: tensor([0.6961, 0.7729, 0.7492, 0.7388], device='cuda:0')\n",
      "train_recall: tensor([0.6552, 0.8245, 0.7629, 0.4105], device='cuda:0')\n",
      "val_loss: 0.5611842006893086\n",
      "val_precision: tensor([0.7333, 0.8337, 0.7662, 0.6352], device='cuda:0')\n",
      "val_recall: tensor([0.6921, 0.8302, 0.8148, 0.5617], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 25 time:  124.44000325799789\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 26\n",
      "train_loss: 0.6252460777759552\n",
      "train_precision: tensor([0.6987, 0.7733, 0.7506, 0.7280], device='cuda:0')\n",
      "train_recall: tensor([0.6593, 0.8251, 0.7623, 0.4095], device='cuda:0')\n",
      "val_loss: 0.5607620623326839\n",
      "val_precision: tensor([0.7153, 0.7851, 0.8279, 0.7193], device='cuda:0')\n",
      "val_recall: tensor([0.7198, 0.8772, 0.7424, 0.5376], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 26 time:  124.32983760899879\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 27\n",
      "train_loss: 0.623380039007433\n",
      "train_precision: tensor([0.6981, 0.7731, 0.7517, 0.7331], device='cuda:0')\n",
      "train_recall: tensor([0.6592, 0.8263, 0.7612, 0.4189], device='cuda:0')\n",
      "val_loss: 0.5526211703182163\n",
      "val_precision: tensor([0.7505, 0.8394, 0.7455, 0.7304], device='cuda:0')\n",
      "val_recall: tensor([0.6577, 0.8281, 0.8520, 0.5541], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 27 time:  124.4300529760003\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 28\n",
      "train_loss: 0.619452105078005\n",
      "train_precision: tensor([0.7010, 0.7748, 0.7538, 0.7447], device='cuda:0')\n",
      "train_recall: tensor([0.6612, 0.8272, 0.7650, 0.4226], device='cuda:0')\n",
      "val_loss: 0.5559139106058537\n",
      "val_precision: tensor([0.7479, 0.8069, 0.7747, 0.7819], device='cuda:0')\n",
      "val_recall: tensor([0.6711, 0.8630, 0.8145, 0.4929], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 28 time:  124.43711351000093\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 29\n",
      "train_loss: 0.6182766075095822\n",
      "train_precision: tensor([0.7009, 0.7756, 0.7528, 0.7326], device='cuda:0')\n",
      "train_recall: tensor([0.6637, 0.8258, 0.7636, 0.4156], device='cuda:0')\n",
      "val_loss: 0.5552753652621033\n",
      "val_precision: tensor([0.7378, 0.8359, 0.7525, 0.7528], device='cuda:0')\n",
      "val_recall: tensor([0.6662, 0.8230, 0.8443, 0.5511], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 29 time:  124.4158413260011\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 30\n",
      "train_loss: 0.615985723512788\n",
      "train_precision: tensor([0.7030, 0.7761, 0.7538, 0.7440], device='cuda:0')\n",
      "train_recall: tensor([0.6630, 0.8267, 0.7656, 0.4410], device='cuda:0')\n",
      "val_loss: 0.5396995522025833\n",
      "val_precision: tensor([0.7290, 0.8207, 0.8001, 0.7524], device='cuda:0')\n",
      "val_recall: tensor([0.7180, 0.8499, 0.7970, 0.5535], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 30 time:  124.39766833300018\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 31\n",
      "train_loss: 0.6151441028521907\n",
      "train_precision: tensor([0.7034, 0.7763, 0.7544, 0.7388], device='cuda:0')\n",
      "train_recall: tensor([0.6640, 0.8257, 0.7668, 0.4405], device='cuda:0')\n",
      "val_loss: 0.5427020677274331\n",
      "val_precision: tensor([0.7472, 0.8210, 0.7802, 0.7337], device='cuda:0')\n",
      "val_recall: tensor([0.6867, 0.8509, 0.8211, 0.5875], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 31 time:  124.41137387900017\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 32\n",
      "train_loss: 0.6095653737264295\n",
      "train_precision: tensor([0.7041, 0.7783, 0.7563, 0.7516], device='cuda:0')\n",
      "train_recall: tensor([0.6648, 0.8282, 0.7682, 0.4461], device='cuda:0')\n",
      "val_loss: 0.542924765693514\n",
      "val_precision: tensor([0.7563, 0.8337, 0.7669, 0.7240], device='cuda:0')\n",
      "val_recall: tensor([0.6870, 0.8368, 0.8389, 0.5811], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 32 time:  124.39357314199879\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 33\n",
      "train_loss: 0.6092098495652599\n",
      "train_precision: tensor([0.7042, 0.7796, 0.7575, 0.7485], device='cuda:0')\n",
      "train_recall: tensor([0.6687, 0.8292, 0.7665, 0.4372], device='cuda:0')\n",
      "val_loss: 0.5519195293125353\n",
      "val_precision: tensor([0.7408, 0.8366, 0.7660, 0.6371], device='cuda:0')\n",
      "val_recall: tensor([0.6798, 0.8272, 0.8328, 0.6363], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 33 time:  124.4129826319986\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 34\n",
      "train_loss: 0.6074965680318494\n",
      "train_precision: tensor([0.7057, 0.7770, 0.7565, 0.7378], device='cuda:0')\n",
      "train_recall: tensor([0.6674, 0.8279, 0.7656, 0.4496], device='cuda:0')\n",
      "val_loss: 0.5422210468161375\n",
      "val_precision: tensor([0.7408, 0.8245, 0.7929, 0.6079], device='cuda:0')\n",
      "val_recall: tensor([0.7028, 0.8479, 0.8077, 0.6204], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 34 time:  124.38631886999792\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 35\n",
      "train_loss: 0.6048953831676513\n",
      "train_precision: tensor([0.7073, 0.7815, 0.7600, 0.7370], device='cuda:0')\n",
      "train_recall: tensor([0.6717, 0.8309, 0.7677, 0.4534], device='cuda:0')\n",
      "val_loss: 0.5400266214869076\n",
      "val_precision: tensor([0.7383, 0.8058, 0.8082, 0.6691], device='cuda:0')\n",
      "val_recall: tensor([0.7065, 0.8661, 0.7872, 0.5987], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 35 time:  124.40184861499802\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 36\n",
      "train_loss: 0.6047633824809905\n",
      "train_precision: tensor([0.7073, 0.7805, 0.7587, 0.7345], device='cuda:0')\n",
      "train_recall: tensor([0.6717, 0.8302, 0.7661, 0.4519], device='cuda:0')\n",
      "val_loss: 0.5478657075112924\n",
      "val_precision: tensor([0.7561, 0.8078, 0.7753, 0.7708], device='cuda:0')\n",
      "val_recall: tensor([0.6638, 0.8687, 0.8198, 0.5611], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 36 time:  124.3882595920004\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 37\n",
      "train_loss: 0.6023801840120746\n",
      "train_precision: tensor([0.7079, 0.7808, 0.7619, 0.7408], device='cuda:0')\n",
      "train_recall: tensor([0.6743, 0.8309, 0.7672, 0.4539], device='cuda:0')\n",
      "val_loss: 0.5400748556493816\n",
      "val_precision: tensor([0.7713, 0.8128, 0.7714, 0.7767], device='cuda:0')\n",
      "val_recall: tensor([0.6628, 0.8675, 0.8345, 0.5723], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 37 time:  124.95360607299881\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 38\n",
      "train_loss: 0.5993620072160998\n",
      "train_precision: tensor([0.7114, 0.7829, 0.7618, 0.7424], device='cuda:0')\n",
      "train_recall: tensor([0.6745, 0.8317, 0.7716, 0.4511], device='cuda:0')\n",
      "val_loss: 0.5309439085255888\n",
      "val_precision: tensor([0.7544, 0.8103, 0.8000, 0.7500], device='cuda:0')\n",
      "val_recall: tensor([0.6970, 0.8705, 0.8091, 0.5975], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 38 time:  130.51162102099988\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 39\n",
      "train_loss: 0.5981116045867243\n",
      "train_precision: tensor([0.7113, 0.7827, 0.7632, 0.7513], device='cuda:0')\n",
      "train_recall: tensor([0.6748, 0.8310, 0.7718, 0.4751], device='cuda:0')\n",
      "val_loss: 0.5456298801459765\n",
      "val_precision: tensor([0.7577, 0.8061, 0.7869, 0.6849], device='cuda:0')\n",
      "val_recall: tensor([0.6738, 0.8707, 0.8120, 0.6105], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 39 time:  131.60675823600104\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 40\n",
      "train_loss: 0.5966739900169834\n",
      "train_precision: tensor([0.7108, 0.7844, 0.7630, 0.7486], device='cuda:0')\n",
      "train_recall: tensor([0.6765, 0.8302, 0.7719, 0.4758], device='cuda:0')\n",
      "val_loss: 0.5403547769874558\n",
      "val_precision: tensor([0.7417, 0.8278, 0.7779, 0.7766], device='cuda:0')\n",
      "val_recall: tensor([0.6851, 0.8460, 0.8310, 0.5452], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 40 time:  133.93498403600097\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 41\n",
      "train_loss: 0.5952668596179255\n",
      "train_precision: tensor([0.7091, 0.7843, 0.7656, 0.7454], device='cuda:0')\n",
      "train_recall: tensor([0.6770, 0.8305, 0.7718, 0.4745], device='cuda:0')\n",
      "val_loss: 0.548509388377792\n",
      "val_precision: tensor([0.7276, 0.7784, 0.8482, 0.7566], device='cuda:0')\n",
      "val_recall: tensor([0.7277, 0.8922, 0.7398, 0.5899], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 41 time:  129.4107263170008\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 42\n",
      "train_loss: 0.5925259615625105\n",
      "train_precision: tensor([0.7125, 0.7849, 0.7653, 0.7427], device='cuda:0')\n",
      "train_recall: tensor([0.6794, 0.8323, 0.7716, 0.4685], device='cuda:0')\n",
      "val_loss: 0.536728409672142\n",
      "val_precision: tensor([0.7461, 0.8340, 0.7808, 0.7131], device='cuda:0')\n",
      "val_recall: tensor([0.7006, 0.8408, 0.8274, 0.5899], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 42 time:  124.97051189099875\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 43\n",
      "train_loss: 0.5909101763079243\n",
      "train_precision: tensor([0.7114, 0.7853, 0.7658, 0.7602], device='cuda:0')\n",
      "train_recall: tensor([0.6786, 0.8317, 0.7730, 0.4778], device='cuda:0')\n",
      "val_loss: 0.5287671412964513\n",
      "val_precision: tensor([0.7364, 0.7995, 0.8247, 0.7233], device='cuda:0')\n",
      "val_recall: tensor([0.7221, 0.8774, 0.7698, 0.6128], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 43 time:  124.95655045199965\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 44\n",
      "train_loss: 0.5901729817832669\n",
      "train_precision: tensor([0.7135, 0.7864, 0.7674, 0.7468], device='cuda:0')\n",
      "train_recall: tensor([0.6807, 0.8333, 0.7731, 0.4816], device='cuda:0')\n",
      "val_loss: 0.5402062612592726\n",
      "val_precision: tensor([0.7761, 0.8164, 0.7601, 0.6580], device='cuda:0')\n",
      "val_recall: tensor([0.6450, 0.8584, 0.8448, 0.6093], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 44 time:  124.47139317800247\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 45\n",
      "train_loss: 0.5897483231559877\n",
      "train_precision: tensor([0.7124, 0.7853, 0.7692, 0.7498], device='cuda:0')\n",
      "train_recall: tensor([0.6813, 0.8328, 0.7734, 0.4751], device='cuda:0')\n",
      "val_loss: 0.5249518716245666\n",
      "val_precision: tensor([0.7426, 0.8328, 0.7911, 0.7076], device='cuda:0')\n",
      "val_recall: tensor([0.7082, 0.8447, 0.8191, 0.6369], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 45 time:  124.43649196800106\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 46\n",
      "train_loss: 0.5867242293011757\n",
      "train_precision: tensor([0.7146, 0.7858, 0.7669, 0.7398], device='cuda:0')\n",
      "train_recall: tensor([0.6806, 0.8332, 0.7730, 0.4821], device='cuda:0')\n",
      "val_loss: 0.5349172349262955\n",
      "val_precision: tensor([0.7400, 0.8538, 0.7719, 0.6946], device='cuda:0')\n",
      "val_recall: tensor([0.7118, 0.8204, 0.8368, 0.5746], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 46 time:  124.411836553998\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 47\n",
      "train_loss: 0.584827014178999\n",
      "train_precision: tensor([0.7164, 0.7856, 0.7710, 0.7541], device='cuda:0')\n",
      "train_recall: tensor([0.6836, 0.8336, 0.7749, 0.4985], device='cuda:0')\n",
      "val_loss: 0.5377830285999111\n",
      "val_precision: tensor([0.7398, 0.8443, 0.7742, 0.7410], device='cuda:0')\n",
      "val_recall: tensor([0.7130, 0.8194, 0.8327, 0.5834], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 47 time:  125.88915296099731\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 48\n",
      "train_loss: 0.5837767519777821\n",
      "train_precision: tensor([0.7162, 0.7875, 0.7695, 0.7564], device='cuda:0')\n",
      "train_recall: tensor([0.6828, 0.8344, 0.7756, 0.4929], device='cuda:0')\n",
      "val_loss: 0.5151483706737819\n",
      "val_precision: tensor([0.7385, 0.8366, 0.8050, 0.7321], device='cuda:0')\n",
      "val_recall: tensor([0.7297, 0.8467, 0.8126, 0.6134], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 48 time:  126.55534808499942\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 49\n",
      "train_loss: 0.5823260205407297\n",
      "train_precision: tensor([0.7178, 0.7884, 0.7704, 0.7524], device='cuda:0')\n",
      "train_recall: tensor([0.6843, 0.8343, 0.7775, 0.4924], device='cuda:0')\n",
      "val_loss: 0.5255773811412037\n",
      "val_precision: tensor([0.7348, 0.8328, 0.8100, 0.7262], device='cuda:0')\n",
      "val_recall: tensor([0.7389, 0.8410, 0.8050, 0.6281], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 49 time:  127.05063543999859\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 50\n",
      "train_loss: 0.5801022091219502\n",
      "train_precision: tensor([0.7189, 0.7893, 0.7707, 0.7526], device='cuda:0')\n",
      "train_recall: tensor([0.6854, 0.8350, 0.7777, 0.4975], device='cuda:0')\n",
      "val_loss: 0.5195485242551431\n",
      "val_precision: tensor([0.7351, 0.8424, 0.8036, 0.7164], device='cuda:0')\n",
      "val_recall: tensor([0.7354, 0.8371, 0.8159, 0.6069], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 50 time:  126.31500488799793\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 51\n",
      "train_loss: 0.5797394723180802\n",
      "train_precision: tensor([0.7164, 0.7898, 0.7700, 0.7498], device='cuda:0')\n",
      "train_recall: tensor([0.6863, 0.8332, 0.7761, 0.4909], device='cuda:0')\n",
      "val_loss: 0.5269923822996312\n",
      "val_precision: tensor([0.7577, 0.8200, 0.7896, 0.7007], device='cuda:0')\n",
      "val_recall: tensor([0.6946, 0.8559, 0.8218, 0.6328], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 51 time:  127.80065052499776\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 52\n",
      "train_loss: 0.5775733264223222\n",
      "train_precision: tensor([0.7197, 0.7915, 0.7724, 0.7536], device='cuda:0')\n",
      "train_recall: tensor([0.6887, 0.8359, 0.7781, 0.4987], device='cuda:0')\n",
      "val_loss: 0.5184786328695771\n",
      "val_precision: tensor([0.7609, 0.8245, 0.7967, 0.6796], device='cuda:0')\n",
      "val_recall: tensor([0.6985, 0.8626, 0.8239, 0.6492], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 52 time:  126.609232169998\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 53\n",
      "train_loss: 0.5746001619004434\n",
      "train_precision: tensor([0.7199, 0.7915, 0.7721, 0.7585], device='cuda:0')\n",
      "train_recall: tensor([0.6878, 0.8361, 0.7784, 0.5058], device='cuda:0')\n",
      "val_loss: 0.5300973226925484\n",
      "val_precision: tensor([0.7560, 0.8119, 0.8028, 0.6782], device='cuda:0')\n",
      "val_recall: tensor([0.7018, 0.8675, 0.8073, 0.6216], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 53 time:  126.86696223499894\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 54\n",
      "train_loss: 0.5734090543081684\n",
      "train_precision: tensor([0.7202, 0.7940, 0.7731, 0.7493], device='cuda:0')\n",
      "train_recall: tensor([0.6896, 0.8356, 0.7800, 0.5113], device='cuda:0')\n",
      "val_loss: 0.5282764400082424\n",
      "val_precision: tensor([0.7684, 0.8044, 0.8030, 0.6552], device='cuda:0')\n",
      "val_recall: tensor([0.6864, 0.8778, 0.8122, 0.6463], device='cuda:0')\n",
      "Learning rate: 0.00023\n",
      "epoch 54 time:  126.31301988099949\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     20\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mMyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m     32\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_recall(preds, labels)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:373\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:466\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/classification/stat_scores.py:333\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m    337\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    338\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:309\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    307\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_model = MyModel(model=model, learning_rate=lr)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform, reduce=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test, reduce=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 15\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1bb09ab-8ddb-45b3-a10e-439409c28b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[589  95 157   4]\n",
      " [ 86 764  34   5]\n",
      " [102  45 757   4]\n",
      " [  3   7  12  36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.72       845\n",
      "           1       0.84      0.86      0.85       889\n",
      "           2       0.79      0.83      0.81       908\n",
      "           3       0.73      0.62      0.67        58\n",
      "\n",
      "    accuracy                           0.79      2700\n",
      "   macro avg       0.78      0.75      0.76      2700\n",
      "weighted avg       0.79      0.79      0.79      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))\n",
    "\n",
    "def test_report(model, dataloader):\n",
    "    \"\"\"Prints confusion matrix for testing dataset\n",
    "    dataloader should be of batch_size=1.\"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            output = model(data)\n",
    "            label = label.numpy()\n",
    "            output = output.numpy()\n",
    "            y_pred.append(np.argmax(output))\n",
    "            y_test.append(np.argmax(label))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "testset =ImageDataset(data_path='test_data', transform=transform_test, reduce=True)\n",
    "dataloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "test_report(my_model.to('cpu'), dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c71b73-aa2c-4b05-b87d-3ddeab9247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafd826-3f8c-40bf-9149-d88856a859b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
