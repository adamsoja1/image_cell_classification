{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159624e2-9c95-4d27-99e7-1e8bb20c9916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240526_015943-ffy2lzqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/ffy2lzqh' target=\"_blank\">vit_normalized_conv_patch2024-05-26 01:59:42.631751</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/ffy2lzqh' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/ffy2lzqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 1.0101479250050727\n",
      "train_precision: tensor([0.4862, 0.5873, 0.5475, 0.0000], device='cuda:0')\n",
      "train_recall: tensor([0.4524, 0.7540, 0.4849, 0.0000], device='cuda:0')\n",
      "val_loss: 0.958369374440776\n",
      "val_precision: tensor([0.5627, 0.6603, 0.5524, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.4144, 0.6882, 0.7299, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 0 time:  26.401222496999935\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.950970278396493\n",
      "train_precision: tensor([0.5260, 0.6262, 0.6028, 0.0000], device='cuda:0')\n",
      "train_recall: tensor([0.5141, 0.7485, 0.5589, 0.0000], device='cuda:0')\n",
      "val_loss: 0.977106744547685\n",
      "val_precision: tensor([0.5163, 0.6970, 0.5764, 1.0000], device='cuda:0')\n",
      "val_recall: tensor([5.4434e-01, 6.1576e-01, 6.6980e-01, 3.3670e-04], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 1 time:  23.88495900299995\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.9363938521771203\n",
      "train_precision: tensor([0.5341, 0.6353, 0.6095, 0.5000], device='cuda:0')\n",
      "train_recall: tensor([5.2242e-01, 7.4932e-01, 5.7431e-01, 1.4430e-04], device='cuda:0')\n",
      "val_loss: 0.9245384155048264\n",
      "val_precision: tensor([0.5983, 0.6714, 0.5523, 0.2000], device='cuda:0')\n",
      "val_recall: tensor([0.3628, 0.7301, 0.7762, 0.0013], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 2 time:  23.37895212300009\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.9279236982266108\n",
      "train_precision: tensor([0.5374, 0.6408, 0.6145, 0.3636], device='cuda:0')\n",
      "train_recall: tensor([0.5287, 0.7504, 0.5806, 0.0012], device='cuda:0')\n",
      "val_loss: 0.8993019653691185\n",
      "val_precision: tensor([0.5636, 0.6573, 0.6405, 0.5882], device='cuda:0')\n",
      "val_recall: tensor([0.5190, 0.7718, 0.6432, 0.0034], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 3 time:  24.11958439299997\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.9278760321793103\n",
      "train_precision: tensor([0.5389, 0.6396, 0.6105, 0.4390], device='cuda:0')\n",
      "train_recall: tensor([0.5288, 0.7466, 0.5805, 0.0026], device='cuda:0')\n",
      "val_loss: 0.9225827821426922\n",
      "val_precision: tensor([0.5416, 0.7161, 0.5733, 0.4231], device='cuda:0')\n",
      "val_recall: tensor([0.4837, 0.6550, 0.7403, 0.0037], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 4 time:  24.10060750599996\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.930113363478865\n",
      "train_precision: tensor([0.5373, 0.6376, 0.6082, 0.5208], device='cuda:0')\n",
      "train_recall: tensor([0.5228, 0.7496, 0.5780, 0.0036], device='cuda:0')\n",
      "val_loss: 0.9566407694584793\n",
      "val_precision: tensor([0.5526, 0.7531, 0.5294, 0.4074], device='cuda:0')\n",
      "val_recall: tensor([0.4638, 0.5848, 0.7852, 0.0037], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 5 time:  24.034335084000077\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.929439602011726\n",
      "train_precision: tensor([0.5374, 0.6390, 0.6057, 0.5465], device='cuda:0')\n",
      "train_recall: tensor([0.5230, 0.7493, 0.5772, 0.0068], device='cuda:0')\n",
      "val_loss: 0.9644704677992397\n",
      "val_precision: tensor([0.5702, 0.6779, 0.5540, 0.3964], device='cuda:0')\n",
      "val_recall: tensor([0.3771, 0.7011, 0.7739, 0.0296], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 6 time:  23.944645745000116\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.9300934822076843\n",
      "train_precision: tensor([0.5375, 0.6383, 0.6039, 0.4588], device='cuda:0')\n",
      "train_recall: tensor([0.5236, 0.7496, 0.5738, 0.0056], device='cuda:0')\n",
      "val_loss: 0.9180235301454862\n",
      "val_precision: tensor([0.5988, 0.6694, 0.5652, 0.4759], device='cuda:0')\n",
      "val_recall: tensor([0.4165, 0.7319, 0.7384, 0.0232], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 7 time:  23.760802297000055\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.9214191591455823\n",
      "train_precision: tensor([0.5403, 0.6437, 0.6107, 0.4396], device='cuda:0')\n",
      "train_recall: tensor([0.5269, 0.7500, 0.5844, 0.0115], device='cuda:0')\n",
      "val_loss: 0.9066971704363823\n",
      "val_precision: tensor([0.5656, 0.7055, 0.5786, 0.4643], device='cuda:0')\n",
      "val_recall: tensor([0.4605, 0.6937, 0.7525, 0.0088], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 8 time:  24.132633568000074\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.9091061415416853\n",
      "train_precision: tensor([0.5467, 0.6492, 0.6200, 0.5105], device='cuda:0')\n",
      "train_recall: tensor([0.5381, 0.7533, 0.5905, 0.0140], device='cuda:0')\n",
      "val_loss: 0.9007337536248896\n",
      "val_precision: tensor([0.5902, 0.7121, 0.5810, 0.4865], device='cuda:0')\n",
      "val_recall: tensor([0.4549, 0.7197, 0.7610, 0.0424], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 9 time:  23.812780586999907\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.9035924639730226\n",
      "train_precision: tensor([0.5491, 0.6519, 0.6230, 0.4771], device='cuda:0')\n",
      "train_recall: tensor([0.5426, 0.7534, 0.5927, 0.0225], device='cuda:0')\n",
      "val_loss: 0.9044674917227692\n",
      "val_precision: tensor([0.5911, 0.7197, 0.5700, 0.4819], device='cuda:0')\n",
      "val_recall: tensor([0.4351, 0.7053, 0.7820, 0.0582], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 10 time:  23.931207604000065\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.9001811296457336\n",
      "train_precision: tensor([0.5519, 0.6539, 0.6249, 0.5174], device='cuda:0')\n",
      "train_recall: tensor([0.5432, 0.7543, 0.5982, 0.0257], device='cuda:0')\n",
      "val_loss: 0.9048353292047977\n",
      "val_precision: tensor([0.5810, 0.7262, 0.5796, 0.5583], device='cuda:0')\n",
      "val_recall: tensor([0.4772, 0.6957, 0.7609, 0.0451], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 11 time:  24.12832792999984\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.8971320408440772\n",
      "train_precision: tensor([0.5526, 0.6568, 0.6259, 0.4694], device='cuda:0')\n",
      "train_recall: tensor([0.5426, 0.7538, 0.6037, 0.0277], device='cuda:0')\n",
      "val_loss: 0.9079920951690938\n",
      "val_precision: tensor([0.5828, 0.7337, 0.5695, 0.5273], device='cuda:0')\n",
      "val_recall: tensor([0.4609, 0.6805, 0.7816, 0.0488], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 12 time:  23.80443729800004\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.8942045892987932\n",
      "train_precision: tensor([0.5545, 0.6589, 0.6252, 0.4841], device='cuda:0')\n",
      "train_recall: tensor([0.5440, 0.7528, 0.6067, 0.0286], device='cuda:0')\n",
      "val_loss: 0.9148640254305469\n",
      "val_precision: tensor([0.5955, 0.7308, 0.5612, 0.5560], device='cuda:0')\n",
      "val_recall: tensor([0.4412, 0.6830, 0.7946, 0.0468], device='cuda:0')\n",
      "Learning rate: 1e-05\n",
      "Early stopping at epoch 13 with best validation loss 0.8993019653691185\n",
      "training_checkpoints/vit_normalized_conv_patch2024-05-26 01:59:42.631751.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet50\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "from vit.vit import VisionTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'vit_normalized_conv_patch{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.4, p=1), RandomContrast(limit=0.4, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.6,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=4, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        # Logowanie precision dla każdej klasy\n",
    "        wandb.log({'Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        # Logowanie recall dla każdej klasy\n",
    "        wandb.log({'Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        # Obliczanie głównych metryk\n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        # Logowanie głównych metryk\n",
    "        wandb.log({'main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        wandb.log({'f1_score': (main_metrics_recall + main_metrics_precision )/ 2}, step=self.step)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "    \n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        \n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'val_main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "        wandb.log({'f1_score_val': (main_metrics_recall + main_metrics_precision )/ 2}, step=self.step)\n",
    "        \n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 256\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "\n",
    "model = VisionTransformer(image_size=32, \n",
    "                          in_channels=3, \n",
    "                          num_classes=4, \n",
    "                          hidden_dims=[32], \n",
    "                          dropout_rate=0.5,\n",
    "                          embedding_dim=256,\n",
    "                          patch_size=4,\n",
    "                          num_layers=3,\n",
    "                          num_heads=4,\n",
    "                          use_linear_patch=False,\n",
    "                          use_conv_stem=False,\n",
    "                         use_conv_patch=True)\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45239414-09b7-4d09-9f74-3179bf87ab65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
