{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdd64bd-dc9c-4547-88f4-913183a0680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240424_221534-mlw797al</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/mlw797al' target=\"_blank\">vit_normalize_32x322024-04-24 22:15:33.718230</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/mlw797al' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/mlw797al</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 0.9418622387307031\n",
      "train_precision: tensor([0.5374, 0.6151, 0.6103, 0.6145], device='cuda:0')\n",
      "train_recall: tensor([0.4657, 0.7660, 0.6017, 0.0147], device='cuda:0')\n",
      "val_loss: 0.8268931256400214\n",
      "val_precision: tensor([0.6039, 0.7324, 0.6226, 0.8834], device='cuda:0')\n",
      "val_recall: tensor([0.5033, 0.7398, 0.7752, 0.1020], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 0 time:  1.4102790667166725\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.8585198217204639\n",
      "train_precision: tensor([0.5747, 0.6747, 0.6610, 0.6333], device='cuda:0')\n",
      "train_recall: tensor([0.5518, 0.7668, 0.6562, 0.0658], device='cuda:0')\n",
      "val_loss: 0.8195770169297855\n",
      "val_precision: tensor([0.6522, 0.7282, 0.5976, 0.6272], device='cuda:0')\n",
      "val_recall: tensor([0.4213, 0.7610, 0.8252, 0.1768], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 1 time:  1.3842193001166683\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.8363936498051598\n",
      "train_precision: tensor([0.5859, 0.6869, 0.6681, 0.5825], device='cuda:0')\n",
      "train_recall: tensor([0.5592, 0.7742, 0.6672, 0.1156], device='cuda:0')\n",
      "val_loss: 0.7751718921793832\n",
      "val_precision: tensor([0.6264, 0.7684, 0.6453, 0.7453], device='cuda:0')\n",
      "val_recall: tensor([0.5461, 0.7459, 0.7921, 0.2246], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 2 time:  1.3783857367000034\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.8177706800046421\n",
      "train_precision: tensor([0.5980, 0.6959, 0.6757, 0.5654], device='cuda:0')\n",
      "train_recall: tensor([0.5685, 0.7768, 0.6769, 0.1765], device='cuda:0')\n",
      "val_loss: 0.7641239984995789\n",
      "val_precision: tensor([0.6447, 0.7472, 0.6631, 0.8068], device='cuda:0')\n",
      "val_recall: tensor([0.5573, 0.7866, 0.7678, 0.2010], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 3 time:  1.3769093670333328\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.8074439981863613\n",
      "train_precision: tensor([0.6020, 0.7025, 0.6792, 0.5627], device='cuda:0')\n",
      "train_recall: tensor([0.5741, 0.7811, 0.6792, 0.1937], device='cuda:0')\n",
      "val_loss: 0.773318024393585\n",
      "val_precision: tensor([0.6242, 0.8224, 0.6482, 0.5152], device='cuda:0')\n",
      "val_recall: tensor([0.6099, 0.6763, 0.7886, 0.4327], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 4 time:  1.3874895140833359\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.799114481537115\n",
      "train_precision: tensor([0.6078, 0.7043, 0.6814, 0.5719], device='cuda:0')\n",
      "train_recall: tensor([0.5778, 0.7815, 0.6827, 0.2157], device='cuda:0')\n",
      "val_loss: 0.7906266802714931\n",
      "val_precision: tensor([0.6207, 0.8401, 0.6210, 0.7092], device='cuda:0')\n",
      "val_recall: tensor([0.5833, 0.6478, 0.8351, 0.3162], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 5 time:  1.4118857241666622\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.7866519881146294\n",
      "train_precision: tensor([0.6145, 0.7087, 0.6874, 0.5790], device='cuda:0')\n",
      "train_recall: tensor([0.5857, 0.7841, 0.6872, 0.2364], device='cuda:0')\n",
      "val_loss: 0.7185792850951354\n",
      "val_precision: tensor([0.6618, 0.7465, 0.7073, 0.7647], device='cuda:0')\n",
      "val_recall: tensor([0.5925, 0.8225, 0.7530, 0.2933], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 6 time:  1.3815534413666684\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.7790831541021664\n",
      "train_precision: tensor([0.6172, 0.7118, 0.6893, 0.5788], device='cuda:0')\n",
      "train_recall: tensor([0.5897, 0.7853, 0.6901, 0.2320], device='cuda:0')\n",
      "val_loss: 0.7579514962103632\n",
      "val_precision: tensor([0.6374, 0.8190, 0.6580, 0.7149], device='cuda:0')\n",
      "val_recall: tensor([0.6119, 0.7051, 0.8125, 0.3175], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 7 time:  1.3914094321833395\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.7720115758833431\n",
      "train_precision: tensor([0.6247, 0.7146, 0.6930, 0.5829], device='cuda:0')\n",
      "train_recall: tensor([0.5931, 0.7885, 0.6954, 0.2543], device='cuda:0')\n",
      "val_loss: 0.8079283380673992\n",
      "val_precision: tensor([0.6158, 0.8531, 0.6186, 0.7176], device='cuda:0')\n",
      "val_recall: tensor([0.5931, 0.6178, 0.8437, 0.3492], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 8 time:  1.395696886150002\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.7663103538609687\n",
      "train_precision: tensor([0.6262, 0.7169, 0.6983, 0.6017], device='cuda:0')\n",
      "train_recall: tensor([0.5988, 0.7906, 0.6960, 0.2664], device='cuda:0')\n",
      "val_loss: 0.7122832379407353\n",
      "val_precision: tensor([0.6697, 0.7629, 0.7096, 0.7511], device='cuda:0')\n",
      "val_recall: tensor([0.6190, 0.8110, 0.7632, 0.2785], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 9 time:  1.3864537573333413\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.7583060604475793\n",
      "train_precision: tensor([0.6337, 0.7206, 0.6984, 0.6069], device='cuda:0')\n",
      "train_recall: tensor([0.6028, 0.7927, 0.6999, 0.2830], device='cuda:0')\n",
      "val_loss: 0.8184251095271773\n",
      "val_precision: tensor([0.6385, 0.8609, 0.5993, 0.7595], device='cuda:0')\n",
      "val_recall: tensor([0.5459, 0.6244, 0.8853, 0.3232], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 10 time:  1.430419981683326\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.7527609542012215\n",
      "train_precision: tensor([0.6362, 0.7231, 0.7005, 0.6231], device='cuda:0')\n",
      "train_recall: tensor([0.6073, 0.7939, 0.7011, 0.2903], device='cuda:0')\n",
      "val_loss: 0.7359260068171554\n",
      "val_precision: tensor([0.6271, 0.8339, 0.6786, 0.5713], device='cuda:0')\n",
      "val_recall: tensor([0.6452, 0.6867, 0.7914, 0.4653], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 11 time:  1.4363125063500017\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.7466824063942545\n",
      "train_precision: tensor([0.6377, 0.7249, 0.7016, 0.6131], device='cuda:0')\n",
      "train_recall: tensor([0.6088, 0.7923, 0.7039, 0.3007], device='cuda:0')\n",
      "val_loss: 0.7046446310977141\n",
      "val_precision: tensor([0.6801, 0.7924, 0.6851, 0.7545], device='cuda:0')\n",
      "val_recall: tensor([0.6083, 0.7743, 0.8096, 0.3519], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 12 time:  1.4068195827166619\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.7437255014975865\n",
      "train_precision: tensor([0.6411, 0.7254, 0.7024, 0.6311], device='cuda:0')\n",
      "train_recall: tensor([0.6094, 0.7955, 0.7046, 0.3136], device='cuda:0')\n",
      "val_loss: 0.7341963403754764\n",
      "val_precision: tensor([0.6321, 0.8201, 0.7038, 0.8222], device='cuda:0')\n",
      "val_recall: tensor([0.6745, 0.7253, 0.7837, 0.2865], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 13 time:  1.4261761477166601\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.7383309528231621\n",
      "train_precision: tensor([0.6424, 0.7294, 0.7040, 0.6259], device='cuda:0')\n",
      "train_recall: tensor([0.6099, 0.7969, 0.7088, 0.3216], device='cuda:0')\n",
      "val_loss: 0.7265613332390786\n",
      "val_precision: tensor([0.6787, 0.7803, 0.6827, 0.8053], device='cuda:0')\n",
      "val_recall: tensor([0.5955, 0.7852, 0.8043, 0.3077], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 14 time:  1.4636682892666764\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.7368370804758299\n",
      "train_precision: tensor([0.6443, 0.7280, 0.7056, 0.6212], device='cuda:0')\n",
      "train_recall: tensor([0.6123, 0.7963, 0.7080, 0.3261], device='cuda:0')\n",
      "val_loss: 0.7065093461838033\n",
      "val_precision: tensor([0.7049, 0.7648, 0.6943, 0.7277], device='cuda:0')\n",
      "val_recall: tensor([0.5807, 0.8166, 0.8054, 0.3535], device='cuda:0')\n",
      "Learning rate: 5e-05\n",
      "epoch 15 time:  1.4697855357333234\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.7095842783649763\n",
      "train_precision: tensor([0.6576, 0.7417, 0.7145, 0.6503], device='cuda:0')\n",
      "train_recall: tensor([0.6258, 0.8033, 0.7220, 0.3553], device='cuda:0')\n",
      "val_loss: 0.6823979455563757\n",
      "val_precision: tensor([0.6949, 0.7881, 0.7089, 0.7466], device='cuda:0')\n",
      "val_recall: tensor([0.6232, 0.7970, 0.8066, 0.4057], device='cuda:0')\n",
      "Learning rate: 5e-05\n",
      "epoch 16 time:  1.4740011015500083\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.7054980277305558\n",
      "train_precision: tensor([0.6594, 0.7432, 0.7182, 0.6518], device='cuda:0')\n",
      "train_recall: tensor([0.6305, 0.8027, 0.7230, 0.3717], device='cuda:0')\n",
      "val_loss: 0.6904519705308808\n",
      "val_precision: tensor([0.7040, 0.7934, 0.6941, 0.7557], device='cuda:0')\n",
      "val_recall: tensor([0.6027, 0.7931, 0.8255, 0.4145], device='cuda:0')\n",
      "Learning rate: 5e-05\n",
      "epoch 17 time:  1.4715070704166615\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.703958653197402\n",
      "train_precision: tensor([0.6600, 0.7445, 0.7185, 0.6542], device='cuda:0')\n",
      "train_recall: tensor([0.6317, 0.8015, 0.7247, 0.3773], device='cuda:0')\n",
      "val_loss: 0.6813214827742842\n",
      "val_precision: tensor([0.6975, 0.8035, 0.6987, 0.7334], device='cuda:0')\n",
      "val_recall: tensor([0.6249, 0.7811, 0.8196, 0.4327], device='cuda:0')\n",
      "Learning rate: 5e-05\n",
      "epoch 18 time:  1.4670377305166766\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.7015175307080859\n",
      "train_precision: tensor([0.6623, 0.7463, 0.7188, 0.6541], device='cuda:0')\n",
      "train_recall: tensor([0.6323, 0.8043, 0.7254, 0.3820], device='cuda:0')\n",
      "val_loss: 0.6819560592373212\n",
      "val_precision: tensor([0.7004, 0.7985, 0.7007, 0.7482], device='cuda:0')\n",
      "val_recall: tensor([0.6179, 0.7917, 0.8200, 0.4192], device='cuda:0')\n",
      "Learning rate: 5e-05\n",
      "epoch 19 time:  1.4482412185666667\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n",
      "train_loss: 0.6995758661911601\n",
      "train_precision: tensor([0.6620, 0.7451, 0.7216, 0.6575], device='cuda:0')\n",
      "train_recall: tensor([0.6330, 0.8051, 0.7249, 0.3856], device='cuda:0')\n",
      "val_loss: 0.6900992068151633\n",
      "val_precision: tensor([0.7045, 0.7948, 0.6985, 0.7514], device='cuda:0')\n",
      "val_recall: tensor([0.6098, 0.7972, 0.8219, 0.4111], device='cuda:0')\n",
      "Learning rate: 5e-05\n",
      "epoch 20 time:  1.460755232733345\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 21\n",
      "train_loss: 0.6978473164495967\n",
      "train_precision: tensor([0.6652, 0.7466, 0.7220, 0.6605], device='cuda:0')\n",
      "train_recall: tensor([0.6343, 0.8052, 0.7286, 0.3872], device='cuda:0')\n",
      "val_loss: 0.6849880912237697\n",
      "val_precision: tensor([0.6954, 0.7994, 0.7087, 0.7603], device='cuda:0')\n",
      "val_recall: tensor([0.6334, 0.7937, 0.8101, 0.4047], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 21 time:  1.4733388464333226\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 22\n",
      "train_loss: 0.6975070087682633\n",
      "train_precision: tensor([0.6621, 0.7491, 0.7231, 0.6570], device='cuda:0')\n",
      "train_recall: tensor([0.6383, 0.8052, 0.7250, 0.3843], device='cuda:0')\n",
      "val_loss: 0.685551561994685\n",
      "val_precision: tensor([0.7034, 0.8013, 0.6980, 0.7359], device='cuda:0')\n",
      "val_recall: tensor([0.6160, 0.7893, 0.8243, 0.4296], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 22 time:  1.482916577916664\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 23\n",
      "train_loss: 0.6960722901281856\n",
      "train_precision: tensor([0.6643, 0.7485, 0.7221, 0.6666], device='cuda:0')\n",
      "train_recall: tensor([0.6355, 0.8062, 0.7270, 0.3958], device='cuda:0')\n",
      "val_loss: 0.6815100054774019\n",
      "val_precision: tensor([0.7026, 0.8005, 0.7022, 0.7387], device='cuda:0')\n",
      "val_recall: tensor([0.6229, 0.7908, 0.8193, 0.4340], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 23 time:  1.4723153019333344\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 24\n",
      "train_loss: 0.6958493173831986\n",
      "train_precision: tensor([0.6636, 0.7483, 0.7224, 0.6610], device='cuda:0')\n",
      "train_recall: tensor([0.6357, 0.8060, 0.7267, 0.3903], device='cuda:0')\n",
      "val_loss: 0.6837808929383755\n",
      "val_precision: tensor([0.7013, 0.7986, 0.7020, 0.7463], device='cuda:0')\n",
      "val_recall: tensor([0.6226, 0.7914, 0.8176, 0.4219], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 24 time:  1.4664816844500062\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 25\n",
      "train_loss: 0.6973439831818853\n",
      "train_precision: tensor([0.6638, 0.7477, 0.7222, 0.6535], device='cuda:0')\n",
      "train_recall: tensor([0.6355, 0.8056, 0.7262, 0.3898], device='cuda:0')\n",
      "val_loss: 0.6762597065005037\n",
      "val_precision: tensor([0.7055, 0.7904, 0.7116, 0.7404], device='cuda:0')\n",
      "val_recall: tensor([0.6240, 0.8043, 0.8108, 0.4330], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 25 time:  1.4671034945833374\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 26\n",
      "train_loss: 0.6958054905846005\n",
      "train_precision: tensor([0.6643, 0.7469, 0.7217, 0.6553], device='cuda:0')\n",
      "train_recall: tensor([0.6332, 0.8074, 0.7260, 0.3922], device='cuda:0')\n",
      "val_loss: 0.6796924442880683\n",
      "val_precision: tensor([0.7060, 0.7965, 0.7032, 0.7381], device='cuda:0')\n",
      "val_recall: tensor([0.6184, 0.7964, 0.8195, 0.4347], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 26 time:  1.470540947933326\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 27\n",
      "train_loss: 0.6964111670851707\n",
      "train_precision: tensor([0.6637, 0.7462, 0.7215, 0.6615], device='cuda:0')\n",
      "train_recall: tensor([0.6343, 0.8034, 0.7269, 0.3973], device='cuda:0')\n",
      "val_loss: 0.6819702137675551\n",
      "val_precision: tensor([0.7054, 0.7991, 0.7005, 0.7423], device='cuda:0')\n",
      "val_recall: tensor([0.6162, 0.7937, 0.8232, 0.4316], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 27 time:  1.471480758049999\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 28\n",
      "train_loss: 0.6960912253175463\n",
      "train_precision: tensor([0.6644, 0.7476, 0.7218, 0.6547], device='cuda:0')\n",
      "train_recall: tensor([0.6353, 0.8063, 0.7257, 0.3912], device='cuda:0')\n",
      "val_loss: 0.6814583115279674\n",
      "val_precision: tensor([0.7034, 0.8001, 0.7027, 0.7465], device='cuda:0')\n",
      "val_recall: tensor([0.6223, 0.7929, 0.8201, 0.4263], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 28 time:  1.4697279357333324\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 29\n",
      "train_loss: 0.6965391252722059\n",
      "train_precision: tensor([0.6639, 0.7470, 0.7224, 0.6646], device='cuda:0')\n",
      "train_recall: tensor([0.6344, 0.8066, 0.7269, 0.3889], device='cuda:0')\n",
      "val_loss: 0.686715235395564\n",
      "val_precision: tensor([0.6978, 0.8075, 0.6949, 0.7434], device='cuda:0')\n",
      "val_recall: tensor([0.6219, 0.7782, 0.8254, 0.4263], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 29 time:  1.4578210086499894\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 212\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m    211\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 212\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[1], line 104\u001b[0m, in \u001b[0;36mMyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m    102\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_recall(preds, labels)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:373\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:466\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/classification/stat_scores.py:333\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m    337\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    338\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:309\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    307\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet18\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "from vit.vit import VisionTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'vit_normalize_32x32{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.4, p=1), RandomContrast(limit=0.4, p=1)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.05, patience=2, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss},step=self.step)\n",
    "        wandb.log({'Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()},step=self.step)\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'val_Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 256\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "\n",
    "model = VisionTransformer(image_size=32, in_channels=3, num_classes=4, hidden_dims=[32, 32], dropout_rate=0.6)\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch/60}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f5d0f-599d-4e76-8716-db41af662d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
