{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07894f8-9db0-437c-9193-036d51d7b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240423_235112-4g8kmmja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/4g8kmmja' target=\"_blank\">resnet18_normalize_batch_322024-04-23 23:51:11.928714</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/4g8kmmja' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/4g8kmmja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 0.9865018075727656\n",
      "train_precision: tensor([0.5200, 0.6162, 0.5917, 0.2143], device='cuda:0')\n",
      "train_recall: tensor([0.4868, 0.7271, 0.5818, 0.0017], device='cuda:0')\n",
      "val_loss: 0.8347683799005864\n",
      "val_precision: tensor([0.5849, 0.7522, 0.6487, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.6057, 0.7093, 0.7274, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 0 time:  0.8272645684666713\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.8846138953132788\n",
      "train_precision: tensor([0.5767, 0.6676, 0.6454, 0.4286], device='cuda:0')\n",
      "train_recall: tensor([0.5510, 0.7635, 0.6424, 0.0229], device='cuda:0')\n",
      "val_loss: 0.7775489191283278\n",
      "val_precision: tensor([0.6688, 0.6802, 0.6746, 0.6649], device='cuda:0')\n",
      "val_recall: tensor([0.5122, 0.8271, 0.7376, 0.1650], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 1 time:  0.840995898483349\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.8461898323287202\n",
      "train_precision: tensor([0.6005, 0.6793, 0.6671, 0.5251], device='cuda:0')\n",
      "train_recall: tensor([0.5665, 0.7860, 0.6551, 0.0922], device='cuda:0')\n",
      "val_loss: 0.7537485524142178\n",
      "val_precision: tensor([0.6980, 0.7047, 0.6693, 0.6297], device='cuda:0')\n",
      "val_recall: tensor([0.4997, 0.8441, 0.7576, 0.3418], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 2 time:  0.8379808708499998\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.8176974382097252\n",
      "train_precision: tensor([0.6155, 0.6898, 0.6777, 0.5627], device='cuda:0')\n",
      "train_recall: tensor([0.5722, 0.7948, 0.6703, 0.1639], device='cuda:0')\n",
      "val_loss: 0.71838329152008\n",
      "val_precision: tensor([0.6670, 0.7372, 0.7046, 0.7105], device='cuda:0')\n",
      "val_recall: tensor([0.6091, 0.8291, 0.7184, 0.3017], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 3 time:  0.8355905338166546\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.7979454418751396\n",
      "train_precision: tensor([0.6256, 0.7003, 0.6830, 0.6094], device='cuda:0')\n",
      "train_recall: tensor([0.5812, 0.7982, 0.6812, 0.2058], device='cuda:0')\n",
      "val_loss: 0.711920078551297\n",
      "val_precision: tensor([0.6818, 0.7793, 0.6693, 0.6347], device='cuda:0')\n",
      "val_recall: tensor([0.5736, 0.7789, 0.7989, 0.4172], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 4 time:  0.8388608460500109\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.7827751043787804\n",
      "train_precision: tensor([0.6358, 0.7040, 0.6915, 0.6188], device='cuda:0')\n",
      "train_recall: tensor([0.5890, 0.8032, 0.6886, 0.2281], device='cuda:0')\n",
      "val_loss: 0.692330213679134\n",
      "val_precision: tensor([0.6451, 0.7822, 0.7447, 0.5946], device='cuda:0')\n",
      "val_recall: tensor([0.6886, 0.7865, 0.7102, 0.4360], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 5 time:  0.8372532508333582\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.7701157380027503\n",
      "train_precision: tensor([0.6427, 0.7114, 0.6967, 0.6453], device='cuda:0')\n",
      "train_recall: tensor([0.5958, 0.8056, 0.6972, 0.2570], device='cuda:0')\n",
      "val_loss: 0.6692114747838066\n",
      "val_precision: tensor([0.6810, 0.7712, 0.7278, 0.6662], device='cuda:0')\n",
      "val_recall: tensor([0.6363, 0.8197, 0.7544, 0.4401], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 6 time:  0.8418877993666684\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.7600229263643016\n",
      "train_precision: tensor([0.6449, 0.7139, 0.7005, 0.6617], device='cuda:0')\n",
      "train_recall: tensor([0.6004, 0.8083, 0.6978, 0.2690], device='cuda:0')\n",
      "val_loss: 0.682917950595933\n",
      "val_precision: tensor([0.6715, 0.8185, 0.6802, 0.7418], device='cuda:0')\n",
      "val_recall: tensor([0.6344, 0.7349, 0.8178, 0.4091], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 7 time:  0.8390996144166517\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.7517111229646195\n",
      "train_precision: tensor([0.6506, 0.7174, 0.7050, 0.6505], device='cuda:0')\n",
      "train_recall: tensor([0.6055, 0.8107, 0.7029, 0.2745], device='cuda:0')\n",
      "val_loss: 0.6600830750261603\n",
      "val_precision: tensor([0.6745, 0.7865, 0.7378, 0.8540], device='cuda:0')\n",
      "val_recall: tensor([0.6722, 0.8049, 0.7656, 0.3623], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 8 time:  0.8713212967333371\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.7425364118745222\n",
      "train_precision: tensor([0.6562, 0.7199, 0.7085, 0.6613], device='cuda:0')\n",
      "train_recall: tensor([0.6082, 0.8132, 0.7075, 0.2973], device='cuda:0')\n",
      "val_loss: 0.6426266686443325\n",
      "val_precision: tensor([0.6917, 0.7739, 0.7567, 0.6382], device='cuda:0')\n",
      "val_recall: tensor([0.6669, 0.8389, 0.7344, 0.5192], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 9 time:  0.8719300466333455\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.7365035915884334\n",
      "train_precision: tensor([0.6559, 0.7223, 0.7115, 0.6758], device='cuda:0')\n",
      "train_recall: tensor([0.6102, 0.8150, 0.7087, 0.3065], device='cuda:0')\n",
      "val_loss: 0.6384202614478389\n",
      "val_precision: tensor([0.7341, 0.7430, 0.7448, 0.6990], device='cuda:0')\n",
      "val_recall: tensor([0.6004, 0.8721, 0.7743, 0.4801], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 10 time:  0.868268889233362\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.7280600944693187\n",
      "train_precision: tensor([0.6631, 0.7273, 0.7134, 0.6809], device='cuda:0')\n",
      "train_recall: tensor([0.6136, 0.8178, 0.7148, 0.3273], device='cuda:0')\n",
      "val_loss: 0.6337669995497547\n",
      "val_precision: tensor([0.6939, 0.7843, 0.7438, 0.7372], device='cuda:0')\n",
      "val_recall: tensor([0.6647, 0.8279, 0.7641, 0.4364], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 11 time:  0.8688289893833523\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.7215507726451631\n",
      "train_precision: tensor([0.6649, 0.7271, 0.7180, 0.6826], device='cuda:0')\n",
      "train_recall: tensor([0.6148, 0.8198, 0.7175, 0.3320], device='cuda:0')\n",
      "val_loss: 0.6378809893562033\n",
      "val_precision: tensor([0.7237, 0.7851, 0.7183, 0.8516], device='cuda:0')\n",
      "val_recall: tensor([0.6279, 0.8344, 0.8072, 0.3902], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 12 time:  0.8510311681999762\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.716480370536602\n",
      "train_precision: tensor([0.6680, 0.7299, 0.7180, 0.6906], device='cuda:0')\n",
      "train_recall: tensor([0.6184, 0.8195, 0.7192, 0.3446], device='cuda:0')\n",
      "val_loss: 0.6354573326983992\n",
      "val_precision: tensor([0.7544, 0.7843, 0.6913, 0.7912], device='cuda:0')\n",
      "val_recall: tensor([0.5795, 0.8318, 0.8394, 0.4515], device='cuda:0')\n",
      "Learning rate: 0.01\n",
      "epoch 13 time:  0.8513634817833311\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.7106272515829454\n",
      "train_precision: tensor([0.6695, 0.7359, 0.7233, 0.6886], device='cuda:0')\n",
      "train_recall: tensor([0.6295, 0.8203, 0.7194, 0.3475], device='cuda:0')\n",
      "val_loss: 0.6513883230108781\n",
      "val_precision: tensor([0.6319, 0.8278, 0.7470, 0.7806], device='cuda:0')\n",
      "val_recall: tensor([0.7308, 0.7386, 0.7431, 0.4407], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 14 time:  0.8282972533666604\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.677142267772084\n",
      "train_precision: tensor([0.6869, 0.7458, 0.7349, 0.7052], device='cuda:0')\n",
      "train_recall: tensor([0.6409, 0.8308, 0.7338, 0.3835], device='cuda:0')\n",
      "val_loss: 0.6098284828033275\n",
      "val_precision: tensor([0.7047, 0.8253, 0.7315, 0.7539], device='cuda:0')\n",
      "val_recall: tensor([0.6735, 0.7957, 0.8147, 0.5003], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 15 time:  0.8323456428833197\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.665809266151578\n",
      "train_precision: tensor([0.6897, 0.7506, 0.7409, 0.7185], device='cuda:0')\n",
      "train_recall: tensor([0.6462, 0.8331, 0.7380, 0.4104], device='cuda:0')\n",
      "val_loss: 0.5954118015566909\n",
      "val_precision: tensor([0.7299, 0.8126, 0.7325, 0.7680], device='cuda:0')\n",
      "val_recall: tensor([0.6575, 0.8244, 0.8205, 0.4960], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 16 time:  0.8370091103499969\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.6617145760360487\n",
      "train_precision: tensor([0.6911, 0.7534, 0.7405, 0.7238], device='cuda:0')\n",
      "train_recall: tensor([0.6496, 0.8314, 0.7397, 0.4170], device='cuda:0')\n",
      "val_loss: 0.5953399565037213\n",
      "val_precision: tensor([0.7240, 0.8192, 0.7311, 0.7832], device='cuda:0')\n",
      "val_recall: tensor([0.6655, 0.8132, 0.8226, 0.4939], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 17 time:  0.8348739390166884\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.6568001562289478\n",
      "train_precision: tensor([0.6915, 0.7576, 0.7455, 0.7168], device='cuda:0')\n",
      "train_recall: tensor([0.6564, 0.8322, 0.7400, 0.4270], device='cuda:0')\n",
      "val_loss: 0.5939161464768702\n",
      "val_precision: tensor([0.7171, 0.8191, 0.7397, 0.7653], device='cuda:0')\n",
      "val_recall: tensor([0.6806, 0.8121, 0.8089, 0.5051], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 18 time:  0.8374909187000109\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.6553058957712686\n",
      "train_precision: tensor([0.6923, 0.7582, 0.7471, 0.7267], device='cuda:0')\n",
      "train_recall: tensor([0.6593, 0.8323, 0.7410, 0.4228], device='cuda:0')\n",
      "val_loss: 0.5920561895323263\n",
      "val_precision: tensor([0.7416, 0.8055, 0.7320, 0.7541], device='cuda:0')\n",
      "val_recall: tensor([0.6468, 0.8333, 0.8238, 0.5111], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 19 time:  0.8362416441833375\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n",
      "train_loss: 0.6532024326985316\n",
      "train_precision: tensor([0.6969, 0.7576, 0.7450, 0.7303], device='cuda:0')\n",
      "train_recall: tensor([0.6576, 0.8319, 0.7441, 0.4341], device='cuda:0')\n",
      "val_loss: 0.5901517500379916\n",
      "val_precision: tensor([0.7402, 0.8048, 0.7348, 0.7613], device='cuda:0')\n",
      "val_recall: tensor([0.6494, 0.8356, 0.8210, 0.5111], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 20 time:  0.8458928691166572\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 21\n",
      "train_loss: 0.652388712103288\n",
      "train_precision: tensor([0.6964, 0.7577, 0.7455, 0.7192], device='cuda:0')\n",
      "train_recall: tensor([0.6574, 0.8347, 0.7411, 0.4328], device='cuda:0')\n",
      "val_loss: 0.594794133441203\n",
      "val_precision: tensor([0.7206, 0.8230, 0.7366, 0.7562], device='cuda:0')\n",
      "val_recall: tensor([0.6759, 0.8090, 0.8177, 0.5212], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 21 time:  0.8508400525333248\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 22\n",
      "train_loss: 0.6512994753096407\n",
      "train_precision: tensor([0.6954, 0.7607, 0.7445, 0.7211], device='cuda:0')\n",
      "train_recall: tensor([0.6592, 0.8327, 0.7428, 0.4304], device='cuda:0')\n",
      "val_loss: 0.5859897137154217\n",
      "val_precision: tensor([0.7331, 0.8119, 0.7445, 0.7440], device='cuda:0')\n",
      "val_recall: tensor([0.6673, 0.8292, 0.8148, 0.5508], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 22 time:  0.8713767781666927\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 23\n",
      "train_loss: 0.64830597110774\n",
      "train_precision: tensor([0.6971, 0.7606, 0.7476, 0.7277], device='cuda:0')\n",
      "train_recall: tensor([0.6604, 0.8337, 0.7448, 0.4381], device='cuda:0')\n",
      "val_loss: 0.5811713470030858\n",
      "val_precision: tensor([0.7315, 0.8123, 0.7479, 0.7523], device='cuda:0')\n",
      "val_recall: tensor([0.6722, 0.8304, 0.8128, 0.5411], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 23 time:  0.8524965040333276\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 24\n",
      "train_loss: 0.6467366321947307\n",
      "train_precision: tensor([0.6972, 0.7623, 0.7452, 0.7224], device='cuda:0')\n",
      "train_recall: tensor([0.6588, 0.8312, 0.7475, 0.4455], device='cuda:0')\n",
      "val_loss: 0.5901786920208746\n",
      "val_precision: tensor([0.7133, 0.8304, 0.7400, 0.7509], device='cuda:0')\n",
      "val_recall: tensor([0.6880, 0.7994, 0.8142, 0.5441], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 24 time:  0.8181814528166493\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 25\n",
      "train_loss: 0.6458389690794475\n",
      "train_precision: tensor([0.6979, 0.7638, 0.7472, 0.7248], device='cuda:0')\n",
      "train_recall: tensor([0.6622, 0.8321, 0.7466, 0.4530], device='cuda:0')\n",
      "val_loss: 0.5797882952091179\n",
      "val_precision: tensor([0.7356, 0.8063, 0.7507, 0.7602], device='cuda:0')\n",
      "val_recall: tensor([0.6690, 0.8409, 0.8090, 0.5316], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 25 time:  0.8373820056666773\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 26\n",
      "train_loss: 0.6460528892639992\n",
      "train_precision: tensor([0.6966, 0.7627, 0.7480, 0.7304], device='cuda:0')\n",
      "train_recall: tensor([0.6627, 0.8315, 0.7456, 0.4508], device='cuda:0')\n",
      "val_loss: 0.5844107280544227\n",
      "val_precision: tensor([0.7241, 0.8187, 0.7496, 0.7348], device='cuda:0')\n",
      "val_recall: tensor([0.6814, 0.8240, 0.8068, 0.5606], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 26 time:  0.8143400632333396\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 27\n",
      "train_loss: 0.6450945168183182\n",
      "train_precision: tensor([0.6998, 0.7623, 0.7500, 0.7106], device='cuda:0')\n",
      "train_recall: tensor([0.6609, 0.8358, 0.7473, 0.4462], device='cuda:0')\n",
      "val_loss: 0.583061776609989\n",
      "val_precision: tensor([0.7231, 0.8148, 0.7513, 0.7542], device='cuda:0')\n",
      "val_recall: tensor([0.6808, 0.8272, 0.8051, 0.5414], device='cuda:0')\n",
      "Learning rate: 0.0005\n",
      "epoch 27 time:  0.8170407203833141\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 28\n",
      "train_loss: 0.6424047447827589\n",
      "train_precision: tensor([0.7011, 0.7627, 0.7489, 0.7257], device='cuda:0')\n",
      "train_recall: tensor([0.6598, 0.8347, 0.7497, 0.4609], device='cuda:0')\n",
      "val_loss: 0.5855371906157373\n",
      "val_precision: tensor([0.7113, 0.8251, 0.7531, 0.7541], device='cuda:0')\n",
      "val_recall: tensor([0.6991, 0.8060, 0.8051, 0.5380], device='cuda:0')\n",
      "Learning rate: 2.5e-05\n",
      "epoch 28 time:  0.867776124033359\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 29\n",
      "train_loss: 0.640943710465569\n",
      "train_precision: tensor([0.6972, 0.7650, 0.7514, 0.7236], device='cuda:0')\n",
      "train_recall: tensor([0.6679, 0.8324, 0.7447, 0.4545], device='cuda:0')\n",
      "val_loss: 0.5821104041916202\n",
      "val_precision: tensor([0.7218, 0.8203, 0.7477, 0.7680], device='cuda:0')\n",
      "val_recall: tensor([0.6848, 0.8174, 0.8118, 0.5306], device='cuda:0')\n",
      "Learning rate: 2.5e-05\n",
      "epoch 29 time:  1.083030691599985\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 30\n",
      "train_loss: 0.6379599225198829\n",
      "train_precision: tensor([0.7011, 0.7674, 0.7510, 0.7351], device='cuda:0')\n",
      "train_recall: tensor([0.6675, 0.8343, 0.7502, 0.4544], device='cuda:0')\n",
      "val_loss: 0.5814311768551557\n",
      "val_precision: tensor([0.7211, 0.8208, 0.7516, 0.7598], device='cuda:0')\n",
      "val_recall: tensor([0.6879, 0.8197, 0.8089, 0.5411], device='cuda:0')\n",
      "Learning rate: 2.5e-05\n",
      "epoch 30 time:  0.859690570816656\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 31\n",
      "train_loss: 0.6405969735924104\n",
      "train_precision: tensor([0.6995, 0.7646, 0.7493, 0.7150], device='cuda:0')\n",
      "train_recall: tensor([0.6642, 0.8331, 0.7471, 0.4564], device='cuda:0')\n",
      "val_loss: 0.5830470885410957\n",
      "val_precision: tensor([0.7281, 0.8198, 0.7449, 0.7550], device='cuda:0')\n",
      "val_recall: tensor([0.6762, 0.8205, 0.8183, 0.5438], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 31 time:  0.8518759433833414\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 32\n",
      "train_loss: 0.6382704376930242\n",
      "train_precision: tensor([0.7014, 0.7649, 0.7501, 0.7269], device='cuda:0')\n",
      "train_recall: tensor([0.6640, 0.8347, 0.7492, 0.4602], device='cuda:0')\n",
      "val_loss: 0.5779935153171452\n",
      "val_precision: tensor([0.7227, 0.8124, 0.7602, 0.7759], device='cuda:0')\n",
      "val_recall: tensor([0.6909, 0.8311, 0.8003, 0.5293], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 32 time:  0.874331503600024\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 33\n",
      "train_loss: 0.6401873143045257\n",
      "train_precision: tensor([0.6992, 0.7643, 0.7495, 0.7240], device='cuda:0')\n",
      "train_recall: tensor([0.6653, 0.8313, 0.7475, 0.4606], device='cuda:0')\n",
      "val_loss: 0.5821006600385825\n",
      "val_precision: tensor([0.7263, 0.8207, 0.7452, 0.7624], device='cuda:0')\n",
      "val_recall: tensor([0.6784, 0.8191, 0.8172, 0.5444], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 33 time:  0.8522999260166519\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 34\n",
      "train_loss: 0.6400716949386329\n",
      "train_precision: tensor([0.7000, 0.7640, 0.7486, 0.7319], device='cuda:0')\n",
      "train_recall: tensor([0.6631, 0.8322, 0.7494, 0.4569], device='cuda:0')\n",
      "val_loss: 0.5845423095637597\n",
      "val_precision: tensor([0.7260, 0.8242, 0.7389, 0.7696], device='cuda:0')\n",
      "val_recall: tensor([0.6746, 0.8143, 0.8226, 0.5354], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 34 time:  0.8506419043666634\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 35\n",
      "train_loss: 0.6403289979586191\n",
      "train_precision: tensor([0.7002, 0.7655, 0.7492, 0.7298], device='cuda:0')\n",
      "train_recall: tensor([0.6634, 0.8332, 0.7503, 0.4576], device='cuda:0')\n",
      "val_loss: 0.5811442699984928\n",
      "val_precision: tensor([0.7298, 0.8164, 0.7467, 0.7444], device='cuda:0')\n",
      "val_recall: tensor([0.6743, 0.8253, 0.8145, 0.5532], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 35 time:  0.8497219634999965\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 36\n",
      "train_loss: 0.6401918274254752\n",
      "train_precision: tensor([0.7010, 0.7647, 0.7507, 0.7280], device='cuda:0')\n",
      "train_recall: tensor([0.6656, 0.8323, 0.7490, 0.4693], device='cuda:0')\n",
      "val_loss: 0.5860952808391601\n",
      "val_precision: tensor([0.7313, 0.8244, 0.7345, 0.7549], device='cuda:0')\n",
      "val_recall: tensor([0.6678, 0.8148, 0.8272, 0.5455], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 36 time:  0.8492372486499941\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 37\n",
      "train_loss: 0.6382976651928558\n",
      "train_precision: tensor([0.7010, 0.7656, 0.7490, 0.7277], device='cuda:0')\n",
      "train_recall: tensor([0.6642, 0.8338, 0.7488, 0.4644], device='cuda:0')\n",
      "val_loss: 0.5811328069488774\n",
      "val_precision: tensor([0.7228, 0.8148, 0.7543, 0.7659], device='cuda:0')\n",
      "val_recall: tensor([0.6853, 0.8270, 0.8049, 0.5343], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 37 time:  0.8543264312500166\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 214\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m    213\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 214\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m, in \u001b[0;36mMyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m     95\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet18\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'resnet18_normalize_batch_32{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.05, patience=2, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss},step=self.step)\n",
    "        wandb.log({'Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()},step=self.step)\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'val_Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 32\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.00005\n",
    "\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "num_classes = 4\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(model.fc.in_features, num_classes))\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch/60}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8607b0-4928-4e08-9bfd-ea6df43fdbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[666 106 218  10]\n",
      " [ 99 830  64   7]\n",
      " [118  42 834   6]\n",
      " [ 12   8  19  61]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.70      1000\n",
      "           1       0.84      0.83      0.84      1000\n",
      "           2       0.73      0.83      0.78      1000\n",
      "           3       0.73      0.61      0.66       100\n",
      "\n",
      "    accuracy                           0.77      3100\n",
      "   macro avg       0.76      0.73      0.75      3100\n",
      "weighted avg       0.77      0.77      0.77      3100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch \n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def test_report(model, dataloader):\n",
    "    \"\"\"Prints confusion matrix for testing dataset\n",
    "    dataloader should be of batch_size=1.\"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            output = model(data)\n",
    "            label = label.numpy()\n",
    "            output = output.numpy()\n",
    "            y_pred.append(np.argmax(output))\n",
    "            y_test.append(np.argmax(label))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "testset =ImageDataset(data_path='test_data', transform=transform_test)\n",
    "dataloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "test_report(my_model.to('cpu'), dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d074b-cd1d-4e21-9260-a7583b04629e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
