{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7465aa-01bf-4d4f-b74e-0ebcbd21a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240423_002723-plp4325l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/plp4325l' target=\"_blank\">resnet18_16batch_2024-04-23 00:27:22.144376</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/plp4325l' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/plp4325l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 0.9025787634900865\n",
      "train_precision: tensor([0.5427, 0.6737, 0.6314, 0.5463], device='cuda:0')\n",
      "train_recall: tensor([0.5325, 0.7543, 0.6259, 0.0434], device='cuda:0')\n",
      "val_loss: 0.8126742279731534\n",
      "val_precision: tensor([0.5511, 0.7584, 0.7127, 0.7302], device='cuda:0')\n",
      "val_recall: tensor([0.7033, 0.7320, 0.5953, 0.1704], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 0 time:  1.4835906824333354\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.8157382512628397\n",
      "train_precision: tensor([0.5901, 0.7061, 0.6709, 0.6616], device='cuda:0')\n",
      "train_recall: tensor([0.5678, 0.7826, 0.6753, 0.1518], device='cuda:0')\n",
      "val_loss: 0.7711266155324741\n",
      "val_precision: tensor([0.6457, 0.8007, 0.6195, 0.5987], device='cuda:0')\n",
      "val_recall: tensor([0.5532, 0.7054, 0.8111, 0.3165], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 1 time:  1.4967827012500001\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.7709215901147816\n",
      "train_precision: tensor([0.6107, 0.7264, 0.6910, 0.6573], device='cuda:0')\n",
      "train_recall: tensor([0.5913, 0.7909, 0.6940, 0.2557], device='cuda:0')\n",
      "val_loss: 0.7457722668708354\n",
      "val_precision: tensor([0.6889, 0.7797, 0.6343, 0.5435], device='cuda:0')\n",
      "val_recall: tensor([0.4942, 0.7695, 0.8325, 0.4519], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 2 time:  1.500418770416665\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.7419343360321622\n",
      "train_precision: tensor([0.6291, 0.7395, 0.7024, 0.6618], device='cuda:0')\n",
      "train_recall: tensor([0.6080, 0.7971, 0.7074, 0.3208], device='cuda:0')\n",
      "val_loss: 0.6916533945049026\n",
      "val_precision: tensor([0.6738, 0.7604, 0.6994, 0.7291], device='cuda:0')\n",
      "val_recall: tensor([0.5941, 0.8204, 0.7568, 0.4168], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 3 time:  1.5191626869999975\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.7179072285440714\n",
      "train_precision: tensor([0.6397, 0.7500, 0.7102, 0.6752], device='cuda:0')\n",
      "train_recall: tensor([0.6179, 0.8033, 0.7163, 0.3671], device='cuda:0')\n",
      "val_loss: 0.7323373537442666\n",
      "val_precision: tensor([0.6504, 0.6798, 0.7724, 0.7099], device='cuda:0')\n",
      "val_recall: tensor([0.6116, 0.8738, 0.6321, 0.3980], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 4 time:  1.5254789476000004\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.7000316187518524\n",
      "train_precision: tensor([0.6497, 0.7572, 0.7214, 0.6754], device='cuda:0')\n",
      "train_recall: tensor([0.6312, 0.8085, 0.7221, 0.4032], device='cuda:0')\n",
      "val_loss: 0.6910010286070387\n",
      "val_precision: tensor([0.6095, 0.8591, 0.7313, 0.5630], device='cuda:0')\n",
      "val_recall: tensor([0.7311, 0.7074, 0.7201, 0.5192], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 5 time:  1.5497836382166648\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.6841275856046459\n",
      "train_precision: tensor([0.6573, 0.7618, 0.7284, 0.6861], device='cuda:0')\n",
      "train_recall: tensor([0.6381, 0.8130, 0.7276, 0.4325], device='cuda:0')\n",
      "val_loss: 0.7200344318053082\n",
      "val_precision: tensor([0.6632, 0.8606, 0.6443, 0.5558], device='cuda:0')\n",
      "val_recall: tensor([0.5951, 0.6937, 0.8352, 0.5582], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 6 time:  1.5285180614166696\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.6707146932966987\n",
      "train_precision: tensor([0.6651, 0.7695, 0.7349, 0.6863], device='cuda:0')\n",
      "train_recall: tensor([0.6516, 0.8167, 0.7309, 0.4417], device='cuda:0')\n",
      "val_loss: 0.6332300959334594\n",
      "val_precision: tensor([0.6918, 0.7877, 0.7278, 0.7413], device='cuda:0')\n",
      "val_recall: tensor([0.6320, 0.8321, 0.7719, 0.5162], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 7 time:  1.5404650546166674\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.656435303671358\n",
      "train_precision: tensor([0.6728, 0.7739, 0.7392, 0.6991], device='cuda:0')\n",
      "train_recall: tensor([0.6521, 0.8197, 0.7411, 0.4818], device='cuda:0')\n",
      "val_loss: 0.6222342630348239\n",
      "val_precision: tensor([0.6814, 0.7947, 0.7687, 0.7116], device='cuda:0')\n",
      "val_recall: tensor([0.6881, 0.8276, 0.7500, 0.5209], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 8 time:  1.54511856925\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.6448027951211578\n",
      "train_precision: tensor([0.6768, 0.7800, 0.7443, 0.7089], device='cuda:0')\n",
      "train_recall: tensor([0.6619, 0.8215, 0.7424, 0.5049], device='cuda:0')\n",
      "val_loss: 0.6436105389889171\n",
      "val_precision: tensor([0.6866, 0.8013, 0.7273, 0.5750], device='cuda:0')\n",
      "val_recall: tensor([0.6540, 0.8009, 0.7563, 0.6209], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 9 time:  1.5229770498666653\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.6334643673151454\n",
      "train_precision: tensor([0.6807, 0.7827, 0.7503, 0.7012], device='cuda:0')\n",
      "train_recall: tensor([0.6674, 0.8264, 0.7436, 0.5100], device='cuda:0')\n",
      "val_loss: 0.648480663848612\n",
      "val_precision: tensor([0.7033, 0.7266, 0.7784, 0.7582], device='cuda:0')\n",
      "val_recall: tensor([0.6304, 0.8851, 0.7132, 0.5246], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 10 time:  1.525873705799999\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.6217414712865044\n",
      "train_precision: tensor([0.6888, 0.7878, 0.7568, 0.7056], device='cuda:0')\n",
      "train_recall: tensor([0.6754, 0.8303, 0.7488, 0.5367], device='cuda:0')\n",
      "val_loss: 0.6028046401102681\n",
      "val_precision: tensor([0.6964, 0.8061, 0.7630, 0.6628], device='cuda:0')\n",
      "val_recall: tensor([0.6848, 0.8239, 0.7658, 0.6024], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 11 time:  1.5470243366833338\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.6110189852768164\n",
      "train_precision: tensor([0.6934, 0.7915, 0.7600, 0.7157], device='cuda:0')\n",
      "train_recall: tensor([0.6800, 0.8321, 0.7524, 0.5577], device='cuda:0')\n",
      "val_loss: 0.59587592545421\n",
      "val_precision: tensor([0.7046, 0.7818, 0.7801, 0.7850], device='cuda:0')\n",
      "val_recall: tensor([0.6854, 0.8582, 0.7488, 0.5458], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 12 time:  1.5472367082500038\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.601482276754874\n",
      "train_precision: tensor([0.6993, 0.7962, 0.7649, 0.7275], device='cuda:0')\n",
      "train_recall: tensor([0.6861, 0.8352, 0.7585, 0.5687], device='cuda:0')\n",
      "val_loss: 0.6010574139182098\n",
      "val_precision: tensor([0.7210, 0.8113, 0.7290, 0.7464], device='cuda:0')\n",
      "val_recall: tensor([0.6453, 0.8333, 0.8043, 0.5579], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 13 time:  1.5321828035500025\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.591637203226967\n",
      "train_precision: tensor([0.7018, 0.8003, 0.7698, 0.7240], device='cuda:0')\n",
      "train_recall: tensor([0.6919, 0.8393, 0.7583, 0.5825], device='cuda:0')\n",
      "val_loss: 0.6597767846880531\n",
      "val_precision: tensor([0.6549, 0.8639, 0.7117, 0.6001], device='cuda:0')\n",
      "val_recall: tensor([0.7087, 0.7074, 0.7816, 0.6057], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 14 time:  1.5220870065833347\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.5823940478147093\n",
      "train_precision: tensor([0.7081, 0.8030, 0.7744, 0.7340], device='cuda:0')\n",
      "train_recall: tensor([0.6969, 0.8405, 0.7640, 0.6072], device='cuda:0')\n",
      "val_loss: 0.6149411073386617\n",
      "val_precision: tensor([0.7360, 0.8257, 0.7040, 0.7605], device='cuda:0')\n",
      "val_recall: tensor([0.6235, 0.8117, 0.8399, 0.5838], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 15 time:  1.5257516063000063\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.5353531192891328\n",
      "train_precision: tensor([0.7309, 0.8202, 0.7935, 0.7640], device='cuda:0')\n",
      "train_recall: tensor([0.7208, 0.8548, 0.7815, 0.6625], device='cuda:0')\n",
      "val_loss: 0.5720213062400977\n",
      "val_precision: tensor([0.7171, 0.8356, 0.7580, 0.7700], device='cuda:0')\n",
      "val_recall: tensor([0.7027, 0.8249, 0.7981, 0.6155], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 16 time:  1.51762766945\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.5227764660369492\n",
      "train_precision: tensor([0.7348, 0.8254, 0.7978, 0.7841], device='cuda:0')\n",
      "train_recall: tensor([0.7268, 0.8565, 0.7866, 0.6843], device='cuda:0')\n",
      "val_loss: 0.5702235505496234\n",
      "val_precision: tensor([0.7179, 0.8295, 0.7678, 0.7546], device='cuda:0')\n",
      "val_recall: tensor([0.7047, 0.8320, 0.7916, 0.6357], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 17 time:  1.504820372166667\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.5156861031481388\n",
      "train_precision: tensor([0.7393, 0.8276, 0.8036, 0.7819], device='cuda:0')\n",
      "train_recall: tensor([0.7330, 0.8603, 0.7882, 0.6902], device='cuda:0')\n",
      "val_loss: 0.5687370373646556\n",
      "val_precision: tensor([0.7285, 0.8381, 0.7555, 0.7570], device='cuda:0')\n",
      "val_recall: tensor([0.6953, 0.8271, 0.8120, 0.6357], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 18 time:  1.5219396957999984\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.5095118227925776\n",
      "train_precision: tensor([0.7427, 0.8312, 0.8044, 0.7909], device='cuda:0')\n",
      "train_recall: tensor([0.7367, 0.8603, 0.7919, 0.7003], device='cuda:0')\n",
      "val_loss: 0.5656876129626607\n",
      "val_precision: tensor([0.7272, 0.8322, 0.7661, 0.7449], device='cuda:0')\n",
      "val_recall: tensor([0.7022, 0.8375, 0.7979, 0.6441], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 19 time:  1.5070012521000005\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n",
      "train_loss: 0.5053489643575723\n",
      "train_precision: tensor([0.7451, 0.8308, 0.8091, 0.7876], device='cuda:0')\n",
      "train_recall: tensor([0.7388, 0.8625, 0.7925, 0.7154], device='cuda:0')\n",
      "val_loss: 0.5686912051407697\n",
      "val_precision: tensor([0.7322, 0.8317, 0.7542, 0.7659], device='cuda:0')\n",
      "val_recall: tensor([0.6881, 0.8358, 0.8097, 0.6269], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 20 time:  1.4808005912333329\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 21\n",
      "train_loss: 0.49939835532237886\n",
      "train_precision: tensor([0.7486, 0.8339, 0.8093, 0.7930], device='cuda:0')\n",
      "train_recall: tensor([0.7414, 0.8647, 0.7955, 0.7126], device='cuda:0')\n",
      "val_loss: 0.5660597479040471\n",
      "val_precision: tensor([0.7260, 0.8357, 0.7646, 0.7346], device='cuda:0')\n",
      "val_recall: tensor([0.7021, 0.8303, 0.8015, 0.6700], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 21 time:  1.4788905409000033\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 22\n",
      "train_loss: 0.4952456895768345\n",
      "train_precision: tensor([0.7490, 0.8356, 0.8119, 0.7911], device='cuda:0')\n",
      "train_recall: tensor([0.7427, 0.8649, 0.7973, 0.7224], device='cuda:0')\n",
      "val_loss: 0.5630457473088616\n",
      "val_precision: tensor([0.7344, 0.8287, 0.7651, 0.7485], device='cuda:0')\n",
      "val_recall: tensor([0.6936, 0.8413, 0.8041, 0.6684], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 22 time:  1.4998787139166703\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 23\n",
      "train_loss: 0.4901133502314094\n",
      "train_precision: tensor([0.7536, 0.8375, 0.8139, 0.8013], device='cuda:0')\n",
      "train_recall: tensor([0.7457, 0.8684, 0.7994, 0.7325], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    201\u001b[0m my_model\u001b[38;5;241m.\u001b[39mtrain_one_epoch(trainloader)\n\u001b[0;32m--> 202\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n\u001b[1;32m    205\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m val_loss\n",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m, in \u001b[0;36mMyModel.evaluate\u001b[0;34m(self, testloader)\u001b[0m\n\u001b[1;32m    134\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    135\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_recall(preds, labels)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:363\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m global_state \u001b[38;5;241m=\u001b[39m {attr: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defaults}\n\u001b[1;32m    362\u001b[0m _update_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_count\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# local synchronization settings\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_on_step\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:682\u001b[0m, in \u001b[0;36mMetric.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m current_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(default, Tensor):\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, \u001b[43mdefault\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(current_val\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, [])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet18\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'resnet18_16batch_{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        OneOf([RandomBrightness(limit=0.4, p=1), RandomContrast(limit=0.4, p=1)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),]\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=2, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss},step=self.step)\n",
    "        wandb.log({'Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()},step=self.step)\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'val_Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 16\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data')\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "num_classes = 4\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(model.fc.in_features, num_classes))\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch/60}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1243363d-30a9-4de2-8e1b-85dc6d75ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_checkpoints/resnet18_16batch_2024-04-23 00:27:22.144376.pth\n"
     ]
    }
   ],
   "source": [
    "print(f'{run_path}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97296bec-b108-46dd-a9c3-3e904c66d1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
