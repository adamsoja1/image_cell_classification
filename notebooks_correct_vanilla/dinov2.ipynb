{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3248a29d-bae9-4dfd-b13b-9d74b0a94959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240424_215756-saduehxf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/saduehxf' target=\"_blank\">dinov2_42x42_1024batch2024-04-24 21:57:55.581912</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/saduehxf' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/saduehxf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/adam/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 1.4373444369861057\n",
      "train_precision: tensor([0.3917, 0.4803, 0.4096, 0.0325], device='cuda:0')\n",
      "train_recall: tensor([0.3823, 0.5644, 0.3855, 0.0025], device='cuda:0')\n",
      "val_loss: 1.0736426684591505\n",
      "val_precision: tensor([0.4131, 0.5466, 0.5368, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.6086, 0.6828, 0.2028, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 0 time:  0.45747745426666975\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 1.0465719418866293\n",
      "train_precision: tensor([0.4334, 0.5673, 0.5094, 0.0000], device='cuda:0')\n",
      "train_recall: tensor([0.3538, 0.7228, 0.5143, 0.0000], device='cuda:0')\n",
      "val_loss: 0.9866776532597012\n",
      "val_precision: tensor([0.4825, 0.6186, 0.5592, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.4296, 0.7462, 0.5611, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 1 time:  0.4959801736499988\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.9907404380185264\n",
      "train_precision: tensor([0.4877, 0.6102, 0.5596, 0.0000], device='cuda:0')\n",
      "train_recall: tensor([0.4270, 0.7230, 0.5818, 0.0000], device='cuda:0')\n",
      "val_loss: 0.9422531088193258\n",
      "val_precision: tensor([0.5361, 0.6730, 0.5759, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.4645, 0.7197, 0.6704, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 2 time:  0.48887000973333367\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.9508751727285839\n",
      "train_precision: tensor([0.5218, 0.6340, 0.5963, 0.5686], device='cuda:0')\n",
      "train_recall: tensor([0.4971, 0.7287, 0.5947, 0.0042], device='cuda:0')\n",
      "val_loss: 0.902608162826962\n",
      "val_precision: tensor([0.5119, 0.6870, 0.6939, 0.7333], device='cuda:0')\n",
      "val_recall: tensor([0.6679, 0.7226, 0.5133, 0.0259], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 3 time:  0.48423958954999763\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.92224931035723\n",
      "train_precision: tensor([0.5409, 0.6486, 0.6157, 0.6211], device='cuda:0')\n",
      "train_recall: tensor([0.5180, 0.7416, 0.6128, 0.0229], device='cuda:0')\n",
      "val_loss: 0.853017019563251\n",
      "val_precision: tensor([0.5946, 0.6776, 0.6415, 0.7824], device='cuda:0')\n",
      "val_recall: tensor([0.5273, 0.7901, 0.6671, 0.0569], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 4 time:  0.48349343566666597\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.9004196130094074\n",
      "train_precision: tensor([0.5573, 0.6568, 0.6281, 0.6329], device='cuda:0')\n",
      "train_recall: tensor([0.5318, 0.7505, 0.6254, 0.0450], device='cuda:0')\n",
      "val_loss: 0.8513158645894793\n",
      "val_precision: tensor([0.6006, 0.7301, 0.5980, 0.6629], device='cuda:0')\n",
      "val_recall: tensor([0.5104, 0.7240, 0.7456, 0.0788], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 5 time:  0.4975997780666679\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.8883524869169508\n",
      "train_precision: tensor([0.5618, 0.6624, 0.6339, 0.6025], device='cuda:0')\n",
      "train_recall: tensor([0.5372, 0.7533, 0.6322, 0.0564], device='cuda:0')\n",
      "val_loss: 0.8140357891718547\n",
      "val_precision: tensor([0.5889, 0.7009, 0.6919, 0.6271], device='cuda:0')\n",
      "val_recall: tensor([0.6084, 0.7827, 0.6398, 0.1603], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 6 time:  0.49722849551666665\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.8765794495741527\n",
      "train_precision: tensor([0.5704, 0.6656, 0.6416, 0.5615], device='cuda:0')\n",
      "train_recall: tensor([0.5468, 0.7573, 0.6357, 0.0724], device='cuda:0')\n",
      "val_loss: 0.8072899626360999\n",
      "val_precision: tensor([0.6445, 0.6741, 0.6569, 0.6093], device='cuda:0')\n",
      "val_recall: tensor([0.5055, 0.8227, 0.6994, 0.1859], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 7 time:  0.49909560660000046\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.8693858206272125\n",
      "train_precision: tensor([0.5759, 0.6688, 0.6447, 0.5675], device='cuda:0')\n",
      "train_recall: tensor([0.5498, 0.7592, 0.6413, 0.0873], device='cuda:0')\n",
      "val_loss: 0.7975639568434821\n",
      "val_precision: tensor([0.6418, 0.6798, 0.6758, 0.6362], device='cuda:0')\n",
      "val_recall: tensor([0.5289, 0.8302, 0.6928, 0.1872], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 8 time:  0.49537318218333437\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.8661879315262749\n",
      "train_precision: tensor([0.5758, 0.6697, 0.6467, 0.5699], device='cuda:0')\n",
      "train_recall: tensor([0.5505, 0.7595, 0.6427, 0.0906], device='cuda:0')\n",
      "val_loss: 0.798320492108663\n",
      "val_precision: tensor([0.5962, 0.7091, 0.6950, 0.6584], device='cuda:0')\n",
      "val_recall: tensor([0.6208, 0.7794, 0.6510, 0.1519], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 9 time:  0.45568007488332873\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.8625736213865735\n",
      "train_precision: tensor([0.5797, 0.6683, 0.6488, 0.5524], device='cuda:0')\n",
      "train_recall: tensor([0.5533, 0.7582, 0.6438, 0.1026], device='cuda:0')\n",
      "val_loss: 0.787146532535553\n",
      "val_precision: tensor([0.6382, 0.6806, 0.6930, 0.5849], device='cuda:0')\n",
      "val_recall: tensor([0.5506, 0.8295, 0.6767, 0.2458], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 10 time:  0.4937328999666685\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.8592495381832123\n",
      "train_precision: tensor([0.5804, 0.6712, 0.6482, 0.5420], device='cuda:0')\n",
      "train_recall: tensor([0.5510, 0.7587, 0.6484, 0.1081], device='cuda:0')\n",
      "val_loss: 0.7957462850544188\n",
      "val_precision: tensor([0.6778, 0.6645, 0.6647, 0.6671], device='cuda:0')\n",
      "val_recall: tensor([0.4753, 0.8424, 0.7326, 0.1929], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 11 time:  0.458122578433328\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.8513881618068332\n",
      "train_precision: tensor([0.5836, 0.6757, 0.6521, 0.5711], device='cuda:0')\n",
      "train_recall: tensor([0.5583, 0.7604, 0.6495, 0.1258], device='cuda:0')\n",
      "val_loss: 0.7798860639333725\n",
      "val_precision: tensor([0.6203, 0.6893, 0.7269, 0.7463], device='cuda:0')\n",
      "val_recall: tensor([0.6167, 0.8222, 0.6469, 0.1714], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 12 time:  0.49653765256666704\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.8443259693327404\n",
      "train_precision: tensor([0.5890, 0.6767, 0.6566, 0.5494], device='cuda:0')\n",
      "train_recall: tensor([0.5646, 0.7608, 0.6515, 0.1372], device='cuda:0')\n",
      "val_loss: 0.7847980936368306\n",
      "val_precision: tensor([0.5804, 0.7277, 0.7238, 0.7179], device='cuda:0')\n",
      "val_recall: tensor([0.6800, 0.7670, 0.6137, 0.1902], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 13 time:  0.44209203161666816\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.8379078371184213\n",
      "train_precision: tensor([0.5893, 0.6820, 0.6579, 0.5581], device='cuda:0')\n",
      "train_recall: tensor([0.5636, 0.7627, 0.6574, 0.1462], device='cuda:0')\n",
      "val_loss: 0.7557409716977014\n",
      "val_precision: tensor([0.6631, 0.7073, 0.6913, 0.6116], device='cuda:0')\n",
      "val_recall: tensor([0.5622, 0.8219, 0.7228, 0.2731], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 14 time:  0.48192326588333195\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.8277689434233166\n",
      "train_precision: tensor([0.5963, 0.6877, 0.6638, 0.5490], device='cuda:0')\n",
      "train_recall: tensor([0.5731, 0.7645, 0.6627, 0.1583], device='cuda:0')\n",
      "val_loss: 0.7545008030202653\n",
      "val_precision: tensor([0.6732, 0.6890, 0.7039, 0.6944], device='cuda:0')\n",
      "val_recall: tensor([0.5556, 0.8455, 0.7123, 0.2471], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 15 time:  0.47822181255000185\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.8510482756864457\n",
      "train_precision: tensor([0.5861, 0.6740, 0.6558, 0.5256], device='cuda:0')\n",
      "train_recall: tensor([0.5601, 0.7593, 0.6515, 0.1289], device='cuda:0')\n",
      "val_loss: 0.7792591826783286\n",
      "val_precision: tensor([0.6251, 0.7036, 0.6973, 0.6764], device='cuda:0')\n",
      "val_recall: tensor([0.5901, 0.8134, 0.6788, 0.1795], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 16 time:  0.4393533561166654\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.8675054788589478\n",
      "train_precision: tensor([0.5780, 0.6657, 0.6450, 0.5402], device='cuda:0')\n",
      "train_recall: tensor([0.5513, 0.7542, 0.6416, 0.0999], device='cuda:0')\n",
      "val_loss: 0.7829254865646362\n",
      "val_precision: tensor([0.6650, 0.6799, 0.6698, 0.6596], device='cuda:0')\n",
      "val_recall: tensor([0.4859, 0.8391, 0.7359, 0.2414], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 17 time:  0.441168373000005\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.8435937469913846\n",
      "train_precision: tensor([0.5901, 0.6752, 0.6596, 0.5249], device='cuda:0')\n",
      "train_recall: tensor([0.5657, 0.7599, 0.6529, 0.1369], device='cuda:0')\n",
      "val_loss: 0.7573378493388494\n",
      "val_precision: tensor([0.6545, 0.7101, 0.6872, 0.6874], device='cuda:0')\n",
      "val_recall: tensor([0.5547, 0.8201, 0.7300, 0.2414], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 18 time:  0.44236670443332665\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.8160672590846108\n",
      "train_precision: tensor([0.6067, 0.6900, 0.6702, 0.5785], device='cuda:0')\n",
      "train_recall: tensor([0.5767, 0.7742, 0.6697, 0.1628], device='cuda:0')\n",
      "val_loss: 0.7412175708346896\n",
      "val_precision: tensor([0.6397, 0.7395, 0.6977, 0.6871], device='cuda:0')\n",
      "val_recall: tensor([0.6083, 0.7900, 0.7272, 0.2646], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 19 time:  0.4897894335666706\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n",
      "train_loss: 0.8097588147435869\n",
      "train_precision: tensor([0.6082, 0.6928, 0.6730, 0.5882], device='cuda:0')\n",
      "train_recall: tensor([0.5839, 0.7718, 0.6705, 0.1737], device='cuda:0')\n",
      "val_loss: 0.7374073161019219\n",
      "val_precision: tensor([0.6446, 0.7405, 0.6984, 0.6717], device='cuda:0')\n",
      "val_recall: tensor([0.6078, 0.7928, 0.7292, 0.2852], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 20 time:  0.4916964455333376\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 21\n",
      "train_loss: 0.8065015914894286\n",
      "train_precision: tensor([0.6096, 0.6937, 0.6737, 0.5765], device='cuda:0')\n",
      "train_recall: tensor([0.5848, 0.7724, 0.6715, 0.1762], device='cuda:0')\n",
      "val_loss: 0.7354547679424286\n",
      "val_precision: tensor([0.6474, 0.7430, 0.6957, 0.6782], device='cuda:0')\n",
      "val_recall: tensor([0.6059, 0.7914, 0.7354, 0.2845], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 21 time:  0.4896680014333318\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 22\n",
      "train_loss: 0.8045671355156672\n",
      "train_precision: tensor([0.6123, 0.6959, 0.6734, 0.5907], device='cuda:0')\n",
      "train_recall: tensor([0.5857, 0.7723, 0.6753, 0.1823], device='cuda:0')\n",
      "val_loss: 0.7333468914031982\n",
      "val_precision: tensor([0.6479, 0.7441, 0.6980, 0.6587], device='cuda:0')\n",
      "val_recall: tensor([0.6099, 0.7901, 0.7343, 0.2956], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 22 time:  0.4893136671833342\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 23\n",
      "train_loss: 0.8031107573282151\n",
      "train_precision: tensor([0.6128, 0.6976, 0.6751, 0.5808], device='cuda:0')\n",
      "train_recall: tensor([0.5864, 0.7722, 0.6775, 0.1882], device='cuda:0')\n",
      "val_loss: 0.7319796178076002\n",
      "val_precision: tensor([0.6536, 0.7394, 0.6973, 0.6708], device='cuda:0')\n",
      "val_recall: tensor([0.6023, 0.7975, 0.7368, 0.2909], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 23 time:  0.4866612288000018\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 24\n",
      "train_loss: 0.8020831408954802\n",
      "train_precision: tensor([0.6151, 0.6962, 0.6741, 0.5701], device='cuda:0')\n",
      "train_recall: tensor([0.5850, 0.7729, 0.6781, 0.1877], device='cuda:0')\n",
      "val_loss: 0.7302272352907393\n",
      "val_precision: tensor([0.6481, 0.7442, 0.7019, 0.6677], device='cuda:0')\n",
      "val_recall: tensor([0.6169, 0.7921, 0.7299, 0.2929], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 24 time:  0.4948971032499988\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 25\n",
      "train_loss: 0.8013375319185712\n",
      "train_precision: tensor([0.6151, 0.6978, 0.6760, 0.5696], device='cuda:0')\n",
      "train_recall: tensor([0.5876, 0.7734, 0.6778, 0.1906], device='cuda:0')\n",
      "val_loss: 0.72888652616077\n",
      "val_precision: tensor([0.6499, 0.7442, 0.7010, 0.6703], device='cuda:0')\n",
      "val_recall: tensor([0.6141, 0.7935, 0.7327, 0.2916], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 25 time:  0.4895426772833389\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 26\n",
      "train_loss: 0.7994345102991377\n",
      "train_precision: tensor([0.6146, 0.6971, 0.6755, 0.5830], device='cuda:0')\n",
      "train_recall: tensor([0.5860, 0.7717, 0.6795, 0.1967], device='cuda:0')\n",
      "val_loss: 0.7277553379535675\n",
      "val_precision: tensor([0.6519, 0.7423, 0.7032, 0.6624], device='cuda:0')\n",
      "val_recall: tensor([0.6147, 0.7973, 0.7297, 0.2993], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 26 time:  0.49552416955000356\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 27\n",
      "train_loss: 0.7987217843532562\n",
      "train_precision: tensor([0.6147, 0.6986, 0.6771, 0.5680], device='cuda:0')\n",
      "train_recall: tensor([0.5879, 0.7736, 0.6790, 0.1899], device='cuda:0')\n",
      "val_loss: 0.7275222619374593\n",
      "val_precision: tensor([0.6559, 0.7426, 0.6975, 0.6627], device='cuda:0')\n",
      "val_recall: tensor([0.6044, 0.7970, 0.7392, 0.3017], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 27 time:  0.4963294453666625\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 28\n",
      "train_loss: 0.7981414488383702\n",
      "train_precision: tensor([0.6157, 0.6980, 0.6752, 0.5853], device='cuda:0')\n",
      "train_recall: tensor([0.5851, 0.7734, 0.6801, 0.2020], device='cuda:0')\n",
      "val_loss: 0.7262275324927436\n",
      "val_precision: tensor([0.6518, 0.7469, 0.6999, 0.6669], device='cuda:0')\n",
      "val_recall: tensor([0.6142, 0.7923, 0.7360, 0.3020], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 28 time:  0.4957785815333333\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 29\n",
      "train_loss: 0.7962515172504243\n",
      "train_precision: tensor([0.6169, 0.6987, 0.6780, 0.5816], device='cuda:0')\n",
      "train_recall: tensor([0.5888, 0.7724, 0.6814, 0.2042], device='cuda:0')\n",
      "val_loss: 0.7255883508258396\n",
      "val_precision: tensor([0.6554, 0.7452, 0.6999, 0.6637], device='cuda:0')\n",
      "val_recall: tensor([0.6100, 0.7965, 0.7382, 0.3030], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 29 time:  0.4911004991500022\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 212\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m    211\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 212\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m, in \u001b[0;36mMyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m    101\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    102\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_recall(preds, labels)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:373\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:466\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/classification/stat_scores.py:333\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m    337\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    338\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:309\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    307\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet18\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'dinov2_42x42_1024batch{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5005, 0.3526, 0.5494]\n",
    "std = [0.1493, 0.1340, 0.1123]\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.4, p=1), RandomContrast(limit=0.4, p=1)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.005, patience=2, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss},step=self.step)\n",
    "        wandb.log({'Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()},step=self.step)\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'val_Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 1024\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00005\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_lc')\n",
    "model.linear_head = nn.Linear(1920, 4)\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch/60}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee79db-a551-4bae-bc82-c89b13be5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch \n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))\n",
    "def test_report(model, dataloader):\n",
    "    \"\"\"Prints confusion matrix for testing dataset\n",
    "    dataloader should be of batch_size=1.\"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            output = model(data)\n",
    "            label = label.numpy()\n",
    "            output = output.numpy()\n",
    "            y_pred.append(np.argmax(output))\n",
    "            y_test.append(np.argmax(label))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "testset =ImageDataset(data_path='test_data')\n",
    "dataloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "test_report(my_model.to('cpu'), dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd3ecf-3a50-4991-9274-c0639aaacaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
