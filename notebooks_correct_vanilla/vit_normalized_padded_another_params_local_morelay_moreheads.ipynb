{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca87734f-ec85-43a1-9bfd-012877f718c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240426_004356-h4avxqkw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/h4avxqkw' target=\"_blank\">vit_normalized_padded_another_params_local_morelay_moreheads2024-04-26 00:43:55.823042</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/h4avxqkw' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/h4avxqkw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 1.0352543643542698\n",
      "train_precision: tensor([0.4657, 0.5658, 0.5379, 0.5000], device='cuda:0')\n",
      "train_recall: tensor([3.8971e-01, 7.8029e-01, 4.7563e-01, 1.4430e-04], device='cuda:0')\n",
      "val_loss: 0.9115199900335735\n",
      "val_precision: tensor([0.5249, 0.6675, 0.6433, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.5702, 0.7749, 0.5485, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 0 time:  21.19242034800004\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.9353850909641811\n",
      "train_precision: tensor([0.5249, 0.6410, 0.6202, 0.0000], device='cuda:0')\n",
      "train_recall: tensor([0.5300, 0.7485, 0.5722, 0.0000], device='cuda:0')\n",
      "val_loss: 0.9113322218259176\n",
      "val_precision: tensor([0.5237, 0.7612, 0.5820, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.5075, 0.6314, 0.7574, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 1 time:  20.881262431000096\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.914523409377961\n",
      "train_precision: tensor([0.5357, 0.6532, 0.6303, 0.5000], device='cuda:0')\n",
      "train_recall: tensor([5.4658e-01, 7.5121e-01, 5.8584e-01, 1.4430e-04], device='cuda:0')\n",
      "val_loss: 0.8561152117119895\n",
      "val_precision: tensor([0.5462, 0.7217, 0.6617, 0.0000], device='cuda:0')\n",
      "val_recall: tensor([0.6031, 0.7320, 0.6495, 0.0000], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 2 time:  20.972974744000112\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.8960234431993394\n",
      "train_precision: tensor([0.5491, 0.6613, 0.6397, 0.3846], device='cuda:0')\n",
      "train_recall: tensor([5.5615e-01, 7.5713e-01, 6.0264e-01, 7.2150e-04], device='cuda:0')\n",
      "val_loss: 0.8431484997272491\n",
      "val_precision: tensor([0.5794, 0.6945, 0.6691, 0.3386], device='cuda:0')\n",
      "val_recall: tensor([0.5542, 0.7943, 0.6632, 0.0286], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 3 time:  20.805276823999975\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.8817570737430027\n",
      "train_precision: tensor([0.5585, 0.6684, 0.6453, 0.5407], device='cuda:0')\n",
      "train_recall: tensor([0.5612, 0.7632, 0.6135, 0.0134], device='cuda:0')\n",
      "val_loss: 0.8242186016506619\n",
      "val_precision: tensor([0.5653, 0.7436, 0.6637, 0.4915], device='cuda:0')\n",
      "val_recall: tensor([0.6269, 0.7363, 0.6630, 0.0098], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 4 time:  21.379656619000116\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.8730747882808958\n",
      "train_precision: tensor([0.5638, 0.6721, 0.6496, 0.5164], device='cuda:0')\n",
      "train_recall: tensor([0.5634, 0.7667, 0.6190, 0.0364], device='cuda:0')\n",
      "val_loss: 0.8427429957522287\n",
      "val_precision: tensor([0.5436, 0.6932, 0.7744, 0.4722], device='cuda:0')\n",
      "val_recall: tensor([0.6988, 0.7985, 0.4821, 0.1889], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 5 time:  21.404045533000044\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.8629724838903972\n",
      "train_precision: tensor([0.5695, 0.6776, 0.6524, 0.5325], device='cuda:0')\n",
      "train_recall: tensor([0.5693, 0.7670, 0.6240, 0.0639], device='cuda:0')\n",
      "val_loss: 0.9198943197727203\n",
      "val_precision: tensor([0.5371, 0.7501, 0.6759, 0.1716], device='cuda:0')\n",
      "val_recall: tensor([0.5634, 0.6651, 0.6239, 0.4138], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 6 time:  21.057890905000022\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.8565696733338493\n",
      "train_precision: tensor([0.5731, 0.6809, 0.6548, 0.5241], device='cuda:0')\n",
      "train_recall: tensor([0.5691, 0.7693, 0.6306, 0.0739], device='cuda:0')\n",
      "val_loss: 0.8016096346908146\n",
      "val_precision: tensor([0.5702, 0.7687, 0.6751, 0.6731], device='cuda:0')\n",
      "val_recall: tensor([0.6487, 0.7054, 0.6889, 0.1636], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 7 time:  20.903332429000102\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.8488825809387933\n",
      "train_precision: tensor([0.5743, 0.6835, 0.6570, 0.5502], device='cuda:0')\n",
      "train_recall: tensor([0.5731, 0.7694, 0.6291, 0.1036], device='cuda:0')\n",
      "val_loss: 0.7889614529079861\n",
      "val_precision: tensor([0.6062, 0.7466, 0.6800, 0.4335], device='cuda:0')\n",
      "val_recall: tensor([0.6098, 0.7656, 0.6766, 0.3195], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 8 time:  21.070016841000097\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.8414350776445298\n",
      "train_precision: tensor([0.5823, 0.6878, 0.6617, 0.5461], device='cuda:0')\n",
      "train_recall: tensor([0.5774, 0.7720, 0.6377, 0.1214], device='cuda:0')\n",
      "val_loss: 0.776471401585473\n",
      "val_precision: tensor([0.6005, 0.7100, 0.7446, 0.5008], device='cuda:0')\n",
      "val_recall: tensor([0.6481, 0.8095, 0.6065, 0.3310], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 9 time:  21.363896548999946\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.8369117116644269\n",
      "train_precision: tensor([0.5851, 0.6878, 0.6618, 0.5373], device='cuda:0')\n",
      "train_recall: tensor([0.5775, 0.7723, 0.6401, 0.1238], device='cuda:0')\n",
      "val_loss: 0.7706703229082955\n",
      "val_precision: tensor([0.6439, 0.7401, 0.6517, 0.6505], device='cuda:0')\n",
      "val_recall: tensor([0.5304, 0.7846, 0.7672, 0.2525], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 10 time:  21.064309195000078\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.8328902018921716\n",
      "train_precision: tensor([0.5868, 0.6881, 0.6642, 0.5529], device='cuda:0')\n",
      "train_recall: tensor([0.5775, 0.7712, 0.6435, 0.1447], device='cuda:0')\n",
      "val_loss: 0.7676235694024298\n",
      "val_precision: tensor([0.6471, 0.7200, 0.6740, 0.5016], device='cuda:0')\n",
      "val_recall: tensor([0.5330, 0.8180, 0.7190, 0.3690], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 11 time:  20.944359132000045\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.8276512258109592\n",
      "train_precision: tensor([0.5904, 0.6926, 0.6648, 0.5554], device='cuda:0')\n",
      "train_recall: tensor([0.5821, 0.7737, 0.6454, 0.1455], device='cuda:0')\n",
      "val_loss: 0.7696321984132131\n",
      "val_precision: tensor([0.6099, 0.7665, 0.6878, 0.4561], device='cuda:0')\n",
      "val_recall: tensor([0.6247, 0.7527, 0.6952, 0.3791], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 12 time:  21.00187270599986\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.8251830846071243\n",
      "train_precision: tensor([0.5914, 0.6937, 0.6640, 0.5467], device='cuda:0')\n",
      "train_recall: tensor([0.5809, 0.7749, 0.6462, 0.1504], device='cuda:0')\n",
      "val_loss: 0.760160689883762\n",
      "val_precision: tensor([0.6150, 0.7671, 0.6935, 0.4120], device='cuda:0')\n",
      "val_recall: tensor([0.6152, 0.7523, 0.7063, 0.4145], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 13 time:  21.088038607000044\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.8204701189483915\n",
      "train_precision: tensor([0.5977, 0.6965, 0.6670, 0.5607], device='cuda:0')\n",
      "train_recall: tensor([0.5843, 0.7776, 0.6516, 0.1626], device='cuda:0')\n",
      "val_loss: 0.7658374180396398\n",
      "val_precision: tensor([0.6493, 0.7622, 0.6416, 0.4806], device='cuda:0')\n",
      "val_recall: tensor([0.5201, 0.7625, 0.7807, 0.3929], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 14 time:  21.386725049999995\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.8188205331563949\n",
      "train_precision: tensor([0.5962, 0.6970, 0.6672, 0.5579], device='cuda:0')\n",
      "train_recall: tensor([0.5845, 0.7767, 0.6518, 0.1571], device='cuda:0')\n",
      "val_loss: 0.7461807000968191\n",
      "val_precision: tensor([0.6431, 0.7334, 0.6964, 0.5411], device='cuda:0')\n",
      "val_recall: tensor([0.5837, 0.8042, 0.7160, 0.3660], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 15 time:  20.967518072000075\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.8136969364824749\n",
      "train_precision: tensor([0.5985, 0.6973, 0.6684, 0.5753], device='cuda:0')\n",
      "train_recall: tensor([0.5850, 0.7767, 0.6544, 0.1714], device='cuda:0')\n",
      "val_loss: 0.7641036768754323\n",
      "val_precision: tensor([0.6459, 0.7526, 0.6814, 0.3717], device='cuda:0')\n",
      "val_recall: tensor([0.5645, 0.7824, 0.7287, 0.4347], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 16 time:  20.911586787000033\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.8106548181601934\n",
      "train_precision: tensor([0.6031, 0.7006, 0.6686, 0.5885], device='cuda:0')\n",
      "train_recall: tensor([0.5855, 0.7783, 0.6602, 0.1824], device='cuda:0')\n",
      "val_loss: 0.7876266913281547\n",
      "val_precision: tensor([0.5933, 0.7833, 0.6918, 0.3738], device='cuda:0')\n",
      "val_recall: tensor([0.6370, 0.7248, 0.6819, 0.4303], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 17 time:  20.930723685999965\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.8102243317025049\n",
      "train_precision: tensor([0.6030, 0.7019, 0.6699, 0.5731], device='cuda:0')\n",
      "train_recall: tensor([0.5887, 0.7790, 0.6588, 0.1742], device='cuda:0')\n",
      "val_loss: 0.7392620189322365\n",
      "val_precision: tensor([0.6386, 0.7385, 0.7245, 0.4781], device='cuda:0')\n",
      "val_recall: tensor([0.6198, 0.8095, 0.6807, 0.4478], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 18 time:  20.85479010400013\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.8062366516817183\n",
      "train_precision: tensor([0.6051, 0.7022, 0.6702, 0.5735], device='cuda:0')\n",
      "train_recall: tensor([0.5876, 0.7806, 0.6609, 0.1784], device='cuda:0')\n",
      "val_loss: 0.7402030623621411\n",
      "val_precision: tensor([0.6433, 0.7248, 0.7369, 0.4403], device='cuda:0')\n",
      "val_recall: tensor([0.6182, 0.8266, 0.6599, 0.4542], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 19 time:  20.954335491999927\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n",
      "train_loss: 0.8045548708665938\n",
      "train_precision: tensor([0.6053, 0.7019, 0.6715, 0.5860], device='cuda:0')\n",
      "train_recall: tensor([0.5877, 0.7804, 0.6604, 0.1981], device='cuda:0')\n",
      "val_loss: 0.7466555135117636\n",
      "val_precision: tensor([0.6367, 0.7740, 0.6714, 0.5416], device='cuda:0')\n",
      "val_recall: tensor([0.5877, 0.7577, 0.7571, 0.3811], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 20 time:  20.952916307000123\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 21\n",
      "train_loss: 0.8026006359429586\n",
      "train_precision: tensor([0.6059, 0.7031, 0.6740, 0.5834], device='cuda:0')\n",
      "train_recall: tensor([0.5899, 0.7807, 0.6627, 0.1913], device='cuda:0')\n",
      "val_loss: 0.7353646907541487\n",
      "val_precision: tensor([0.6468, 0.7227, 0.7219, 0.5495], device='cuda:0')\n",
      "val_recall: tensor([0.6043, 0.8246, 0.6889, 0.3865], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 21 time:  20.98003269800006\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 22\n",
      "train_loss: 0.8011055381525131\n",
      "train_precision: tensor([0.6053, 0.7036, 0.6734, 0.6045], device='cuda:0')\n",
      "train_recall: tensor([0.5897, 0.7810, 0.6624, 0.1932], device='cuda:0')\n",
      "val_loss: 0.7389246731996536\n",
      "val_precision: tensor([0.6305, 0.7777, 0.6767, 0.6293], device='cuda:0')\n",
      "val_recall: tensor([0.6286, 0.7470, 0.7399, 0.3098], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 22 time:  20.912735738000038\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 23\n",
      "train_loss: 0.7978511950799397\n",
      "train_precision: tensor([0.6074, 0.7047, 0.6748, 0.5841], device='cuda:0')\n",
      "train_recall: tensor([0.5912, 0.7808, 0.6656, 0.1895], device='cuda:0')\n",
      "val_loss: 0.742811855342653\n",
      "val_precision: tensor([0.6837, 0.7672, 0.6404, 0.5932], device='cuda:0')\n",
      "val_recall: tensor([0.5202, 0.7745, 0.8113, 0.3727], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 23 time:  20.986823951999895\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 24\n",
      "train_loss: 0.796861480105491\n",
      "train_precision: tensor([0.6091, 0.7039, 0.6775, 0.5948], device='cuda:0')\n",
      "train_recall: tensor([0.5921, 0.7814, 0.6670, 0.1978], device='cuda:0')\n",
      "val_loss: 0.7359182364410825\n",
      "val_precision: tensor([0.6405, 0.7812, 0.6759, 0.6239], device='cuda:0')\n",
      "val_recall: tensor([0.6069, 0.7578, 0.7611, 0.3519], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 24 time:  20.962351815000147\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 25\n",
      "train_loss: 0.7773206886791048\n",
      "train_precision: tensor([0.6197, 0.7146, 0.6828, 0.6076], device='cuda:0')\n",
      "train_recall: tensor([0.6015, 0.7856, 0.6770, 0.2335], device='cuda:0')\n",
      "val_loss: 0.712052904566129\n",
      "val_precision: tensor([0.6506, 0.7687, 0.7090, 0.5572], device='cuda:0')\n",
      "val_recall: tensor([0.6244, 0.7916, 0.7311, 0.4411], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 25 time:  20.913503019000018\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 26\n",
      "train_loss: 0.7755558341741562\n",
      "train_precision: tensor([0.6197, 0.7146, 0.6843, 0.6187], device='cuda:0')\n",
      "train_recall: tensor([0.5992, 0.7874, 0.6799, 0.2325], device='cuda:0')\n",
      "val_loss: 0.7101907547977235\n",
      "val_precision: tensor([0.6714, 0.7675, 0.6884, 0.5793], device='cuda:0')\n",
      "val_recall: tensor([0.5853, 0.7971, 0.7680, 0.4293], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 26 time:  21.013531646000047\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 27\n",
      "train_loss: 0.775527595622199\n",
      "train_precision: tensor([0.6207, 0.7162, 0.6837, 0.6079], device='cuda:0')\n",
      "train_recall: tensor([0.5991, 0.7873, 0.6820, 0.2317], device='cuda:0')\n",
      "val_loss: 0.7115621974070867\n",
      "val_precision: tensor([0.6646, 0.7712, 0.6955, 0.5489], device='cuda:0')\n",
      "val_recall: tensor([0.6002, 0.7935, 0.7575, 0.4327], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 27 time:  21.110972700000048\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 28\n",
      "train_loss: 0.7738230813117254\n",
      "train_precision: tensor([0.6238, 0.7157, 0.6853, 0.6033], device='cuda:0')\n",
      "train_recall: tensor([0.6019, 0.7876, 0.6825, 0.2343], device='cuda:0')\n",
      "val_loss: 0.7075155797931884\n",
      "val_precision: tensor([0.6563, 0.7782, 0.6960, 0.5959], device='cuda:0')\n",
      "val_recall: tensor([0.6178, 0.7799, 0.7560, 0.4185], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 28 time:  21.318607170000178\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 29\n",
      "train_loss: 0.7726300431149347\n",
      "train_precision: tensor([0.6222, 0.7169, 0.6845, 0.6166], device='cuda:0')\n",
      "train_recall: tensor([0.5999, 0.7894, 0.6822, 0.2343], device='cuda:0')\n",
      "val_loss: 0.7031586696704228\n",
      "val_precision: tensor([0.6569, 0.7674, 0.7116, 0.5857], device='cuda:0')\n",
      "val_recall: tensor([0.6240, 0.7969, 0.7389, 0.4290], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 29 time:  21.074761398999954\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 30\n",
      "train_loss: 0.771178059067045\n",
      "train_precision: tensor([0.6233, 0.7162, 0.6872, 0.6157], device='cuda:0')\n",
      "train_recall: tensor([0.6040, 0.7878, 0.6816, 0.2407], device='cuda:0')\n",
      "val_loss: 0.7047181447347005\n",
      "val_precision: tensor([0.6562, 0.7755, 0.7054, 0.5996], device='cuda:0')\n",
      "val_recall: tensor([0.6277, 0.7869, 0.7476, 0.4125], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 30 time:  21.026008347000015\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 31\n",
      "train_loss: 0.7700405020089377\n",
      "train_precision: tensor([0.6223, 0.7176, 0.6871, 0.6267], device='cuda:0')\n",
      "train_recall: tensor([0.6016, 0.7890, 0.6826, 0.2531], device='cuda:0')\n",
      "val_loss: 0.7017264516817199\n",
      "val_precision: tensor([0.6627, 0.7676, 0.7086, 0.6055], device='cuda:0')\n",
      "val_recall: tensor([0.6190, 0.8006, 0.7473, 0.4145], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 31 time:  21.27033659199992\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 32\n",
      "train_loss: 0.7708608977851413\n",
      "train_precision: tensor([0.6237, 0.7173, 0.6868, 0.6136], device='cuda:0')\n",
      "train_recall: tensor([0.6031, 0.7891, 0.6823, 0.2420], device='cuda:0')\n",
      "val_loss: 0.7033745884895325\n",
      "val_precision: tensor([0.6639, 0.7650, 0.7100, 0.5667], device='cuda:0')\n",
      "val_recall: tensor([0.6124, 0.8034, 0.7460, 0.4350], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 32 time:  20.98869357600006\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 33\n",
      "train_loss: 0.7707021540119534\n",
      "train_precision: tensor([0.6234, 0.7150, 0.6869, 0.6154], device='cuda:0')\n",
      "train_recall: tensor([0.6045, 0.7884, 0.6788, 0.2436], device='cuda:0')\n",
      "val_loss: 0.7034582616554366\n",
      "val_precision: tensor([0.6606, 0.7727, 0.7070, 0.5630], device='cuda:0')\n",
      "val_recall: tensor([0.6191, 0.7937, 0.7468, 0.4468], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 33 time:  21.038655094999967\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 34\n",
      "train_loss: 0.7691930579287666\n",
      "train_precision: tensor([0.6245, 0.7176, 0.6868, 0.5927], device='cuda:0')\n",
      "train_recall: tensor([0.6030, 0.7897, 0.6825, 0.2380], device='cuda:0')\n",
      "val_loss: 0.7008335908253988\n",
      "val_precision: tensor([0.6576, 0.7649, 0.7168, 0.5714], device='cuda:0')\n",
      "val_recall: tensor([0.6262, 0.8011, 0.7327, 0.4475], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 34 time:  20.981872518999808\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 35\n",
      "train_loss: 0.7695488692749114\n",
      "train_precision: tensor([0.6240, 0.7178, 0.6868, 0.6155], device='cuda:0')\n",
      "train_recall: tensor([0.6030, 0.7899, 0.6827, 0.2411], device='cuda:0')\n",
      "val_loss: 0.7034978714254168\n",
      "val_precision: tensor([0.6548, 0.7803, 0.7018, 0.5866], device='cuda:0')\n",
      "val_recall: tensor([0.6231, 0.7826, 0.7524, 0.4300], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 35 time:  21.22766195600002\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 36\n",
      "train_loss: 0.7681841738167263\n",
      "train_precision: tensor([0.6238, 0.7166, 0.6873, 0.6152], device='cuda:0')\n",
      "train_recall: tensor([0.6013, 0.7905, 0.6820, 0.2501], device='cuda:0')\n",
      "val_loss: 0.7013170273767577\n",
      "val_precision: tensor([0.6590, 0.7657, 0.7144, 0.5867], device='cuda:0')\n",
      "val_recall: tensor([0.6247, 0.8023, 0.7373, 0.4239], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 36 time:  21.118043919\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 37\n",
      "train_loss: 0.7665856236503238\n",
      "train_precision: tensor([0.6248, 0.7184, 0.6878, 0.6214], device='cuda:0')\n",
      "train_recall: tensor([0.6058, 0.7895, 0.6813, 0.2544], device='cuda:0')\n",
      "val_loss: 0.6999249504672156\n",
      "val_precision: tensor([0.6742, 0.7671, 0.6992, 0.6049], device='cuda:0')\n",
      "val_recall: tensor([0.5992, 0.8048, 0.7632, 0.4263], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 37 time:  21.00893660199995\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 38\n",
      "train_loss: 0.7689403081224078\n",
      "train_precision: tensor([0.6244, 0.7179, 0.6863, 0.6123], device='cuda:0')\n",
      "train_recall: tensor([0.6037, 0.7899, 0.6804, 0.2537], device='cuda:0')\n",
      "val_loss: 0.7038239763842689\n",
      "val_precision: tensor([0.6545, 0.7908, 0.6998, 0.5588], device='cuda:0')\n",
      "val_recall: tensor([0.6291, 0.7719, 0.7563, 0.4589], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 38 time:  21.213418706000084\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 39\n",
      "train_loss: 0.7677100608746211\n",
      "train_precision: tensor([0.6246, 0.7197, 0.6873, 0.6267], device='cuda:0')\n",
      "train_recall: tensor([0.6037, 0.7912, 0.6829, 0.2534], device='cuda:0')\n",
      "val_loss: 0.6959381606843736\n",
      "val_precision: tensor([0.6580, 0.7692, 0.7221, 0.5941], device='cuda:0')\n",
      "val_recall: tensor([0.6372, 0.8027, 0.7329, 0.4337], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 39 time:  21.07252745100004\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 40\n",
      "train_loss: 0.7662370356775465\n",
      "train_precision: tensor([0.6244, 0.7201, 0.6880, 0.6169], device='cuda:0')\n",
      "train_recall: tensor([0.6059, 0.7901, 0.6827, 0.2482], device='cuda:0')\n",
      "val_loss: 0.6977637691630257\n",
      "val_precision: tensor([0.6500, 0.7780, 0.7192, 0.5895], device='cuda:0')\n",
      "val_recall: tensor([0.6461, 0.7912, 0.7300, 0.4347], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 40 time:  21.078558862000136\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 41\n",
      "train_loss: 0.7668358797118777\n",
      "train_precision: tensor([0.6240, 0.7178, 0.6870, 0.6212], device='cuda:0')\n",
      "train_recall: tensor([0.6032, 0.7911, 0.6817, 0.2418], device='cuda:0')\n",
      "val_loss: 0.6971019178628921\n",
      "val_precision: tensor([0.6609, 0.7737, 0.7135, 0.5731], device='cuda:0')\n",
      "val_recall: tensor([0.6311, 0.7964, 0.7405, 0.4475], device='cuda:0')\n",
      "Learning rate: 0.0001\n",
      "epoch 41 time:  21.11106588099983\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 42\n",
      "train_loss: 0.7649751206239065\n",
      "train_precision: tensor([0.6261, 0.7178, 0.6883, 0.6382], device='cuda:0')\n",
      "train_recall: tensor([0.6042, 0.7908, 0.6836, 0.2569], device='cuda:0')\n",
      "val_loss: 0.6977277947796716\n",
      "val_precision: tensor([0.6647, 0.7745, 0.7091, 0.5825], device='cuda:0')\n",
      "val_recall: tensor([0.6240, 0.7964, 0.7494, 0.4434], device='cuda:0')\n",
      "Learning rate: 1e-05\n",
      "epoch 42 time:  20.99406437300013\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 43\n",
      "train_loss: 0.7649581701982588\n",
      "train_precision: tensor([0.6256, 0.7180, 0.6888, 0.6133], device='cuda:0')\n",
      "train_recall: tensor([0.6048, 0.7893, 0.6841, 0.2508], device='cuda:0')\n",
      "val_loss: 0.6973098297913869\n",
      "val_precision: tensor([0.6650, 0.7718, 0.7090, 0.5698], device='cuda:0')\n",
      "val_recall: tensor([0.6200, 0.7996, 0.7464, 0.4505], device='cuda:0')\n",
      "Learning rate: 1e-05\n",
      "epoch 43 time:  21.021518923999793\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 44\n",
      "train_loss: 0.7634927348012016\n",
      "train_precision: tensor([0.6251, 0.7211, 0.6897, 0.6227], device='cuda:0')\n",
      "train_recall: tensor([0.6068, 0.7904, 0.6843, 0.2553], device='cuda:0')\n",
      "val_loss: 0.6957706307371457\n",
      "val_precision: tensor([0.6684, 0.7697, 0.7075, 0.5896], device='cuda:0')\n",
      "val_recall: tensor([0.6137, 0.8030, 0.7529, 0.4387], device='cuda:0')\n",
      "Learning rate: 1e-05\n",
      "epoch 44 time:  20.94176345599999\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 45\n",
      "train_loss: 0.763698643162137\n",
      "train_precision: tensor([0.6262, 0.7204, 0.6876, 0.6237], device='cuda:0')\n",
      "train_recall: tensor([0.6060, 0.7897, 0.6847, 0.2509], device='cuda:0')\n",
      "val_loss: 0.6974878713488579\n",
      "val_precision: tensor([0.6638, 0.7773, 0.7062, 0.5795], device='cuda:0')\n",
      "val_recall: tensor([0.6223, 0.7917, 0.7533, 0.4481], device='cuda:0')\n",
      "Learning rate: 1e-05\n",
      "epoch 45 time:  21.032615605000046\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 220\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m    219\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 220\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m, in \u001b[0;36mMyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m    101\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    102\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_recall(preds, labels)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:373\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:466\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/classification/stat_scores.py:333\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m    337\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    338\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:309\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    307\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet50\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "from vit.vit import VisionTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'vit_normalized_padded_another_params_local_morelay_moreheads{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.4, p=1), RandomContrast(limit=0.4, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.6,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=2, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss},step=self.step)\n",
    "        wandb.log({'Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()},step=self.step)\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'val_Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 512\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "\n",
    "\n",
    "model = VisionTransformer(image_size=32, \n",
    "                          in_channels=3, \n",
    "                          num_classes=4, \n",
    "                          hidden_dims=[64, 8], \n",
    "                          dropout_rate=0.3,\n",
    "                          embedding_dim=32,\n",
    "                          patch_size=8,\n",
    "                          num_layers=8,\n",
    "                          num_heads=8)\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
