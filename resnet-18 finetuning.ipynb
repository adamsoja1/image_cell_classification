{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c499ca-94f7-4baf-863c-0dbcaa855aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1800: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1826: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1952: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n",
      "[I 2024-07-15 23:35:55,076] Using an existing study with name 'resnet18_finetuned' instead of creating a new one.\n",
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-15 23:57:34,006] Trial 13 finished with value: 0.5500288701023079 and parameters: {'batch_size': 266, 'learning_rate': 0.009811117505630837, 'dropout_rate': 0.2012399422033288, 'kernel_size': 2}. Best is trial 13 with value: 0.5500288701023079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 51 with best validation loss 0.5500288701023079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 00:16:33,272] Trial 14 finished with value: 0.5529790749916663 and parameters: {'batch_size': 417, 'learning_rate': 0.008348004952229334, 'dropout_rate': 0.20199841417350675, 'kernel_size': 3}. Best is trial 13 with value: 0.5500288701023079.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 52 with best validation loss 0.5529790749916663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 00:45:20,801] Trial 15 finished with value: 0.5445458108964174 and parameters: {'batch_size': 267, 'learning_rate': 0.00974315673118695, 'dropout_rate': 0.3615227099039917, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 79 with best validation loss 0.5445458108964174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 01:04:42,973] Trial 16 finished with value: 0.5539254196247652 and parameters: {'batch_size': 460, 'learning_rate': 0.0018895630674112159, 'dropout_rate': 0.35960390124716946, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 53 with best validation loss 0.5539254196247652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 01:34:28,912] Trial 17 finished with value: 0.6348846210072739 and parameters: {'batch_size': 465, 'learning_rate': 4.3227054743761036e-05, 'dropout_rate': 0.37887721778766315, 'kernel_size': 4}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 70 with best validation loss 0.6348846210072739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 01:53:29,949] Trial 18 finished with value: 0.5607421553871604 and parameters: {'batch_size': 259, 'learning_rate': 0.0028048313927582853, 'dropout_rate': 0.49184845998102256, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 52 with best validation loss 0.5607421553871604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 02:12:58,279] Trial 19 finished with value: 0.5645083425458798 and parameters: {'batch_size': 382, 'learning_rate': 0.0008476294043103777, 'dropout_rate': 0.3200459772540033, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 46 with best validation loss 0.5645083425458798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 02:37:00,330] Trial 20 finished with value: 0.558690805048556 and parameters: {'batch_size': 500, 'learning_rate': 0.009463931135647466, 'dropout_rate': 0.32639524418551175, 'kernel_size': 4}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 56 with best validation loss 0.558690805048556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 02:56:04,199] Trial 21 finished with value: 0.5779469447602206 and parameters: {'batch_size': 340, 'learning_rate': 0.00012309640608456788, 'dropout_rate': 0.3804394515511538, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 52 with best validation loss 0.5779469447602206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 03:19:27,971] Trial 22 finished with value: 0.5602188929915428 and parameters: {'batch_size': 893, 'learning_rate': 0.0011171297538244912, 'dropout_rate': 0.2950109893233724, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 53 with best validation loss 0.5602188929915428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 03:39:49,269] Trial 23 finished with value: 0.5634858152744445 and parameters: {'batch_size': 262, 'learning_rate': 0.004242897060908084, 'dropout_rate': 0.2247565037807292, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 48 with best validation loss 0.5634858152744445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 04:02:44,290] Trial 24 finished with value: 0.5607979117047708 and parameters: {'batch_size': 305, 'learning_rate': 0.00624755519849164, 'dropout_rate': 0.23741741661015525, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 54 with best validation loss 0.5607979117047708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 04:21:02,413] Trial 25 finished with value: 0.5515563494189646 and parameters: {'batch_size': 383, 'learning_rate': 0.004833412779337448, 'dropout_rate': 0.2025226720018614, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 50 with best validation loss 0.5515563494189646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 04:43:15,611] Trial 26 finished with value: 0.5603165113407632 and parameters: {'batch_size': 267, 'learning_rate': 0.0011504994128595407, 'dropout_rate': 0.3573084350920526, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 52 with best validation loss 0.5603165113407632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 05:02:17,648] Trial 27 finished with value: 0.5467836369016317 and parameters: {'batch_size': 516, 'learning_rate': 0.008516730711535773, 'dropout_rate': 0.2982966989190811, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 52 with best validation loss 0.5467836369016317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 05:25:11,920] Trial 28 finished with value: 0.5651798654686321 and parameters: {'batch_size': 524, 'learning_rate': 0.002138970692671221, 'dropout_rate': 0.33863945511114507, 'kernel_size': 4}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 53 with best validation loss 0.5651798654686321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 05:46:00,939] Trial 29 finished with value: 0.5725475565383309 and parameters: {'batch_size': 695, 'learning_rate': 0.0005818818515396156, 'dropout_rate': 0.30026113611304495, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 57 with best validation loss 0.5725475565383309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 06:07:45,074] Trial 30 finished with value: 0.5726038174573765 and parameters: {'batch_size': 430, 'learning_rate': 0.004578723163006991, 'dropout_rate': 0.3905970668382823, 'kernel_size': 4}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 51 with best validation loss 0.5726038174573765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 06:31:08,070] Trial 31 finished with value: 0.6556063513184308 and parameters: {'batch_size': 345, 'learning_rate': 2.1263753629001702e-05, 'dropout_rate': 0.26499273240948007, 'kernel_size': 5}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 64 with best validation loss 0.6556063513184308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 06:50:53,020] Trial 32 finished with value: 0.603667069583723 and parameters: {'batch_size': 631, 'learning_rate': 7.823768680180858e-05, 'dropout_rate': 0.4644276073374458, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 54 with best validation loss 0.603667069583723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 07:14:26,338] Trial 33 finished with value: 0.5584581292247119 and parameters: {'batch_size': 316, 'learning_rate': 0.00822091408626554, 'dropout_rate': 0.23543060036263688, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 56 with best validation loss 0.5584581292247119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 07:36:45,230] Trial 34 finished with value: 0.5819003427081171 and parameters: {'batch_size': 406, 'learning_rate': 0.003659523813656178, 'dropout_rate': 0.24622157554777802, 'kernel_size': 2}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 52 with best validation loss 0.5819003427081171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-07-16 07:55:28,935] Trial 35 finished with value: 0.5475827245095286 and parameters: {'batch_size': 295, 'learning_rate': 0.009828878452501884, 'dropout_rate': 0.261694447223335, 'kernel_size': 3}. Best is trial 15 with value: 0.5445458108964174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 51 with best validation loss 0.5475827245095286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357849/3486683960.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
      "/tmp/ipykernel_357849/3486683960.py:67: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics import Precision, Recall\n",
    "from dataset import ImageDataset\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    batch_size = trial.suggest_int('batch_size', 256, 1024)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 2, 7)\n",
    "\n",
    "    # Data loaders\n",
    "    trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "    \n",
    "    testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Model setup\n",
    "    model = resnet18()\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(kernel_size, kernel_size), stride=(1, 1), padding=(kernel_size // 2, kernel_size // 2), bias=False)\n",
    "    num_classes = 4\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    # Custom model class\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self, model, learning_rate):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.model = model\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "            self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=7, min_lr=5e-6, verbose=True)\n",
    "            self.step = 0\n",
    "            self.metric_precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "            self.metric_recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "            self.train_loss = []\n",
    "            self.valid_loss = []\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "        def train_one_epoch(self, trainloader):\n",
    "            self.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.train_loss.append(loss.item())\n",
    "            avg_loss = np.mean(self.train_loss)\n",
    "            self.train_loss.clear()\n",
    "            return avg_loss\n",
    "\n",
    "        def evaluate(self, testloader):\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    self.valid_loss.append(loss.item())\n",
    "            avg_loss = np.mean(self.valid_loss)\n",
    "            self.valid_loss.clear()\n",
    "            self.scheduler.step(avg_loss)\n",
    "            return avg_loss\n",
    "\n",
    "    my_model = MyModel(model=model, learning_rate=learning_rate)\n",
    "    my_model = my_model.to('cuda')\n",
    "    early_stop_patience = 15\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        my_model.train_one_epoch(trainloader)\n",
    "        val_loss = my_model.evaluate(testloader)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "            break\n",
    "    return best_val_loss\n",
    "\n",
    "# Start the optimization\n",
    "study = optuna.create_study(direction='minimize',\n",
    "                            storage=\"sqlite:///db.sqlite3\",  \n",
    "                            study_name=\"resnet18_finetuned\",\n",
    "                           load_if_exists=True)\n",
    "start = time.perf_counter()\n",
    "study.optimize(objective, n_trials=50)\n",
    "stop = time.perf_counter()\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843de372-d545-463a-bba1-542329ace38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
