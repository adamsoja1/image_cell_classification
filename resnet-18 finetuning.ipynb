{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60193684-f6bb-4fff-98ec-74b8e6af4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils_cells import get_images_list, transform_image, transform_target, resize_with_padding\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics import Precision, Recall\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, target_transform=None, reduce=False):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dataset = shuffle(self.load_dataset(data_path))\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        files = os.listdir(path)\n",
    "        dataset_final = pd.DataFrame()\n",
    "        dataset_final['filename'] = []\n",
    "        dataset_final['class'] = []\n",
    "        for filename in files:\n",
    "            dataset = pd.DataFrame()\n",
    "            if filename.endswith('.txt'):\n",
    "                files = get_images_list(f'{path}/{filename}')\n",
    "                dataset['filename'] = files\n",
    "                dataset['class'] = filename.split('_')[1][:-3]\n",
    "                dataset_final = pd.concat([dataset_final, dataset], ignore_index=True)\n",
    "        return dataset_final                \n",
    "                          \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(f'{self.dataset[\"filename\"].loc[idx]}')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "        #image = resize_with_padding(image, (32, 32))\n",
    "        image = image.astype(np.float32)\n",
    "        image = image/255.0\n",
    "        image = self.transform(image = image)['image'] if self.transform is not None else image\n",
    "\n",
    "        target = self.dataset[\"class\"].loc[idx]\n",
    "\n",
    "        if target == 'normal.':\n",
    "            target_ = [1, 0, 0, 0]\n",
    "        elif target == 'inflamatory.':\n",
    "            target_ = [0, 1, 0, 0]\n",
    "        elif target == 'tumor.':\n",
    "            target_ = [0, 0, 1, 0]\n",
    "        elif target == 'other.':\n",
    "            target_ = [0, 0, 0, 1]\n",
    "        else:\n",
    "            print(target)\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "       \n",
    "     \n",
    "\n",
    "        \"\"\"To see transorms use:\n",
    "            image, target = trainset[15]\n",
    "            image = image.numpy()\n",
    "            image=np.swapaxes(image,0,1)\n",
    "            image=np.swapaxes(image,1,2)\n",
    "            plt.imshow(image)\"\"\"\n",
    "\n",
    "        return image.float(), torch.Tensor(np.array(target_, dtype=np.float32))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4fa3b8-e315-4e1d-8591-97f7dfff26a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:2587: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2233)\n",
    "\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c499ca-94f7-4baf-863c-0dbcaa855aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-20 23:11:25,849] A new study created in RDB with name: resnet18final_tune3\n",
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-20 23:24:17,496] Trial 0 finished with value: 0.5317334776574915 and parameters: {'batch_size': 465, 'learning_rate': 0.0017886502933646931, 'dropout_rate': 0.2598201503612632, 'kernel_size': 4}. Best is trial 0 with value: 0.5317334776574915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 31 with best validation loss 0.5317334776574915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-20 23:35:40,207] Trial 1 finished with value: 0.5663940708758095 and parameters: {'batch_size': 426, 'learning_rate': 0.0012440943132401108, 'dropout_rate': 0.35889645675680437, 'kernel_size': 5}. Best is trial 0 with value: 0.5317334776574915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 34 with best validation loss 0.5663940708758095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-20 23:47:00,652] Trial 2 finished with value: 0.5477382603776518 and parameters: {'batch_size': 489, 'learning_rate': 0.0038554343239358218, 'dropout_rate': 0.45678908401231544, 'kernel_size': 3}. Best is trial 0 with value: 0.5317334776574915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 35 with best validation loss 0.5477382603776518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-20 23:58:53,024] Trial 3 finished with value: 0.5286229665570416 and parameters: {'batch_size': 434, 'learning_rate': 0.008884309581572903, 'dropout_rate': 0.26675958506505115, 'kernel_size': 3}. Best is trial 3 with value: 0.5286229665570416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 37 with best validation loss 0.5286229665570416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 00:15:00,931] Trial 4 finished with value: 0.5400151671822538 and parameters: {'batch_size': 477, 'learning_rate': 0.006033916922665238, 'dropout_rate': 0.4233610191313468, 'kernel_size': 4}. Best is trial 3 with value: 0.5286229665570416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 39 with best validation loss 0.5400151671822538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 00:27:21,918] Trial 5 finished with value: 0.544618101729588 and parameters: {'batch_size': 336, 'learning_rate': 0.004757068958886098, 'dropout_rate': 0.26563611696100986, 'kernel_size': 3}. Best is trial 3 with value: 0.5286229665570416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 38 with best validation loss 0.544618101729588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 00:45:00,564] Trial 6 finished with value: 0.5532355191418157 and parameters: {'batch_size': 295, 'learning_rate': 0.009657841077571307, 'dropout_rate': 0.3068324548741184, 'kernel_size': 2}. Best is trial 3 with value: 0.5286229665570416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 44 with best validation loss 0.5532355191418157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 01:00:51,864] Trial 7 finished with value: 0.5746607074031124 and parameters: {'batch_size': 379, 'learning_rate': 0.0007030173219993148, 'dropout_rate': 0.4216726995779042, 'kernel_size': 4}. Best is trial 3 with value: 0.5286229665570416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 39 with best validation loss 0.5746607074031124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 01:18:43,621] Trial 8 finished with value: 0.5518565404941054 and parameters: {'batch_size': 271, 'learning_rate': 0.007563734984941601, 'dropout_rate': 0.10094989531846013, 'kernel_size': 2}. Best is trial 3 with value: 0.5286229665570416.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 44 with best validation loss 0.5518565404941054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 01:32:38,481] Trial 9 finished with value: 0.5250286928795534 and parameters: {'batch_size': 397, 'learning_rate': 0.008739763057391768, 'dropout_rate': 0.16740031717969595, 'kernel_size': 5}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 43 with best validation loss 0.5250286928795534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 01:45:54,012] Trial 10 finished with value: 0.5377884789389007 and parameters: {'batch_size': 376, 'learning_rate': 0.007244225083396923, 'dropout_rate': 0.12035046465935981, 'kernel_size': 6}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 32 with best validation loss 0.5377884789389007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 01:57:21,832] Trial 11 finished with value: 0.5354827921412307 and parameters: {'batch_size': 422, 'learning_rate': 0.00947779540437491, 'dropout_rate': 0.19591061592194145, 'kernel_size': 7}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 35 with best validation loss 0.5354827921412307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 02:13:39,324] Trial 12 finished with value: 0.5580311894696643 and parameters: {'batch_size': 433, 'learning_rate': 0.008125917417243111, 'dropout_rate': 0.1832262577771888, 'kernel_size': 5}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 49 with best validation loss 0.5580311894696643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 02:29:10,195] Trial 13 finished with value: 0.5386810011433479 and parameters: {'batch_size': 347, 'learning_rate': 0.009954378117172613, 'dropout_rate': 0.1880737725806416, 'kernel_size': 6}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 38 with best validation loss 0.5386810011433479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 02:42:06,434] Trial 14 finished with value: 0.5374289964484336 and parameters: {'batch_size': 403, 'learning_rate': 0.006134103080000722, 'dropout_rate': 0.3277652659907684, 'kernel_size': 3}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 40 with best validation loss 0.5374289964484336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 02:58:08,736] Trial 15 finished with value: 0.540821409851148 and parameters: {'batch_size': 511, 'learning_rate': 0.008069315741852778, 'dropout_rate': 0.21984152926906725, 'kernel_size': 6}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 38 with best validation loss 0.540821409851148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 03:10:25,410] Trial 16 finished with value: 0.5485964867385845 and parameters: {'batch_size': 448, 'learning_rate': 0.003006947963863844, 'dropout_rate': 0.15108524852391694, 'kernel_size': 5}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 37 with best validation loss 0.5485964867385845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 03:22:29,985] Trial 17 finished with value: 0.5267409864213136 and parameters: {'batch_size': 346, 'learning_rate': 0.008723192040775533, 'dropout_rate': 0.25019714252443587, 'kernel_size': 3}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 36 with best validation loss 0.5267409864213136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 03:33:12,038] Trial 18 finished with value: 0.53697952379783 and parameters: {'batch_size': 334, 'learning_rate': 0.006082369837664515, 'dropout_rate': 0.2301874379615004, 'kernel_size': 7}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 32 with best validation loss 0.53697952379783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 03:49:02,172] Trial 19 finished with value: 0.5322173594847902 and parameters: {'batch_size': 299, 'learning_rate': 0.008710772271677901, 'dropout_rate': 0.3597397359046177, 'kernel_size': 2}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 39 with best validation loss 0.5322173594847902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 04:06:02,535] Trial 20 finished with value: 0.563083801232992 and parameters: {'batch_size': 354, 'learning_rate': 0.0067868960090424705, 'dropout_rate': 0.1394410700028394, 'kernel_size': 4}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 42 with best validation loss 0.563083801232992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 04:17:27,270] Trial 21 finished with value: 0.5318129523233934 and parameters: {'batch_size': 400, 'learning_rate': 0.008720542393308299, 'dropout_rate': 0.26760296762723595, 'kernel_size': 3}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 35 with best validation loss 0.5318129523233934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 04:30:30,297] Trial 22 finished with value: 0.5336031291682647 and parameters: {'batch_size': 403, 'learning_rate': 0.008672744823838754, 'dropout_rate': 0.23343808921092862, 'kernel_size': 3}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 40 with best validation loss 0.5336031291682647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 04:47:06,167] Trial 23 finished with value: 0.533350896855396 and parameters: {'batch_size': 311, 'learning_rate': 0.00906375292298429, 'dropout_rate': 0.1657441388596269, 'kernel_size': 5}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 49 with best validation loss 0.533350896855396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[I 2024-08-21 05:03:00,784] Trial 24 finished with value: 0.5645013882054223 and parameters: {'batch_size': 366, 'learning_rate': 0.006858435092767537, 'dropout_rate': 0.290511468379847, 'kernel_size': 4}. Best is trial 9 with value: 0.5250286928795534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 39 with best validation loss 0.5645013882054223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "[W 2024-08-21 05:05:38,711] Trial 25 failed with parameters: {'batch_size': 394, 'learning_rate': 0.005313714286344858, 'dropout_rate': 0.356847827639013, 'kernel_size': 2} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_6043/1067095944.py\", line 133, in objective\n",
      "    my_model.train_one_epoch(trainloader)\n",
      "  File \"/tmp/ipykernel_6043/1067095944.py\", line 108, in train_one_epoch\n",
      "    self.train_loss.append(loss.item())\n",
      "KeyboardInterrupt\n",
      "[W 2024-08-21 05:05:38,712] Trial 25 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 154\u001b[0m\n\u001b[1;32m    148\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m                             storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///db.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m    150\u001b[0m                             study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18final_tune3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m                            load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    152\u001b[0m                            sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2233\u001b[39m))\n\u001b[1;32m    153\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 154\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m stop \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[3], line 133\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    131\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[3], line 108\u001b[0m, in \u001b[0;36mobjective.<locals>.MyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m    106\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    109\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics import Precision, Recall\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import torchvision.models as models\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    set_seed(2233)\n",
    "    # Hyperparameters to be tuned\n",
    "    batch_size = trial.suggest_int('batch_size', 256, 512)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 2, 7)\n",
    "\n",
    "    # Data loaders\n",
    "    trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "    \n",
    "    testset = ImageDataset(data_path='validation_data')\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Model setup\n",
    "    model = resnet18()\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(kernel_size, kernel_size), stride=(1, 1), padding=(kernel_size // 2, kernel_size // 2), bias=False)\n",
    "    num_classes = 4\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(model.fc.in_features, num_classes)\n",
    "    )\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    # Custom model class\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self, model, learning_rate):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.model = model\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "            self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=7, min_lr=5e-6, verbose=True)\n",
    "            self.step = 0\n",
    "            self.metric_precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "            self.metric_recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "            self.train_loss = []\n",
    "            self.valid_loss = []\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "        def train_one_epoch(self, trainloader):\n",
    "            self.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.train_loss.append(loss.item())\n",
    "            avg_loss = np.mean(self.train_loss)\n",
    "            self.train_loss.clear()\n",
    "            return avg_loss\n",
    "\n",
    "        def evaluate(self, testloader):\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    self.valid_loss.append(loss.item())\n",
    "            avg_loss = np.mean(self.valid_loss)\n",
    "            self.valid_loss.clear()\n",
    "            self.scheduler.step(avg_loss)\n",
    "            return avg_loss\n",
    "            \n",
    "    set_seed(2233)\n",
    "    my_model = MyModel(model=model, learning_rate=learning_rate)\n",
    "    my_model = my_model.to('cuda')\n",
    "    early_stop_patience = 15\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        my_model.train_one_epoch(trainloader)\n",
    "        val_loss = my_model.evaluate(testloader)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "            break\n",
    "    return best_val_loss\n",
    "\n",
    "# Start the optimization\n",
    "study = optuna.create_study(direction='minimize',\n",
    "                            storage=\"sqlite:///db.sqlite3\",  \n",
    "                            study_name=\"resnet18final_tune3\",\n",
    "                           load_if_exists=True,\n",
    "                           sampler=optuna.samplers.TPESampler(seed=2233))\n",
    "start = time.perf_counter()\n",
    "study.optimize(objective, n_trials=50)\n",
    "stop = time.perf_counter()\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843de372-d545-463a-bba1-542329ace38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
