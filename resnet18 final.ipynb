{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3dc9e67-0431-40f2-b3bc-ce857f88bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 346\n",
    "learning_rate = 0.008723192040775533\n",
    "dropout_rate = 0.25019714252443587\n",
    "kernel_size = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5105a2-d608-456c-bd07-4fbca6a15b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils_cells import get_images_list, transform_image, transform_target, resize_with_padding\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics import Precision, Recall\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None, target_transform=None, reduce=False):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dataset = shuffle(self.load_dataset(data_path))\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        files = os.listdir(path)\n",
    "        dataset_final = pd.DataFrame()\n",
    "        dataset_final['filename'] = []\n",
    "        dataset_final['class'] = []\n",
    "        for filename in files:\n",
    "            dataset = pd.DataFrame()\n",
    "            if filename.endswith('.txt'):\n",
    "                files = get_images_list(f'{path}/{filename}')\n",
    "                dataset['filename'] = files\n",
    "                dataset['class'] = filename.split('_')[1][:-3]\n",
    "                dataset_final = pd.concat([dataset_final, dataset], ignore_index=True)\n",
    "        return dataset_final                \n",
    "                          \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(f'{self.dataset[\"filename\"].loc[idx]}')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "        #image = resize_with_padding(image, (32, 32))\n",
    "        image = image.astype(np.float32)\n",
    "        image = image/255.0\n",
    "        image = self.transform(image = image)['image'] if self.transform is not None else image\n",
    "\n",
    "        target = self.dataset[\"class\"].loc[idx]\n",
    "\n",
    "        if target == 'normal.':\n",
    "            target_ = [1, 0, 0, 0]\n",
    "        elif target == 'inflamatory.':\n",
    "            target_ = [0, 1, 0, 0]\n",
    "        elif target == 'tumor.':\n",
    "            target_ = [0, 0, 1, 0]\n",
    "        elif target == 'other.':\n",
    "            target_ = [0, 0, 0, 1]\n",
    "        else:\n",
    "            print(target)\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "       \n",
    "     \n",
    "\n",
    "        \"\"\"To see transorms use:\n",
    "            image, target = trainset[15]\n",
    "            image = image.numpy()\n",
    "            image=np.swapaxes(image,0,1)\n",
    "            image=np.swapaxes(image,1,2)\n",
    "            plt.imshow(image)\"\"\"\n",
    "\n",
    "        return image.float(), torch.Tensor(np.array(target_, dtype=np.float32))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c0dcac-1c09-4caf-b1ed-d357112031ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:2587: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2233)\n",
    "\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc76746-e31d-4264-86e1-0c4941c3208c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11170884"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data')\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model setup\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(kernel_size, kernel_size), stride=(1, 1), padding=(kernel_size // 2, kernel_size // 2), bias=False)\n",
    "num_classes = 4\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "model = model.to('cuda')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1220283-65c6-4476-a8e4-084f05a2f04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamsoja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240821_162253-mp0u9yv1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/mp0u9yv1' target=\"_blank\">resnet18 final_no_norm_no_pad_final2024-08-21 16:22:52.517230</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/mp0u9yv1' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/mp0u9yv1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/cells/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.9020642047918938\n",
      "train_precision: tensor([0.5475, 0.6824, 0.6342, 0.5249], device='cuda:0')\n",
      "train_recall: tensor([0.5484, 0.7517, 0.6274, 0.0395], device='cuda:0')\n",
      "val_loss: 1.069047961174772\n",
      "val_precision: tensor([0.7481, 0.7954, 0.4423, 0.4193], device='cuda:0')\n",
      "val_recall: tensor([0.1083, 0.6272, 0.9404, 0.1697], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.7793060992267012\n",
      "train_precision: tensor([0.6170, 0.7253, 0.6908, 0.5918], device='cuda:0')\n",
      "train_recall: tensor([0.6007, 0.7888, 0.6958, 0.1879], device='cuda:0')\n",
      "val_loss: 0.9765052442693531\n",
      "val_precision: tensor([0.5010, 0.8345, 0.5647, 0.3593], device='cuda:0')\n",
      "val_recall: tensor([0.6191, 0.4447, 0.7201, 0.2017], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.7358719120087063\n",
      "train_precision: tensor([0.6394, 0.7441, 0.7062, 0.6033], device='cuda:0')\n",
      "train_recall: tensor([0.6198, 0.7986, 0.7147, 0.2732], device='cuda:0')\n",
      "val_loss: 0.82359534010905\n",
      "val_precision: tensor([0.6571, 0.6038, 0.7932, 0.8712], device='cuda:0')\n",
      "val_recall: tensor([0.5315, 0.9308, 0.5760, 0.2027], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.7048157595589924\n",
      "train_precision: tensor([0.6548, 0.7586, 0.7172, 0.6336], device='cuda:0')\n",
      "train_recall: tensor([0.6379, 0.8062, 0.7248, 0.3323], device='cuda:0')\n",
      "val_loss: 1.0893169263105713\n",
      "val_precision: tensor([0.4862, 0.9057, 0.4824, 0.7208], device='cuda:0')\n",
      "val_recall: tensor([0.4350, 0.3301, 0.8766, 0.1704], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6796845198637429\n",
      "train_precision: tensor([0.6647, 0.7675, 0.7292, 0.6619], device='cuda:0')\n",
      "train_recall: tensor([0.6479, 0.8127, 0.7359, 0.3779], device='cuda:0')\n",
      "val_loss: 0.7452716947941298\n",
      "val_precision: tensor([0.6107, 0.8122, 0.6973, 0.4576], device='cuda:0')\n",
      "val_recall: tensor([0.6554, 0.7284, 0.7392, 0.3199], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6611898456026775\n",
      "train_precision: tensor([0.6739, 0.7760, 0.7382, 0.6719], device='cuda:0')\n",
      "train_recall: tensor([0.6573, 0.8184, 0.7436, 0.4216], device='cuda:0')\n",
      "val_loss: 0.6984989910982968\n",
      "val_precision: tensor([0.6232, 0.7730, 0.7543, 0.8852], device='cuda:0')\n",
      "val_recall: tensor([0.6975, 0.8085, 0.6814, 0.2778], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6462717787655079\n",
      "train_precision: tensor([0.6819, 0.7801, 0.7437, 0.6764], device='cuda:0')\n",
      "train_recall: tensor([0.6631, 0.8241, 0.7480, 0.4437], device='cuda:0')\n",
      "val_loss: 0.7156370723068938\n",
      "val_precision: tensor([0.6250, 0.7607, 0.7687, 0.7516], device='cuda:0')\n",
      "val_recall: tensor([0.7072, 0.7898, 0.6871, 0.2731], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6312053809227383\n",
      "train_precision: tensor([0.6903, 0.7869, 0.7520, 0.6816], device='cuda:0')\n",
      "train_recall: tensor([0.6744, 0.8274, 0.7534, 0.4736], device='cuda:0')\n",
      "val_loss: 0.8717465673046612\n",
      "val_precision: tensor([0.6661, 0.8613, 0.5546, 0.7559], device='cuda:0')\n",
      "val_recall: tensor([0.4500, 0.6611, 0.8942, 0.3367], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6288697860667095\n",
      "train_precision: tensor([0.6909, 0.7876, 0.7508, 0.6856], device='cuda:0')\n",
      "train_recall: tensor([0.6731, 0.8272, 0.7549, 0.4801], device='cuda:0')\n",
      "val_loss: 1.0244364234503736\n",
      "val_precision: tensor([0.6062, 0.5658, 0.8531, 0.9274], device='cuda:0')\n",
      "val_recall: tensor([0.5542, 0.9352, 0.4369, 0.1936], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6138026030455235\n",
      "train_precision: tensor([0.6988, 0.7924, 0.7593, 0.6928], device='cuda:0')\n",
      "train_recall: tensor([0.6819, 0.8325, 0.7598, 0.5061], device='cuda:0')\n",
      "val_loss: 0.9124415067028017\n",
      "val_precision: tensor([0.6088, 0.8713, 0.5760, 0.3022], device='cuda:0')\n",
      "val_recall: tensor([0.5038, 0.5676, 0.8494, 0.4424], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.6002429347491303\n",
      "train_precision: tensor([0.7034, 0.7975, 0.7641, 0.7041], device='cuda:0')\n",
      "train_recall: tensor([0.6879, 0.8338, 0.7658, 0.5234], device='cuda:0')\n",
      "val_loss: 0.6288902669140463\n",
      "val_precision: tensor([0.6943, 0.7540, 0.7681, 0.8170], device='cuda:0')\n",
      "val_recall: tensor([0.6444, 0.8528, 0.7543, 0.4811], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.5908215832019198\n",
      "train_precision: tensor([0.7071, 0.8015, 0.7716, 0.7056], device='cuda:0')\n",
      "train_recall: tensor([0.6943, 0.8368, 0.7699, 0.5388], device='cuda:0')\n",
      "val_loss: 1.2344808337951867\n",
      "val_precision: tensor([0.5570, 0.9174, 0.4610, 0.2964], device='cuda:0')\n",
      "val_recall: tensor([0.2785, 0.3861, 0.9419, 0.4030], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.5835257505641084\n",
      "train_precision: tensor([0.7094, 0.8040, 0.7746, 0.7133], device='cuda:0')\n",
      "train_recall: tensor([0.7010, 0.8368, 0.7694, 0.5553], device='cuda:0')\n",
      "val_loss: 0.6089831745803133\n",
      "val_precision: tensor([0.6914, 0.8195, 0.7452, 0.8079], device='cuda:0')\n",
      "val_recall: tensor([0.6844, 0.8081, 0.7953, 0.4589], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.5730127141767462\n",
      "train_precision: tensor([0.7140, 0.8081, 0.7789, 0.7152], device='cuda:0')\n",
      "train_recall: tensor([0.7058, 0.8411, 0.7720, 0.5681], device='cuda:0')\n",
      "val_loss: 0.7394689808177591\n",
      "val_precision: tensor([0.6264, 0.8327, 0.7332, 0.2929], device='cuda:0')\n",
      "val_recall: tensor([0.6739, 0.7040, 0.6943, 0.6788], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.566984516363021\n",
      "train_precision: tensor([0.7197, 0.8099, 0.7803, 0.7160], device='cuda:0')\n",
      "train_recall: tensor([0.7077, 0.8421, 0.7768, 0.5820], device='cuda:0')\n",
      "val_loss: 0.6101068494918195\n",
      "val_precision: tensor([0.7035, 0.8703, 0.7048, 0.7453], device='cuda:0')\n",
      "val_recall: tensor([0.6745, 0.7543, 0.8480, 0.5310], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.5536682965194929\n",
      "train_precision: tensor([0.7225, 0.8141, 0.7876, 0.7255], device='cuda:0')\n",
      "train_recall: tensor([0.7160, 0.8438, 0.7794, 0.6023], device='cuda:0')\n",
      "val_loss: 1.071475214056308\n",
      "val_precision: tensor([0.5458, 0.9181, 0.5388, 0.5025], device='cuda:0')\n",
      "val_recall: tensor([0.4961, 0.4106, 0.9037, 0.3343], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.549375924607214\n",
      "train_precision: tensor([0.7265, 0.8160, 0.7902, 0.7257], device='cuda:0')\n",
      "train_recall: tensor([0.7170, 0.8486, 0.7820, 0.6075], device='cuda:0')\n",
      "val_loss: 0.6431980685571607\n",
      "val_precision: tensor([0.6538, 0.7644, 0.7997, 0.5971], device='cuda:0')\n",
      "val_recall: tensor([0.6958, 0.8591, 0.6485, 0.6027], device='cuda:0')\n",
      "Learning rate: 0.008723192040775533\n",
      "train_loss: 0.5393361993937869\n",
      "train_precision: tensor([0.7303, 0.8194, 0.7931, 0.7341], device='cuda:0')\n",
      "train_recall: tensor([0.7237, 0.8501, 0.7829, 0.6206], device='cuda:0')\n",
      "val_loss: 1.7673929846018888\n",
      "val_precision: tensor([0.6454, 0.9194, 0.4234, 0.1558], device='cuda:0')\n",
      "val_recall: tensor([0.2173, 0.2414, 0.9234, 0.4987], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.477164046967855\n",
      "train_precision: tensor([0.7608, 0.8413, 0.8175, 0.7807], device='cuda:0')\n",
      "train_recall: tensor([0.7516, 0.8657, 0.8102, 0.7188], device='cuda:0')\n",
      "val_loss: 0.5466026020295611\n",
      "val_precision: tensor([0.7420, 0.7985, 0.8028, 0.8122], device='cuda:0')\n",
      "val_recall: tensor([0.7096, 0.8729, 0.7865, 0.5751], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.4568553031834619\n",
      "train_precision: tensor([0.7690, 0.8492, 0.8277, 0.7922], device='cuda:0')\n",
      "train_recall: tensor([0.7646, 0.8714, 0.8152, 0.7505], device='cuda:0')\n",
      "val_loss: 0.5247977028848526\n",
      "val_precision: tensor([0.7366, 0.8292, 0.8001, 0.7773], device='cuda:0')\n",
      "val_recall: tensor([0.7317, 0.8522, 0.7962, 0.6522], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.44301029742243975\n",
      "train_precision: tensor([0.7743, 0.8541, 0.8348, 0.8008], device='cuda:0')\n",
      "train_recall: tensor([0.7739, 0.8744, 0.8190, 0.7655], device='cuda:0')\n",
      "val_loss: 0.5843613980414716\n",
      "val_precision: tensor([0.7320, 0.8692, 0.7204, 0.7321], device='cuda:0')\n",
      "val_recall: tensor([0.6786, 0.7760, 0.8602, 0.6303], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.43270955747068407\n",
      "train_precision: tensor([0.7803, 0.8576, 0.8393, 0.8084], device='cuda:0')\n",
      "train_recall: tensor([0.7782, 0.8788, 0.8240, 0.7768], device='cuda:0')\n",
      "val_loss: 0.5431770671172982\n",
      "val_precision: tensor([0.7210, 0.8286, 0.8048, 0.7666], device='cuda:0')\n",
      "val_recall: tensor([0.7420, 0.8469, 0.7756, 0.6525], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.4250462334992997\n",
      "train_precision: tensor([0.7832, 0.8601, 0.8419, 0.8160], device='cuda:0')\n",
      "train_recall: tensor([0.7830, 0.8804, 0.8257, 0.7833], device='cuda:0')\n",
      "val_loss: 0.5617030985123209\n",
      "val_precision: tensor([0.7320, 0.8655, 0.7501, 0.7406], device='cuda:0')\n",
      "val_recall: tensor([0.7139, 0.7929, 0.8410, 0.6478], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.4173200839287609\n",
      "train_precision: tensor([0.7873, 0.8642, 0.8450, 0.8148], device='cuda:0')\n",
      "train_recall: tensor([0.7890, 0.8815, 0.8284, 0.7942], device='cuda:0')\n",
      "val_loss: 0.5513405557652091\n",
      "val_precision: tensor([0.7527, 0.8510, 0.7540, 0.7540], device='cuda:0')\n",
      "val_recall: tensor([0.6936, 0.8296, 0.8414, 0.6616], device='cuda:0')\n",
      "Learning rate: 0.0008723192040775533\n",
      "train_loss: 0.40905389424875355\n",
      "train_precision: tensor([0.7896, 0.8679, 0.8476, 0.8274], device='cuda:0')\n",
      "train_recall: tensor([0.7922, 0.8842, 0.8313, 0.8039], device='cuda:0')\n",
      "val_loss: 0.5438017817249012\n",
      "val_precision: tensor([0.7525, 0.8474, 0.7639, 0.7730], device='cuda:0')\n",
      "val_recall: tensor([0.7053, 0.8325, 0.8365, 0.6593], device='cuda:0')\n",
      "Learning rate: 8.723192040775533e-05\n",
      "train_loss: 0.39688640511362255\n",
      "train_precision: tensor([0.7951, 0.8703, 0.8526, 0.8293], device='cuda:0')\n",
      "train_recall: tensor([0.7955, 0.8896, 0.8343, 0.8195], device='cuda:0')\n",
      "val_loss: 0.5303751120853067\n",
      "val_precision: tensor([0.7443, 0.8428, 0.7937, 0.7694], device='cuda:0')\n",
      "val_recall: tensor([0.7335, 0.8440, 0.8130, 0.6842], device='cuda:0')\n",
      "Learning rate: 8.723192040775533e-05\n",
      "train_loss: 0.3924882001634957\n",
      "train_precision: tensor([0.7982, 0.8717, 0.8532, 0.8386], device='cuda:0')\n",
      "train_recall: tensor([0.7983, 0.8891, 0.8380, 0.8192], device='cuda:0')\n",
      "val_loss: 0.5304805924383442\n",
      "val_precision: tensor([0.7428, 0.8439, 0.7948, 0.7806], device='cuda:0')\n",
      "val_recall: tensor([0.7372, 0.8434, 0.8122, 0.6731], device='cuda:0')\n",
      "Learning rate: 8.723192040775533e-05\n",
      "train_loss: 0.39060784729974474\n",
      "train_precision: tensor([0.7998, 0.8726, 0.8557, 0.8454], device='cuda:0')\n",
      "train_recall: tensor([0.8024, 0.8911, 0.8371, 0.8231], device='cuda:0')\n",
      "val_loss: 0.5318170685446664\n",
      "val_precision: tensor([0.7480, 0.8437, 0.7913, 0.7730], device='cuda:0')\n",
      "val_recall: tensor([0.7310, 0.8458, 0.8160, 0.6889], device='cuda:0')\n",
      "Learning rate: 8.723192040775533e-05\n",
      "train_loss: 0.39024956243625586\n",
      "train_precision: tensor([0.7987, 0.8724, 0.8550, 0.8378], device='cuda:0')\n",
      "train_recall: tensor([0.8000, 0.8891, 0.8388, 0.8219], device='cuda:0')\n",
      "val_loss: 0.5351041809896405\n",
      "val_precision: tensor([0.7533, 0.8444, 0.7850, 0.7748], device='cuda:0')\n",
      "val_recall: tensor([0.7236, 0.8449, 0.8247, 0.6822], device='cuda:0')\n",
      "Learning rate: 8.723192040775533e-05\n",
      "train_loss: 0.3881358960688402\n",
      "train_precision: tensor([0.8012, 0.8752, 0.8550, 0.8377], device='cuda:0')\n",
      "train_recall: tensor([0.8011, 0.8912, 0.8411, 0.8218], device='cuda:0')\n",
      "val_loss: 0.5349876311387909\n",
      "val_precision: tensor([0.7495, 0.8434, 0.7900, 0.7683], device='cuda:0')\n",
      "val_recall: tensor([0.7286, 0.8459, 0.8176, 0.6909], device='cuda:0')\n",
      "Learning rate: 8.723192040775534e-06\n",
      "train_loss: 0.38680809073978\n",
      "train_precision: tensor([0.7988, 0.8743, 0.8577, 0.8451], device='cuda:0')\n",
      "train_recall: tensor([0.8030, 0.8899, 0.8396, 0.8274], device='cuda:0')\n",
      "val_loss: 0.5343230972799022\n",
      "val_precision: tensor([0.7484, 0.8435, 0.7909, 0.7683], device='cuda:0')\n",
      "val_recall: tensor([0.7308, 0.8448, 0.8164, 0.6899], device='cuda:0')\n",
      "Learning rate: 8.723192040775534e-06\n",
      "train_loss: 0.3875905764851593\n",
      "train_precision: tensor([0.7992, 0.8750, 0.8551, 0.8428], device='cuda:0')\n",
      "train_recall: tensor([0.8012, 0.8894, 0.8410, 0.8227], device='cuda:0')\n",
      "val_loss: 0.5334130633859598\n",
      "val_precision: tensor([0.7541, 0.8373, 0.7911, 0.7680], device='cuda:0')\n",
      "val_recall: tensor([0.7220, 0.8537, 0.8172, 0.6909], device='cuda:0')\n",
      "Learning rate: 8.723192040775534e-06\n",
      "train_loss: 0.38729119886330743\n",
      "train_precision: tensor([0.8012, 0.8737, 0.8573, 0.8434], device='cuda:0')\n",
      "train_recall: tensor([0.8024, 0.8907, 0.8415, 0.8229], device='cuda:0')\n",
      "val_loss: 0.5332554197043515\n",
      "val_precision: tensor([0.7506, 0.8409, 0.7904, 0.7795], device='cuda:0')\n",
      "val_recall: tensor([0.7275, 0.8490, 0.8170, 0.6822], device='cuda:0')\n",
      "Learning rate: 8.723192040775534e-06\n",
      "train_loss: 0.38648252810640993\n",
      "train_precision: tensor([0.8001, 0.8739, 0.8567, 0.8403], device='cuda:0')\n",
      "train_recall: tensor([0.8016, 0.8922, 0.8387, 0.8245], device='cuda:0')\n",
      "val_loss: 0.5325154777100024\n",
      "val_precision: tensor([0.7494, 0.8414, 0.7924, 0.7734], device='cuda:0')\n",
      "val_recall: tensor([0.7301, 0.8479, 0.8153, 0.6896], device='cuda:0')\n",
      "Learning rate: 8.723192040775534e-06\n",
      "train_loss: 0.38680489728416223\n",
      "train_precision: tensor([0.8024, 0.8757, 0.8570, 0.8401], device='cuda:0')\n",
      "train_recall: tensor([0.8045, 0.8915, 0.8410, 0.8232], device='cuda:0')\n",
      "val_loss: 0.5340811620267589\n",
      "val_precision: tensor([0.7520, 0.8388, 0.7946, 0.7723], device='cuda:0')\n",
      "val_recall: tensor([0.7291, 0.8528, 0.8143, 0.6875], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "Early stopping at epoch 34 with best validation loss 0.5247977028848526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_name = f'resnet18 final_no_norm_no_pad_final{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "set_seed(2233)\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.1, patience=4, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=num_classes, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        # Logowanie precision dla każdej klasy\n",
    "        wandb.log({'Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        # Logowanie recall dla każdej klasy\n",
    "        wandb.log({'Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        # Obliczanie głównych metryk\n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        # Logowanie głównych metryk\n",
    "        wandb.log({'main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        precision_ = main_metrics_precision\n",
    "        recall_ = main_metrics_recall\n",
    "        \n",
    "        if (precision_ + recall_) > 0:\n",
    "            f1_score_val = 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "        else:\n",
    "            f1_score_val = 0\n",
    "        \n",
    "        wandb.log({'f1_score_val': f1_score_val}, step=self.step)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "    \n",
    "        main_metrics_precision = (precision[0].item() + precision[1].item() + precision[2].item() + precision[3].item()) / 4\n",
    "        \n",
    "        main_metrics_recall = (recall[0].item() + recall[1].item() + recall[2].item() + recall[3].item()) / 4\n",
    "        \n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal recall': recall[0].item()}, step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()}, step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()}, step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_main_metrics_precision': main_metrics_precision}, step=self.step)\n",
    "        wandb.log({'val_main_metrics_recall': main_metrics_recall}, step=self.step)\n",
    "\n",
    "        precision_ = main_metrics_precision\n",
    "        recall_ = main_metrics_recall\n",
    "        \n",
    "        if (precision_ + recall_) > 0:\n",
    "            f1_score_val = 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "        else:\n",
    "            f1_score_val = 0\n",
    "        \n",
    "        wandb.log({'f1_score_val': f1_score_val}, step=self.step)\n",
    "        \n",
    "        \n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "set_seed(2233)\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate)\n",
    "my_model = my_model.to('cuda')\n",
    "early_stop_patience = 15\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(my_model.state_dict(), f\"{run_path}.pt\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed3b8e-94f7-4113-a37c-486e41c4e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))\n",
    "\n",
    "def test_report(model, dataloader):\n",
    "    \"\"\"Prints confusion matrix for testing dataset\n",
    "    dataloader should be of batch_size=1.\"\"\"\n",
    "\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            output = model(data)\n",
    "            label = label.numpy()\n",
    "            output = output.numpy()\n",
    "            y_pred.append(np.argmax(output))\n",
    "            y_test.append(np.argmax(label))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "testset =ImageDataset(data_path='test_data', transform=transform_test, reduce=True)\n",
    "dataloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "test_report(my_model.to('cpu'), dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
