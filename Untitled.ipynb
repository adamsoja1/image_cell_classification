{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9951eac7-cd7a-4250-be20-f2d884b0cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "ckpt_path = 'training_checkpoints/resnet18_normalize_batch_2048_test_from_pretrained2024-04-28 22:44:55.674715.pth'\n",
    "model = resnet18()\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "num_classes = 4\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(model.fc.in_features, num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "886d55d1-21a1-4a95-a436-63cf99d0e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(ckpt_path)\n",
    "\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace('model.', '')  \n",
    "    new_state_dict[new_key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97a38d18-18d1-4c23-8397-0991ef901406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5802f071-c0f7-4675-8df8-bd73b53e8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith('fc'):  \n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd9816a-b82d-4666-b6fc-dd9c6266199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for name, param in model.named_parameters():\n",
    "    if i%2 == 0:\n",
    "        param.requires_grad = True\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "226e21f2-a9dd-4c73-a5ec-b2daed4f2fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight True\n",
      "bn1.weight False\n",
      "bn1.bias True\n",
      "layer1.0.conv1.weight False\n",
      "layer1.0.bn1.weight True\n",
      "layer1.0.bn1.bias False\n",
      "layer1.0.conv2.weight True\n",
      "layer1.0.bn2.weight False\n",
      "layer1.0.bn2.bias True\n",
      "layer1.1.conv1.weight False\n",
      "layer1.1.bn1.weight True\n",
      "layer1.1.bn1.bias False\n",
      "layer1.1.conv2.weight True\n",
      "layer1.1.bn2.weight False\n",
      "layer1.1.bn2.bias True\n",
      "layer2.0.conv1.weight False\n",
      "layer2.0.bn1.weight True\n",
      "layer2.0.bn1.bias False\n",
      "layer2.0.conv2.weight True\n",
      "layer2.0.bn2.weight False\n",
      "layer2.0.bn2.bias True\n",
      "layer2.0.downsample.0.weight False\n",
      "layer2.0.downsample.1.weight True\n",
      "layer2.0.downsample.1.bias False\n",
      "layer2.1.conv1.weight True\n",
      "layer2.1.bn1.weight False\n",
      "layer2.1.bn1.bias True\n",
      "layer2.1.conv2.weight False\n",
      "layer2.1.bn2.weight True\n",
      "layer2.1.bn2.bias False\n",
      "layer3.0.conv1.weight True\n",
      "layer3.0.bn1.weight False\n",
      "layer3.0.bn1.bias True\n",
      "layer3.0.conv2.weight False\n",
      "layer3.0.bn2.weight True\n",
      "layer3.0.bn2.bias False\n",
      "layer3.0.downsample.0.weight True\n",
      "layer3.0.downsample.1.weight False\n",
      "layer3.0.downsample.1.bias True\n",
      "layer3.1.conv1.weight False\n",
      "layer3.1.bn1.weight True\n",
      "layer3.1.bn1.bias False\n",
      "layer3.1.conv2.weight True\n",
      "layer3.1.bn2.weight False\n",
      "layer3.1.bn2.bias True\n",
      "layer4.0.conv1.weight False\n",
      "layer4.0.bn1.weight True\n",
      "layer4.0.bn1.bias False\n",
      "layer4.0.conv2.weight True\n",
      "layer4.0.bn2.weight False\n",
      "layer4.0.bn2.bias True\n",
      "layer4.0.downsample.0.weight False\n",
      "layer4.0.downsample.1.weight True\n",
      "layer4.0.downsample.1.bias False\n",
      "layer4.1.conv1.weight True\n",
      "layer4.1.bn1.weight False\n",
      "layer4.1.bn1.bias True\n",
      "layer4.1.conv2.weight False\n",
      "layer4.1.bn2.weight True\n",
      "layer4.1.bn2.bias False\n",
      "fc.1.weight True\n",
      "fc.1.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2e6c84-e4f9-49c5-a490-730ff85a3f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e3a594a-a65c-42ac-bc21-583a4b3c9035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6edlms10) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Inflamatory precision</td><td>▆▅▅▅▃█▁█▃▅▅▇▂█▄▅▇</td></tr><tr><td>Inflamatory recall</td><td>▁▅▆▅▅▄▄▃▆▆█▅▆▆▅▅▅</td></tr><tr><td>Normal precision</td><td>▁▄▄▆▆▆▃▄▆▆█▇▇▆▆▇█</td></tr><tr><td>Normal recall</td><td>▂▄▄█▄▆█▇▅▁▇▁▅▅▅▅▄</td></tr><tr><td>Other precision</td><td>▁▃▃▅▄▄▅▅▇▄▆▇█▅▆▆▇</td></tr><tr><td>Other recall</td><td>▁█▆▇▇█▆▇▇▆▇▇█▇▆▇█</td></tr><tr><td>Tumor precision</td><td>▁▆▆██▇█▇▇▅█▅▇▇▅▇▆</td></tr><tr><td>Tumor recall</td><td>▅▁▃▃▆▇▁▆▃▇▅█▄▆▄▆▇</td></tr><tr><td>loss</td><td>█▂▂▂▂▁▂▁▂▂▁▁▁▁▂▁▁</td></tr><tr><td>val_Inflamatory precision</td><td>▅▁▂▅█▇▇▄▅▃▄▄▆▁▄▂▄</td></tr><tr><td>val_Inflamatory recall</td><td>▄█▇▅▁▃▃▅▄▆▅▅▃█▅▇▅</td></tr><tr><td>val_Normal precision</td><td>▃▅█▄▃▃▁▆▄█▄▆▄▇▆█▇</td></tr><tr><td>val_Normal recall</td><td>▅▅▁▅▄▅█▃▅▁▅▃▅▃▄▂▃</td></tr><tr><td>val_Other precision</td><td>▅▄▄▁▇▇▃█▆▂▅▃▅▄▅▄▃</td></tr><tr><td>val_Other recall</td><td>▄▅▄█▃▁▆▁▄█▅▆▄▅▅▅▆</td></tr><tr><td>val_Tumor precision</td><td>▃█▃▅▁▃▇▂▄▃▅▃▄▆▄▃▃</td></tr><tr><td>val_Tumor recall</td><td>▄▁▆▃█▆▂▆▅▆▄▆▆▄▅▅▆</td></tr><tr><td>val_loss</td><td>█▄▇▃▆▆▁▄▃▆▂▅▃▂▄▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Inflamatory precision</td><td>0.79873</td></tr><tr><td>Inflamatory recall</td><td>0.84166</td></tr><tr><td>Normal precision</td><td>0.7348</td></tr><tr><td>Normal recall</td><td>0.71625</td></tr><tr><td>Other precision</td><td>0.74092</td></tr><tr><td>Other recall</td><td>0.60952</td></tr><tr><td>Tumor precision</td><td>0.77699</td></tr><tr><td>Tumor recall</td><td>0.76863</td></tr><tr><td>loss</td><td>0.56141</td></tr><tr><td>val_Inflamatory precision</td><td>0.82955</td></tr><tr><td>val_Inflamatory recall</td><td>0.84377</td></tr><tr><td>val_Normal precision</td><td>0.74598</td></tr><tr><td>val_Normal recall</td><td>0.7136</td></tr><tr><td>val_Other precision</td><td>0.70388</td></tr><tr><td>val_Other recall</td><td>0.66027</td></tr><tr><td>val_Tumor precision</td><td>0.78101</td></tr><tr><td>val_Tumor recall</td><td>0.80636</td></tr><tr><td>val_loss</td><td>0.5439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet18_normalize_batch_2048_test_from_pretrained2024-04-28 23:04:11.634985</strong> at: <a href='https://wandb.ai/adamsoja/cells/runs/6edlms10' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/6edlms10</a><br/> View project at: <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240428_230411-6edlms10/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6edlms10). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/adam/Desktop/cells_master_thesis/wandb/run-20240428_231548-mmx6xxaj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adamsoja/cells/runs/mmx6xxaj' target=\"_blank\">resnet18_normalize_batch_2048_test_from_pretrained2024-04-28 23:15:48.440519</a></strong> to <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adamsoja/cells' target=\"_blank\">https://wandb.ai/adamsoja/cells</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adamsoja/cells/runs/mmx6xxaj' target=\"_blank\">https://wandb.ai/adamsoja/cells/runs/mmx6xxaj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EPOCH: 0\n",
      "train_loss: 0.6707911467253984\n",
      "train_precision: tensor([0.6811, 0.7584, 0.7304, 0.6541], device='cuda:0')\n",
      "train_recall: tensor([0.6626, 0.8144, 0.7222, 0.4227], device='cuda:0')\n",
      "val_loss: 0.643190831070145\n",
      "val_precision: tensor([0.7210, 0.7607, 0.7529, 0.6127], device='cuda:0')\n",
      "val_recall: tensor([0.6331, 0.8647, 0.7441, 0.5939], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 0 time:  0.36732950931667196\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 1\n",
      "train_loss: 0.6592486793615763\n",
      "train_precision: tensor([0.6838, 0.7618, 0.7372, 0.6609], device='cuda:0')\n",
      "train_recall: tensor([0.6698, 0.8173, 0.7225, 0.4466], device='cuda:0')\n",
      "val_loss: 0.6020309990478887\n",
      "val_precision: tensor([0.6845, 0.8152, 0.7794, 0.6850], device='cuda:0')\n",
      "val_recall: tensor([0.7337, 0.8123, 0.7400, 0.5630], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 1 time:  0.383525579683328\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 2\n",
      "train_loss: 0.6496819525051856\n",
      "train_precision: tensor([0.6920, 0.7651, 0.7431, 0.6673], device='cuda:0')\n",
      "train_recall: tensor([0.6749, 0.8228, 0.7295, 0.4515], device='cuda:0')\n",
      "val_loss: 0.6068972720868058\n",
      "val_precision: tensor([0.6858, 0.8369, 0.7536, 0.7777], device='cuda:0')\n",
      "val_recall: tensor([0.7324, 0.7888, 0.7759, 0.4663], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 2 time:  0.3679121881833301\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 3\n",
      "train_loss: 0.6398469498819223\n",
      "train_precision: tensor([0.6980, 0.7680, 0.7451, 0.6799], device='cuda:0')\n",
      "train_recall: tensor([0.6774, 0.8238, 0.7350, 0.4791], device='cuda:0')\n",
      "val_loss: 0.6144276436418294\n",
      "val_precision: tensor([0.7265, 0.7817, 0.7728, 0.5723], device='cuda:0')\n",
      "val_recall: tensor([0.6789, 0.8432, 0.7499, 0.6673], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 3 time:  0.3665162897499916\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 4\n",
      "train_loss: 0.6308855830096574\n",
      "train_precision: tensor([0.6990, 0.7700, 0.7527, 0.6858], device='cuda:0')\n",
      "train_recall: tensor([0.6845, 0.8241, 0.7363, 0.4945], device='cuda:0')\n",
      "val_loss: 0.5953443056179417\n",
      "val_precision: tensor([0.7167, 0.8448, 0.7277, 0.6809], device='cuda:0')\n",
      "val_recall: tensor([0.6849, 0.7897, 0.8242, 0.5246], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 4 time:  0.3826261259666656\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 5\n",
      "train_loss: 0.6222650062973972\n",
      "train_precision: tensor([0.7040, 0.7749, 0.7554, 0.6944], device='cuda:0')\n",
      "train_recall: tensor([0.6882, 0.8285, 0.7409, 0.5042], device='cuda:0')\n",
      "val_loss: 0.6074685554537508\n",
      "val_precision: tensor([0.7069, 0.8132, 0.7565, 0.7443], device='cuda:0')\n",
      "val_recall: tensor([0.7079, 0.8107, 0.7812, 0.5135], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 5 time:  0.3603343890333387\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 6\n",
      "train_loss: 0.6183124450326605\n",
      "train_precision: tensor([0.7070, 0.7755, 0.7556, 0.6977], device='cuda:0')\n",
      "train_recall: tensor([0.6891, 0.8299, 0.7419, 0.5115], device='cuda:0')\n",
      "val_loss: 0.5712575205912193\n",
      "val_precision: tensor([0.7207, 0.8319, 0.7653, 0.6784], device='cuda:0')\n",
      "val_recall: tensor([0.7167, 0.8117, 0.7945, 0.6215], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 6 time:  0.3781591434833293\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 7\n",
      "train_loss: 0.6108235128228333\n",
      "train_precision: tensor([0.7099, 0.7779, 0.7589, 0.7038], device='cuda:0')\n",
      "train_recall: tensor([0.6924, 0.8297, 0.7463, 0.5254], device='cuda:0')\n",
      "val_loss: 0.5748911314333479\n",
      "val_precision: tensor([0.7426, 0.7915, 0.7671, 0.7390], device='cuda:0')\n",
      "val_recall: tensor([0.6710, 0.8669, 0.7862, 0.5633], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 7 time:  0.363824746633342\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 8\n",
      "train_loss: 0.6061199162246359\n",
      "train_precision: tensor([0.7128, 0.7799, 0.7603, 0.7106], device='cuda:0')\n",
      "train_recall: tensor([0.6942, 0.8334, 0.7463, 0.5395], device='cuda:0')\n",
      "val_loss: 0.5609285079770618\n",
      "val_precision: tensor([0.7298, 0.8190, 0.7730, 0.7484], device='cuda:0')\n",
      "val_recall: tensor([0.7065, 0.8365, 0.8002, 0.5640], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 8 time:  0.3881353483666771\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 9\n",
      "train_loss: 0.6001836603386478\n",
      "train_precision: tensor([0.7167, 0.7822, 0.7636, 0.7127], device='cuda:0')\n",
      "train_recall: tensor([0.6980, 0.8357, 0.7494, 0.5426], device='cuda:0')\n",
      "val_loss: 0.5766558797823058\n",
      "val_precision: tensor([0.7338, 0.8219, 0.7593, 0.6318], device='cuda:0')\n",
      "val_recall: tensor([0.6892, 0.8319, 0.7925, 0.6626], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 9 time:  0.368161333516673\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 10\n",
      "train_loss: 0.5939697909383563\n",
      "train_precision: tensor([0.7201, 0.7841, 0.7662, 0.7110], device='cuda:0')\n",
      "train_recall: tensor([0.7019, 0.8373, 0.7507, 0.5531], device='cuda:0')\n",
      "val_loss: 0.5668635891543494\n",
      "val_precision: tensor([0.7288, 0.8245, 0.7763, 0.6374], device='cuda:0')\n",
      "val_recall: tensor([0.7138, 0.8305, 0.7843, 0.6559], device='cuda:0')\n",
      "Learning rate: 0.001\n",
      "epoch 10 time:  0.36940061373334176\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 11\n",
      "train_loss: 0.5890308313067291\n",
      "train_precision: tensor([0.7221, 0.7869, 0.7666, 0.7097], device='cuda:0')\n",
      "train_recall: tensor([0.7023, 0.8383, 0.7547, 0.5517], device='cuda:0')\n",
      "val_loss: 0.5738946630515986\n",
      "val_precision: tensor([0.7494, 0.8194, 0.7472, 0.7041], device='cuda:0')\n",
      "val_recall: tensor([0.6677, 0.8393, 0.8210, 0.6057], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 11 time:  0.37534493825000936\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 12\n",
      "train_loss: 0.5733609906685644\n",
      "train_precision: tensor([0.7319, 0.7910, 0.7676, 0.7453], device='cuda:0')\n",
      "train_recall: tensor([0.6997, 0.8419, 0.7694, 0.5750], device='cuda:0')\n",
      "val_loss: 0.5549175573305951\n",
      "val_precision: tensor([0.7392, 0.8303, 0.7687, 0.7214], device='cuda:0')\n",
      "val_recall: tensor([0.7093, 0.8327, 0.8092, 0.6121], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 12 time:  0.3981031340500067\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 13\n",
      "train_loss: 0.5651839589738931\n",
      "train_precision: tensor([0.7340, 0.7947, 0.7771, 0.7474], device='cuda:0')\n",
      "train_recall: tensor([0.7121, 0.8440, 0.7688, 0.5870], device='cuda:0')\n",
      "val_loss: 0.548691891051001\n",
      "val_precision: tensor([0.7433, 0.8229, 0.7792, 0.7249], device='cuda:0')\n",
      "val_recall: tensor([0.7119, 0.8447, 0.8025, 0.6219], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 13 time:  0.40036534786666683\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 14\n",
      "train_loss: 0.5615783663041681\n",
      "train_precision: tensor([0.7343, 0.7949, 0.7778, 0.7496], device='cuda:0')\n",
      "train_recall: tensor([0.7146, 0.8462, 0.7652, 0.5893], device='cuda:0')\n",
      "val_loss: 0.5504464371957712\n",
      "val_precision: tensor([0.7415, 0.8279, 0.7780, 0.7247], device='cuda:0')\n",
      "val_recall: tensor([0.7168, 0.8391, 0.8045, 0.6205], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 14 time:  0.38106025774998975\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 15\n",
      "train_loss: 0.5584006609130006\n",
      "train_precision: tensor([0.7366, 0.7961, 0.7812, 0.7481], device='cuda:0')\n",
      "train_recall: tensor([0.7181, 0.8455, 0.7681, 0.5971], device='cuda:0')\n",
      "val_loss: 0.5495877679437399\n",
      "val_precision: tensor([0.7443, 0.8268, 0.7772, 0.7188], device='cuda:0')\n",
      "val_recall: tensor([0.7137, 0.8413, 0.8059, 0.6222], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 15 time:  0.3667632999000034\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 16\n",
      "train_loss: 0.5560630719671653\n",
      "train_precision: tensor([0.7361, 0.7988, 0.7810, 0.7543], device='cuda:0')\n",
      "train_recall: tensor([0.7179, 0.8477, 0.7677, 0.6075], device='cuda:0')\n",
      "val_loss: 0.5498204838070605\n",
      "val_precision: tensor([0.7461, 0.8309, 0.7737, 0.7078], device='cuda:0')\n",
      "val_recall: tensor([0.7125, 0.8378, 0.8103, 0.6330], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 16 time:  0.36938168110000336\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 17\n",
      "train_loss: 0.5543909603779766\n",
      "train_precision: tensor([0.7366, 0.7986, 0.7814, 0.7496], device='cuda:0')\n",
      "train_recall: tensor([0.7194, 0.8470, 0.7677, 0.6020], device='cuda:0')\n",
      "val_loss: 0.547524771363371\n",
      "val_precision: tensor([0.7380, 0.8264, 0.7899, 0.7239], device='cuda:0')\n",
      "val_recall: tensor([0.7298, 0.8427, 0.7932, 0.6303], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 17 time:  0.38845341429999586\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 18\n",
      "train_loss: 0.5543694099263821\n",
      "train_precision: tensor([0.7369, 0.7996, 0.7811, 0.7477], device='cuda:0')\n",
      "train_recall: tensor([0.7192, 0.8461, 0.7688, 0.6100], device='cuda:0')\n",
      "val_loss: 0.5480653647126423\n",
      "val_precision: tensor([0.7375, 0.8281, 0.7911, 0.7178], device='cuda:0')\n",
      "val_recall: tensor([0.7317, 0.8429, 0.7927, 0.6303], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 18 time:  0.35937405851665666\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 19\n",
      "train_loss: 0.5516964392764289\n",
      "train_precision: tensor([0.7380, 0.8001, 0.7837, 0.7496], device='cuda:0')\n",
      "train_recall: tensor([0.7216, 0.8477, 0.7694, 0.6075], device='cuda:0')\n",
      "val_loss: 0.5495490265389283\n",
      "val_precision: tensor([0.7430, 0.8292, 0.7789, 0.7309], device='cuda:0')\n",
      "val_recall: tensor([0.7188, 0.8409, 0.8045, 0.6256], device='cuda:0')\n",
      "Learning rate: 5e-06\n",
      "epoch 19 time:  0.3588255194500107\n",
      "--------------------------------\n",
      "========================================\n",
      "EPOCH: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 207\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m    206\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 207\u001b[0m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m my_model\u001b[38;5;241m.\u001b[39mevaluate(testloader)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[44], line 101\u001b[0m, in \u001b[0;36mMyModel.train_one_epoch\u001b[0;34m(self, trainloader)\u001b[0m\n\u001b[1;32m     99\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_recall(preds, labels)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:373\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/metric.py:466\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/classification/stat_scores.py:333\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 333\u001b[0m     \u001b[43m_multiclass_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[1;32m    337\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_update(\n\u001b[1;32m    338\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    339\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torchmetrics/functional/classification/stat_scores.py:309\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_classes, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    307\u001b[0m check_value \u001b[38;5;241m=\u001b[39m num_classes \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, name \u001b[38;5;129;01min\u001b[39;00m ((target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;241m+\u001b[39m ((preds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m\"\u001b[39m),) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m ():  \u001b[38;5;66;03m# noqa: RUF005\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     num_unique_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_unique_values \u001b[38;5;241m>\u001b[39m check_value:\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected more unique values in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` than expected. Expected only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheck_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_unique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in `target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/cells/lib/python3.10/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import ImageDataset\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision, Recall\n",
    "from torchvision.models import resnet18\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "import datetime\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "#intecubic interpol\n",
    "\n",
    "run_name = f'resnet18_normalize_batch_2048_test_from_pretrained{datetime.datetime.now()}'\n",
    "run_path = f'training_checkpoints/{run_name}'\n",
    "\n",
    "wandb.init(project=\"cells\", \n",
    "           entity=\"adamsoja\",\n",
    "          name=run_name)\n",
    "\n",
    "import random\n",
    "random.seed(2233)\n",
    "torch.manual_seed(2233)\n",
    "\n",
    "#After /255 so in loading dataset there are no division by 255 just this normalization\n",
    "mean = [0.5006, 0.3526, 0.5495]\n",
    "std = [0.1493, 0.1341, 0.1124]\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose,\n",
    "    Resize,\n",
    "    OneOf,\n",
    "    RandomBrightness,\n",
    "    RandomContrast,\n",
    "    MotionBlur,\n",
    "    MedianBlur,\n",
    "    GaussianBlur,\n",
    "    VerticalFlip,\n",
    "    HorizontalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=mean, std=std),\n",
    "        OneOf([RandomBrightness(limit=0.4, p=1), RandomContrast(limit=0.4, p=0.8)]),\n",
    "        OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3),], p=0.7,),\n",
    "        VerticalFlip(p=0.5),\n",
    "        HorizontalFlip(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = Compose(\n",
    "    [Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model, learning_rate, weight_decay):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.metric_precision = Precision(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.metric_recall = Recall(task=\"multiclass\", num_classes=4, average=None).to('cuda')\n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "        self.precision_per_epochs = []\n",
    "        self.recall_per_epochs = []\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode=\"min\", factor=0.0001, patience=2, min_lr=5e-6, verbose=True)\n",
    "        self.step = 0\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, trainloader):\n",
    "        self.step += 1\n",
    "        self.train()\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(labels, 1)\n",
    "            self.metric_precision(preds, labels)\n",
    "            self.metric_recall(preds, labels)\n",
    "            self.train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        avg_loss = np.mean(self.train_loss)\n",
    "        self.train_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        self.precision_per_epochs.append(precision)\n",
    "        self.recall_per_epochs.append(recall)\n",
    "        print(f'train_loss: {avg_loss}')\n",
    "        print(f'train_precision: {precision}')\n",
    "        print(f'train_recall: {recall}')\n",
    "\n",
    "        wandb.log({'loss': avg_loss},step=self.step)\n",
    "        wandb.log({'Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'Other recall': recall[3].item()},step=self.step)\n",
    "        \n",
    "        \n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate(self, testloader):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(testloader):\n",
    "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                _, labels = torch.max(labels, 1)\n",
    "                self.metric_precision(preds, labels)\n",
    "                self.metric_recall(preds, labels)\n",
    "                self.valid_loss.append(loss.item())\n",
    "    \n",
    "        avg_loss = np.mean(self.valid_loss)\n",
    "        self.scheduler.step(avg_loss)\n",
    "        self.valid_loss.clear()\n",
    "        precision = self.metric_precision.compute()\n",
    "        recall = self.metric_recall.compute()\n",
    "        print(f'val_loss: {avg_loss}')\n",
    "        print(f'val_precision: {precision}')\n",
    "        print(f'val_recall: {recall}')\n",
    "        self.metric_precision.reset()\n",
    "        self.metric_recall.reset()\n",
    "\n",
    "        wandb.log({'val_loss': avg_loss}, step=self.step)\n",
    "        \n",
    "        wandb.log({'val_Normal precision': precision[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory precision': precision[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor precision': precision[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other precision': precision[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        wandb.log({'val_Normal recall': recall[0].item()},step=self.step)\n",
    "        wandb.log({'val_Inflamatory recall': recall[1].item()},step=self.step)\n",
    "        wandb.log({'val_Tumor recall': recall[2].item()},step=self.step)\n",
    "        wandb.log({'val_Other recall': recall[3].item()},step=self.step)\n",
    "\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            print(f\"Learning rate: {param_group['lr']}\")\n",
    "        return avg_loss\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 128\n",
    "\n",
    "trainset = ImageDataset(data_path='train_data', transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "testset = ImageDataset(data_path='validation_data', transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00005\n",
    "\n",
    "\n",
    "model = model.to('cuda')\n",
    "my_model = MyModel(model=model, learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "my_model = my_model.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('========================================')\n",
    "    print(f'EPOCH: {epoch}') \n",
    "    time_start = time.perf_counter()\n",
    "    my_model.train_one_epoch(trainloader)\n",
    "    val_loss = my_model.evaluate(testloader)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state_dict = my_model.state_dict()\n",
    "        torch.save(best_model_state_dict, f'{run_path}.pth')\n",
    "        \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} with best validation loss {best_val_loss}\")\n",
    "        break\n",
    "    time_epoch = time.perf_counter() - time_start\n",
    "    print(f'epoch {epoch} time:  {time_epoch/60}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Load the best model state dict\n",
    "print(f'{run_path}.pth')\n",
    "my_model.load_state_dict(torch.load(f'{run_path}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2bb2ac-0c17-46aa-aec1-f920e3d9034e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
